{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26350157",
   "metadata": {},
   "source": [
    "# Kaj so nevronske mreže\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2432c",
   "metadata": {},
   "source": [
    "Nevronske mreže so bile narejene kot računalniški model delovanja možganov. Z njihovo pomočjo poizkušamo aproksimirati nelinearne funkcije.\n",
    "\n",
    "Uporabljajo se pri avtonomni vožnji, diagnosticiranju bolezni, napovedovanju cen, napovedovanju 3D oblike proteinov, itd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7067c62",
   "metadata": {},
   "source": [
    "Nevronske mreže so zgrajene iz večih plasti medseboj povezanih nevronov, kjer ima vsak nevron določene parametre katere lahko spreminjamo. \n",
    "\n",
    "Začetna plast nevronov sprejme vhodne vrednosti - kot so slika okolice, kemična sestava molekule, itd. - jih spremeni glede na določeno funkcijo in svoje parametre ter te vrednosti posreduje naslednji plasti. Tako vrednosti potujejo do zadnje plasti, ki nam poda končno napoved - ali avto vidi sosednje avte ali ne, način kako se bo protein oblikoval, itd. Če so parametri nevronov pravilno izbrani potem so izhodne vrednosti nevronske mreže blizu resničnim vrednostim. \n",
    "\n",
    "Kako blizu smo resničnim vrednostim lahko matematično izračunamo. Dobljeno vrednost lahko nato uporabimo, da z njeno pomočjo posodobimo parametre nevronov in se tako bolj približamo resnični vrednosti. Na tak način se nevronska mreža uči."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5de82f",
   "metadata": {},
   "source": [
    "Za začetek si poglejmo kako je sestavljen 1 nevron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069fb1f9",
   "metadata": {},
   "source": [
    "# Nevron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924406b",
   "metadata": {},
   "source": [
    "Nevron je sestavljen iz :\n",
    "* vhodnih signalov (*angl.* **inputs**), \n",
    "* uteži (*angl.* **weights**),\n",
    "* praga (*angl.* **bias**)\n",
    "\n",
    "**Inputs** predstavljajo vrednosti, ki so posredovane v neuron. To so lahko značilke (teža, kvadratna površina stanovanja, barva pixla, itd..), lahko pa so vrednosti neuronov iz prednodnje plasti.\n",
    "\n",
    "**Weights** so parametri, ki predstavljajo jakost posameznega input-a. Vsak input v nevron ima svoj specifičen weight.\n",
    "\n",
    "**Bias** je dodaten parameter, specifičen za vsak nevron.\n",
    "\n",
    "Tekom treniranja naše mreže je cilj spreminjati parametre nevronov, tako da, čim bolj natančno napovemo izhodne vrednosti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571754be",
   "metadata": {},
   "source": [
    "Vrednost enega nevrona se izračuna po enačbi:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa7999",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "$\\Large o = \\sum_{j=0}^{n}(i_j w_j) + b$\n",
    "\n",
    "* $i_j$ je specifičen input\n",
    "* $w_j$ je weight specifičnega inputa\n",
    "* $b$   je bias nevrona\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c211ba3",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00b5e3e",
   "metadata": {},
   "source": [
    "Za primer bomo sprogramirali delovanje enega nevrona. \n",
    "* V nevron vstopajo 3 input vrednosti: 1,2,3\n",
    "* Vsak input ima svoj specifični weight: 0.2, 0.8, -0.5\n",
    "* Nevron ima svoj specifični bias: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e7b672",
   "metadata": {},
   "source": [
    "![Nevron](images/01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9634c8",
   "metadata": {},
   "source": [
    "Izhodno vrednost dobimo po zgornji enačbi:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a61215",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "$o = \\sum_{j=0}^{n}(i_j w_j) + b = i_0 \\cdot w_0 + i_1 \\cdot w_1 + i_2 \\cdot w_2 + b = 1 \\cdot 0.2 + 2 \\cdot 0.8 + 3 \\cdot (-0.5) + 2 = 2.3$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1,2,3]\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "bias = 2\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a21de",
   "metadata": {},
   "source": [
    "V primeru večih inputov nevron dobi dodatne weights - dodaten weight za vsak nov input. Bias ostane samo eden saj je le-ta specifičen za nevron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d139e28",
   "metadata": {},
   "source": [
    "![Nevron - 4 inputs](images/02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc07ff",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "$o = \\sum_{j=0}^{n}(i_j w_j) + b = i_0 \\cdot w_0 + i_1 \\cdot w_1 + i_2 \\cdot w_2 + i_3 \\cdot w_3 + b = 1 \\cdot 0.2 + 2 \\cdot 0.8 + 3 \\cdot (-0.5) + 2.5 \\cdot 1.0 + 2 = 4.8$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1,2,3,2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c749b2d",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f019f2",
   "metadata": {},
   "source": [
    "Za hitrejše računanje se uporablja matematične operacije nad tensorji.\n",
    "\n",
    "Za izračun vrednosti neurona se lahko uporabi **dot product**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149a15d6",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "<b>Dot product:</b>\n",
    "\n",
    "$\\Large \\vec{a}^{\\,}\\cdot \\vec{b}^{\\,} = [1,2,3]\\cdot [2,3,4] = 1\\cdot 2 + 2\\cdot 3 + 3\\cdot 4 = 20$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer računanja s tensorji za en neuron\n",
    "import numpy as np\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2.0\n",
    "\n",
    "\n",
    "output = np.dot(weights, inputs) + bias\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4ef40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab5a3375",
   "metadata": {},
   "source": [
    "# Plast nevronov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed92dbb",
   "metadata": {},
   "source": [
    "Nevronske mreže so večinoma sestavljene iz večih plasti (*angl.* **layer**). Plast je preprosto skupina neuronov.\n",
    "\n",
    "V osnovi poznamo vhodno plast (*angl.* **input layer**), ki prejme značilke. Izhodno plast (*angl.* **output layer**), ki vrne napovedano vrednost. Ostale plasti so skrite plasti (*angl.* **hidden layer**). Če ima mreža več kot 1 skrito plast se imenuje **deep neural network**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a25923",
   "metadata": {},
   "source": [
    "Za primer si poglejmo del nevronske mreže kjer imamo vhodno plast sestavljeno iz 4 nevronov, ki so nato povezani z naslednjo plastjo, ki ima 3 nevrone.\n",
    "\n",
    "Vsak izmed štirih nevronov je povezan na vse nevrone naslednje plasti, kar pomeni, da imamo **dense layer**. Razlika med nevroni v isti plasti so njihove weights in bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158331cf",
   "metadata": {},
   "source": [
    "![Dense layer](images/03.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b796c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3, 2.5]                \n",
    "weights1 = [0.2, 0.8, -0.5, 1.0]       # uteži neurona1\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]    # uteži neurona2\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]  # uteži neurona3\n",
    "\n",
    "bias1 = 2    # bias neurona1\n",
    "bias2 = 3    # bias neurona2\n",
    "bias3 = 0.5  # bias neurona3\n",
    "\n",
    "outputs = [\n",
    "    # Output Neuron 1:\n",
    "    inputs[0]*weights1[0] +\n",
    "    inputs[1]*weights1[1] +\n",
    "    inputs[2]*weights1[2] +\n",
    "    inputs[3]*weights1[3] + bias1,\n",
    "\n",
    "    # Output Neuron 2:\n",
    "    inputs[0]*weights2[0] +\n",
    "    inputs[1]*weights2[1] +\n",
    "    inputs[2]*weights2[2] +\n",
    "    inputs[3]*weights2[3] + bias2,\n",
    "\n",
    "    # Output Neuron 3:\n",
    "    inputs[0]*weights3[0] +\n",
    "    inputs[1]*weights3[1] +\n",
    "    inputs[2]*weights3[2] +\n",
    "    inputs[3]*weights3[3] + bias3\n",
    "]\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93bfba",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Ista koda, zapisana z **numpy.dot()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6870bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "layer_outputs = np.dot(weights, inputs) + biases\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d589f",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc0b0db",
   "metadata": {},
   "source": [
    "Sedaj dodamo še en dense layer 3eh nevronov. Inputs te so outputs prejšnje dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96313dc3",
   "metadata": {},
   "source": [
    "![2 Dense layers](images/04.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "\n",
    "# Dense layer 1\n",
    "weights = [[ 0.20,  0.80, -0.50,  1.00],\n",
    "           [ 0.50, -0.91,  0.26, -0.50],\n",
    "           [-0.26, -0.27,  0.17,  0.87]]\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "# Dense layer 2\n",
    "weights2 = [[ 0.10, -0.14,  0.50],\n",
    "            [-0.50,  0.12, -0.33],\n",
    "            [-0.44,  0.73, -0.13]]\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "\n",
    "layer1_outputs = np.dot(weights, inputs) + biases\n",
    "layer2_outputs = np.dot(weights2, layer1_outputs) + biases2\n",
    "\n",
    "print(\"Layer 1: \", layer1_outputs)\n",
    "print(\"Layer 2: \", layer2_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c8aca",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50913a6a",
   "metadata": {},
   "source": [
    "Kodo sedaj spremenimo v python class.\n",
    "\n",
    "Weights parametre inicializiramo naključno.\n",
    "\n",
    "Bias parametre inicializiramo na vrednost 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d1a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda je v CLEAN verziji\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_neurons, n_inputs)\n",
    "        print(\"Weights: \")\n",
    "        print(self.weights)\n",
    "        self.biases = np.zeros(n_neurons)\n",
    "        print(\"Bias: \")\n",
    "        print(self.biases)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(self.weights, inputs) + self.biases\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "\n",
    "print(\"Creating DENSE 1\")\n",
    "dense1 = Layer_Dense(4, 3)\n",
    "print(\"Creating DENSE 2\")\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "dense1.forward(inputs)\n",
    "dense2.forward(dense1.output)\n",
    "\n",
    "print(\"Dense 1 output: \")\n",
    "print(dense1.output)\n",
    "print(\"Dense 2 outut: \")\n",
    "print(dense2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85c7bd1",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cea262",
   "metadata": {},
   "source": [
    "Neuronske mreže ne treniramo z vsakim vzorcem posebej, vendar jih treniramo z večimi vzorci naenkrat. Na tak način se zagotavlja, da se model ne prilagaja vsakemu vzorcu posebej ampak se prilagaja na več vzorcev naenkrat. Tako je model sposoben znanje posplošiti na še ne videne podatke in se izogibamo overfitting-u.\n",
    "\n",
    "Dobra stran treniranja na večih vzorcih naenkrat je tudi zmožnost uporabljanja večih procesorjev kar dodatno pohitri čas učenja. Za ta namen se uporablja GPUs oziroma TPUs (tensor processing units)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1710517",
   "metadata": {},
   "source": [
    "Za primer vzemimo batch, kjer imamo 3 vzorce:\n",
    "```python\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],  # sample 1\n",
    "          [2.0, 5.0, -1.0, 2.0], # sample 2\n",
    "          [-1.5, 2.7, 3.3, -0.8]]# sample 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ce6d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "#layer_outputs = np.dot(np.array(inputs), np.array(weights)) + biases\n",
    "#print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a9eab",
   "metadata": {},
   "source": [
    "Sedaj dobimo error, saj se dimenzije inputs in weights ne poravnajo za izvedbo računanja.\n",
    "\n",
    "Wights želimo v obliki:\n",
    "```python\n",
    "[[ 0.2 ,  0.5 , -0.26],\n",
    "[ 0.8 , -0.91, -0.27],\n",
    "[-0.5 ,  0.26,  0.17],\n",
    "[ 1.  , -0.5 ,  0.87]]\n",
    "```\n",
    "\n",
    "Kjer je stolpec 1 weights nevrona 1, stolpec 2 weights nevrona 2, stolpec 3 weights nevrona 3.\n",
    "\n",
    "Sedaj lahko uporabimo matrični račun:\n",
    "\n",
    "![Matrix calculation](images/05.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],  # Sample 1\n",
    "          [2.0, 5.0, -1.0, 2.0], # Sample 2\n",
    "          [-1.5, 2.7, 3.3, -0.8]]# Sample 3\n",
    "\n",
    "#          # N1    # N2    # N3\n",
    "weights = [[ 0.2 ,  0.5 , -0.26],\n",
    "           [ 0.8 , -0.91, -0.27],\n",
    "           [-0.5 ,  0.26,  0.17],\n",
    "           [ 1.  , -0.5 ,  0.87]]\n",
    "\n",
    "#        # N1   # N2  # N3\n",
    "biases = [2.0,   3.0,  0.5]\n",
    "\n",
    "layer_outputs = np.dot(np.array(inputs), np.array(weights)) + biases\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbddc71",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a80e7e",
   "metadata": {},
   "source": [
    "Posodobimo kodo v Layer_Dense() razredu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a87fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        # <=== HERE ===>\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        # <=== HERE ===>\n",
    "        print(\"Weights: \")\n",
    "        print(self.weights)\n",
    "        # <=== HERE ===>\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # <=== HERE ===>\n",
    "        print(\"Bias: \")\n",
    "        print(self.biases)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        # <=== HERE ===>\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        # <=== HERE ===>\n",
    "        \n",
    "inputs = np.array([[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]])\n",
    "print(\"Inputs:\")\n",
    "print(inputs)\n",
    "\n",
    "dense1 = Layer_Dense(4,3)\n",
    "dense1.forward(inputs)\n",
    "\n",
    "print(\"Dense OUTPUT\")\n",
    "print(dense1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced8cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f40c2c7",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3215b058",
   "metadata": {},
   "source": [
    "Problem je, da lahko z našim trenutnim modelom aproksimiramo samo linearne funkcije.\n",
    "\n",
    "Da rešim to problem se vsakemu nevronu doda aktivacijsko funkcijo (*angl.* **activation function**). To je funkcija, ki sprejme seštevek vhodov, weights in bias-a in vrne neko novo vrednost. Izbira aktivacijske funkcije je pomembna saj le-ta vpliva na vrednosti izhodov, na hitrost učenja in zmožnostjo konvergiranja.\n",
    "\n",
    "Praksa je, da imajo vsi nevroni v isti plasti isto aktivacijsko funkcijo, ni pa potrebno, da je tako."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c10197",
   "metadata": {},
   "source": [
    "![Activation function](images/06.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd73339",
   "metadata": {},
   "source": [
    "## Step Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c6d709",
   "metadata": {},
   "source": [
    "Za začetek si poglejmo preprosto **step** aktivacijsko funkcijo. Njena formula je:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c211c162",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "$\\Large \n",
    "y = \n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t1  & x > 0 \\\\\n",
    "        0  & x \\leq 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88d3a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def step_function(x):\n",
    "    return 1 if x>0 else 0\n",
    "\n",
    "X = np.arange(-10, 10, 0.01)\n",
    "y = [step_function(x) for x in X]\n",
    "\n",
    "plt.plot(X, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa1dad",
   "metadata": {},
   "source": [
    "Step funkcija je bila v uporabi v preteklosti, vendar se večinoma ne uporablja več.\n",
    "\n",
    "Problem se je pojavil, saj funkcija sama po sebi ni dovolj informativna:\n",
    "* Velike številke vrnejo isto vrednost kot pozitivne številke blizu ničle - vrednost 1\n",
    "* Isto velja za velike negativne številke in negativne številke blizu 0 - vrnejo vrednost 0.\n",
    "\n",
    "Pri aktivacijski funkciji želimo večjo granularnost saj tako lažje posodabljamo parametre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70ef9a",
   "metadata": {},
   "source": [
    "## Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad4da5",
   "metadata": {},
   "source": [
    "Zato se je začelo uporabljati **Sigmoid activation function**. Njena definicija je:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d5056",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "$\\Large y = \\frac{1}{1+e^{-x}}$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf276ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / ( 1 + np.exp(-x))\n",
    "\n",
    "X = np.arange(-10, 10, 0.01)\n",
    "y = [sigmoid(x) for x in X]\n",
    "\n",
    "plt.plot(X, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc0e910",
   "metadata": {},
   "source": [
    "Sigmoid vrne vrednosti:\n",
    "* 0 do 0.5 za negativna števila \n",
    "* 0.5 do 1 za pozitivna števila \n",
    "\n",
    "Funkcija je dosti bolj granularna in se veliko uporablja kot aktivacijska funkcija. Vendar pa je tudi dosti bolj kompleksna, kar poveča čas učenja. Funkcija ima tudi problem izginjajočega gradienta kar pomeni, da se pri izredno velikih ali majhnih vhodnih vrednosti izhodna vrednost skoraj ne spremeni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9327f",
   "metadata": {},
   "source": [
    "## Rectified Linear Units - ReLU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d40be",
   "metadata": {},
   "source": [
    "Za reševanje teh problemov se je začela uporabljati ReLU funkcija katere definicija je:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b44cba",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background: lightgreen\">\n",
    "$\\Large \n",
    "y = \n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\tx  & x > 0 \\\\\n",
    "        0  & x \\leq 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897572f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return x if x>0 else 0\n",
    "\n",
    "X = np.arange(-10, 10, 0.01)\n",
    "y = [relu(x) for x in X]\n",
    "\n",
    "plt.plot(X, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06976ed0",
   "metadata": {},
   "source": [
    "## Uporaba aktivacijske funkcije v naši neuronski merži"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb194d30",
   "metadata": {},
   "source": [
    "Napišimo sedaj class aktivacijske funkcije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c74e887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "[[-1.76884571  0.07555227 -1.1306297 ]\n",
      " [-0.65143017 -0.89311563 -1.27410098]\n",
      " [-0.06115443  0.06451384  0.41011295]\n",
      " [-0.57288249 -0.80133362  1.31203519]]\n",
      "Output\n",
      "[[0.         0.07555227 0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.06451384 0.41011295]\n",
      " [0.         0.         1.31203519]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output valzes from input\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "n_samples = 4\n",
    "neurons = 3\n",
    "X = np.random.normal(size=(n_samples,neurons))\n",
    "print(\"Inputs:\")\n",
    "print(X)\n",
    "\n",
    "activation = Activation_ReLU()\n",
    "activation.forward(X)\n",
    "print(\"Output\")\n",
    "print(activation.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67160403",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676932f",
   "metadata": {},
   "source": [
    "Celotna koda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce94475",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        print(\"Weights: \")\n",
    "        print(self.weights)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        print(\"Bias: \")\n",
    "        print(self.biases)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# <=== HERE ===>\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output valzes from input\n",
    "        self.output = np.maximum(0, inputs)\n",
    "# <=== HERE ===>\n",
    "        \n",
    "        \n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "print(\"Ustvarimo DENSE 1\")\n",
    "dense1 = Layer_Dense(4, 3)\n",
    "# <=== HERE ===>\n",
    "activation1 = Activation_ReLU()\n",
    "# <=== HERE ===>\n",
    "print(\"Ustvarimo DENSE 2\")\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "dense1.forward(inputs)\n",
    "# <=== HERE ===>\n",
    "activation1.forward(dense1.output)\n",
    "# <=== HERE ===>\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "print()\n",
    "print(\"Dense 1 output: \")\n",
    "print(dense1.output)\n",
    "# <=== HERE ===>\n",
    "print(\"ReLU 1 output:\")\n",
    "print(activation1.output)\n",
    "# <=== HERE ===>\n",
    "print(\"Dense 2 output: \")\n",
    "print(dense2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f6135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09a8f949",
   "metadata": {},
   "source": [
    "## Classification dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153a3d79",
   "metadata": {},
   "source": [
    "V nadaljnem bomo uporabili težji dataset ter izvajali klasifikacijo treh različnih razredov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def vertical_data(samples, classes):\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        X[ix] = np.c_[np.random.randn(samples)*.1 + (class_number)/3, np.random.randn(samples)*.1 + 0.5]\n",
    "        y[ix] = class_number\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a189590",
   "metadata": {},
   "source": [
    "## Softmax Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b4b996",
   "metadata": {},
   "source": [
    "Kot aktivacijsko funkcijo zadnje plasti bomo uporabili **Softmax**. Tako bomo kot napovedane izhodne vrednosti dobili % kako verjetno je, da vhodni podatki spadajo v določen razred - *confidence score*\n",
    "> Primer: Če bodo naše izhodne vrednosti [0.7, 0.2, 0.1] potem model s 70% natančnostjo verjame, da vhodni podatki pripadajo razredu 0. Z 20% natančnostjo, da so razreda 1 in 10% natančnostjo, da so podatki razreda 2.\n",
    "\n",
    "Enačba softmax funkcije je:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f460b5e",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "$\\Large S_j = \\frac{e^{o_j}}{\\sum_{l=0}^{L}e^{o_l}}$\n",
    "\n",
    "* $S_j$ je confidence score $j$ razreda\n",
    "* $o_j$ je izhodna vrednost neurona\n",
    "* $\\sum_{l=0}^{L}e^{o_l}$ je seštevek $e^o$ vseh izhodnih vrednosti neuronov\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "314c3837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06414769 0.17437149 0.47399085 0.28748998]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(inputs):\n",
    "    exp_values = np.exp(inputs)\n",
    "    return exp_values / np.sum(exp_values)\n",
    "\n",
    "layer_outputs = [1.0, 2.0, 3.0, 2.5]\n",
    "\n",
    "softmax_output = softmax(layer_outputs)\n",
    "print(softmax_output)\n",
    "print(sum(softmax_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d35adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs)\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "layer_outputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "softmax = Activation_Softmax()\n",
    "softmax.forward(layer_outputs)\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255de16f",
   "metadata": {},
   "source": [
    "Problem se lahko pojavi pri prevelikih številkah. Zato bomo pred začetkom izračuna Softmax vrednosti odšteli največjo vrednost od vhodnih vrednosti. S tem, ko odštejemo največjo vrednost končnega rezultata ne spremenimo. Spremenimo pa, da se vhodne vrednosti gibljejo od 1 - kar nam s potenciranjem vrne vrednost $e$ - do izredno majhne številke - kar nam s potenciranjem vrne vrednost blizu 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e444eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "        \n",
    "layer_outputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "softmax = Activation_Softmax()\n",
    "softmax.forward(layer_outputs)\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d613df",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67950d97",
   "metadata": {},
   "source": [
    "Softmax koda in dataset dodana celotni kodi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c74a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "# <=== HERE ===>      \n",
    "        \n",
    "# <=== HERE ===>\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "# <=== HERE ===>\n",
    "\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "# <=== HERE ===>\n",
    "activation2 = Activation_Softmax()\n",
    "# <=== HERE ===>\n",
    "\n",
    "# <=== HERE ===>\n",
    "dense1.forward(X[:5])\n",
    "# <=== HERE ===>\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "# <=== HERE ===>\n",
    "activation2.forward(dense2.output)\n",
    "# <=== HERE ===>\n",
    "\n",
    "print(\"Dense 1 output: \")\n",
    "print(dense1.output)\n",
    "print(\"ReLU 1 output:\")\n",
    "print(activation1.output)\n",
    "print(\"Dense 2 outut: \")\n",
    "print(dense2.output)\n",
    "# <=== HERE ===>\n",
    "print(\"Softmax output - PREDICTION\")\n",
    "print(activation2.output)\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43914f2c",
   "metadata": {},
   "source": [
    "Skoraj vsak prediction ima vrednost 33%, kar pomeni, da model preprosto ugiba v kateri razred naj umesti določeno točko."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325f537",
   "metadata": {},
   "source": [
    "# Loss Function and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d273181e",
   "metadata": {},
   "source": [
    "Sedaj, ko imamo zgrajen model lahko specifične vhodne podatke pretvorimo v napovedano vrednost.\n",
    "> Na podlagi kvadrature stanovanja in lokacije lahko napovemo ceno\n",
    "\n",
    "> Iz dolžnie in širine lista lahko napovemo vrsto rastline\n",
    "\n",
    "\n",
    "Kako uspešen je model pri napvedovanju izračunamo s pomočjo **loss function** (drugo ime je tudi *cost function*). Vrednost loss function nam pove kako blizu resnični vrednosti je bila napovedana vrednost. Idealno bi želeli, da je vrednost loss function enaka 0.\n",
    "\n",
    "Model ocenjujemo tudi z vrednostjo **accuracy**, ki nam pove koliko vrednosti smo pravilno napovedali."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991036bb",
   "metadata": {},
   "source": [
    "## Categorical Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b47caaf",
   "metadata": {},
   "source": [
    "Za naš primer klasifikacije bomo za izračun napake uporabili funkcijo **categorical cross-entropy loss**. Funkcija se uporablja za primerjavo resničnih vrednosti (*ground-truth* vrednosti) z napovedano distribucijo (softmax predictions).\n",
    "\n",
    "Enačba je sledeča:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f442dc1",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "$\\Large L = - \\sum_{j}y_j log(\\hat{y_j})$\n",
    "\n",
    "* $L$ je **loss** vrednost\n",
    "* $y_j$ je resnična vrednost\n",
    "* $\\hat{y_j}$ je napovedana vrednost\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e2a91",
   "metadata": {},
   "source": [
    "Za primer uzemimo sledeče vrednosti - model, kjer imamo kot zadnjo funkcijo Softmax, je napovedal sledeč *confidence score* [0.7, 0.1, 0.2]. S tem bi lahko našo točko ocenili, da spada v razred 0.\n",
    "\n",
    "Resnične vrednosti so sledeče [1, 0, 0], kar pomeni, da naša točka spada v razred 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b753f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [0.7, 0.1, 0.2]\n",
    "real_values =  [1, 0, 0]\n",
    "\n",
    "loss = -(real_values[0]*np.log(inputs[0]) +\n",
    "         real_values[1]*np.log(inputs[1]) +\n",
    "         real_values[2]*np.log(inputs[2]))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8546c3ff",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95303a97",
   "metadata": {},
   "source": [
    "Za naš specifičen primer lahko naredimo nekaj posplošitev.\n",
    "\n",
    "Ker lahko naša točka spada samo v 1 razred, vemo, da ima iskan razred vedno vrednost 1 in preostala dva razreda vedno vrednost 0. Zato lahko našo loss funkcijo posplošimo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34efb34a",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "$\\Large L = - \\sum_{j}y_j log(\\hat{y_j}) = \\\\ \n",
    "\\Large -( y_0 log(\\hat{y_0}) + y_1 log(\\hat{y_1}) + y_2 log(\\hat{y_2})) = \\\\ \n",
    "\\Large -(1 \\cdot log(\\hat{y_0}) + 0 \\cdot log(\\hat{y_1}) + 0 \\cdot log(\\hat{y_2})) = \\\\\n",
    "\\Large - log(\\hat{y_0}) = - log(\\hat{y_k})$\n",
    "\n",
    "* $k$ - index pravilnega razreda\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [0.7, 0.1, 0.2]\n",
    "real_values =  [1, 0, 0]\n",
    "\n",
    "correct_class_index = real_values.index(1)\n",
    "print(\"Correct class index: \", correct_class_index)\n",
    "loss = -np.log(inputs[correct_class_index])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0933f",
   "metadata": {},
   "source": [
    "Dodaten problem se pojavi, če model za resnično vrednost napove 0, saj tako dobimo izraz:\n",
    "\n",
    "$log( 0 )$\n",
    "\n",
    "vredonst katerega je $-\\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46208345",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [0.7, 0.0, 0.3]\n",
    "real_values =  [0, 1, 0]\n",
    "\n",
    "correct_class_index = real_values.index(1)\n",
    "print(\"Correct class index: \", correct_class_index)\n",
    "loss = -np.log(inputs[correct_class_index])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf8d18",
   "metadata": {},
   "source": [
    "Problem rešimo tako, da najmanjšo vrednost povečamo za neko minimalno številko. Na tak način se znebimo problema z ničlo in na končni loss rezultat bistveno ne vplivamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54976170",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-np.log(0.1)) # ne-korigiran rezultat\n",
    "print(-np.log(0.1+1e-7)) # korigiran rezultat\n",
    "# končna razlika med rezultatoma je skoraj neopazna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b12331",
   "metadata": {},
   "source": [
    "Dodaten problem se sedaj pojavi, če model napove vrednost 1, kjer imamo sedaj negativno vrednost loss funkcije. Pravilna vrednost pa bi morala biti 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412895db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-np.log(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf75f3",
   "metadata": {},
   "source": [
    "Da se rešimo tega problema bomo največjo številko pomanjšal za neko minimalno vrednost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-np.log(1)) # pravilna in željena vrednost\n",
    "print(-np.log(1-1e-7)) # korigirana vrednost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ca902",
   "metadata": {},
   "source": [
    "Obe dve rešitvi najdemo znotraj numpy paketa v funkciji *clip()*, ki vrednosti v listu, ki so zunaj specificiranega intervala postavi na vrednosti mej intervala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1.7, 0.0, 0.3]\n",
    "print(\"Inputs:\")\n",
    "print(inputs)\n",
    "cor_inputs = np.clip(inputs, 1e-7, 1-1e-7)\n",
    "print(\"Corrected inputs:\")\n",
    "print(cor_inputs)\n",
    "\n",
    "real_values =  [0, 1, 0]\n",
    "\n",
    "correct_class_index = real_values.index(1)\n",
    "print(\"Correct class index: \", correct_class_index)\n",
    "loss = -np.log(cor_inputs[correct_class_index])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb508b1",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e08d8",
   "metadata": {},
   "source": [
    "Stvar bomo sedaj zapisali v python razrede.\n",
    "\n",
    "Sprva ustvarimo splošni Loss razred. Ker izračunamo **loss** vrednosti večih sampl-ov naeankrat je potrebno končno vrnjeno loss vrednost povprečiti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db4169c",
   "metadata": {},
   "source": [
    "Nato sledi razred specifične loss funkcije. V našem primeru je to Categorical Crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bda6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "        print(\"Correct confidencesa:\")\n",
    "        print(correct_confidences)\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "#class_targets = np.array([0, 2, 2])\n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 1, 0]])\n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3cbbd2",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd28ebab",
   "metadata": {},
   "source": [
    "Celotna koda do sedaj.\n",
    "\n",
    "Izračunati znamo vrednosti nevronov, dodamo jim lahko aktivacijsko funkcijo in na koncu lahko dobimo izhodni rezultat. Vsem korakom skupaj se reče **ONE FORWARD PASS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67516d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "# <=== HERE ===>\n",
    "\n",
    "        \n",
    "\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "# <=== HERE ===>\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# <=== HERE ===>\n",
    "\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Let's see output of the first few samples:\n",
    "print(\"Predictions\")\n",
    "print(activation2.output[:5])\n",
    "\n",
    "# Perform a forward pass through loss function\n",
    "# it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd5207",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844e1688",
   "metadata": {},
   "source": [
    "Za konec dodajmo še preverjanje natančnosti (*angl.* **accuracy**). To je preprosto % vrednost koliko pravilnih napovedi je napravil model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc5dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "np.random.seed(2020)\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "\n",
    "# Perform a forward pass through loss function\n",
    "# it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "\n",
    "\n",
    "# <=== HERE ===>\n",
    "\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "plt.show()\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514103b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2a5b247",
   "metadata": {},
   "source": [
    "# Optimizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4aea2c",
   "metadata": {},
   "source": [
    "Sedaj znamo ustvariti model, kateri se inicializira z random weights in biases z vrednostimi 0. Znamo ocenit rezultate našega modela.\n",
    "\n",
    "Naslednji korak je izboljšati naš model. To pomeni spremeniti naše weights in biases tako, da bo končna loss vrednost bližje 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e675e771",
   "metadata": {},
   "source": [
    "## Random search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2940a",
   "metadata": {},
   "source": [
    "Začnimo z *random search* pristopom:\n",
    "\n",
    "1. Ustvarili bomo model z random inicializiranimi weights in biases\n",
    "2. Izračunali bomo njegovo loss vrednost\n",
    "3. Ustvarili bomo nov model z random inicializiranimi weights in biases\n",
    "4. Izračunali bomo loss vrednost novega modela\n",
    "5. 1. Če ima manjšo loss vrednost bomo obdržali novi model\n",
    "5. 2. Če ima večjo loss vrednost bomo obdržali stari model\n",
    "6. Ponovimo od 3. koraka naprej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4759b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "        \n",
    "# Create dataset\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "\n",
    "# Create model\n",
    "dense1 = Layer_Dense(2, 3)  # first dense layer, 2 inputs\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Helper variables\n",
    "lowest_loss = 9999999  # some initial value\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "for iteration in range(2_000):\n",
    "\n",
    "    # Generate a new set of weights for iteration\n",
    "    dense1.weights = 0.05 * np.random.randn(2, 3)\n",
    "    dense1.biases = 0.05 * np.random.randn(1, 3)\n",
    "    dense2.weights = 0.05 * np.random.randn(3, 3)\n",
    "    dense2.biases = 0.05 * np.random.randn(1, 3)\n",
    "\n",
    "    # Perform a forward pass of the training data through this layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Perform a forward pass through loss function\n",
    "    # it takes the output of second dense layer here and returns loss\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    # If loss is smaller - print and save weights and biases aside\n",
    "    if loss < lowest_loss:\n",
    "        print('New set of weights found, iteration:', iteration,\n",
    "              'loss:', loss, 'acc:', accuracy)\n",
    "        best_dense1_weights = dense1.weights.copy()\n",
    "        best_dense1_biases = dense1.biases.copy()\n",
    "        best_dense2_weights = dense2.weights.copy()\n",
    "        best_dense2_biases = dense2.biases.copy()\n",
    "        lowest_loss = loss\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ab9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f45ae",
   "metadata": {},
   "source": [
    "Vidimo, da takšno iskanje optimalni parametrov ni učinkovito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e55d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1255be6c",
   "metadata": {},
   "source": [
    "## Fraction Change\n",
    "\n",
    "Namesto, da spremenimo vse weights in biases bomo vsak parameter spremenili za neko majhno število. Če se bo model izkazal za boljšega bomo to spremembo obdržali, če se bo model izkazal za slabšega bomo to spremembo zavrgli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "        \n",
    "# Create dataset\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "\n",
    "# Create model\n",
    "dense1 = Layer_Dense(2, 3)  # first dense layer, 2 inputs\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "# Helper variables\n",
    "lowest_loss = 9999999  # some initial value\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "for iteration in range(2_000):\n",
    "    # <=== HERE ===>\n",
    "    # Generate a new set of weights for iteration\n",
    "    dense1.weights += 0.05 * np.random.randn(2, 3)\n",
    "    dense1.biases += 0.05 * np.random.randn(1, 3)\n",
    "    dense2.weights += 0.05 * np.random.randn(3, 3)\n",
    "    dense2.biases += 0.05 * np.random.randn(1, 3)\n",
    "    # <=== HERE ===>\n",
    "\n",
    "    # Perform a forward pass of the training data through this layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # it takes the output of second dense layer here and returns loss\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    # If loss is smaller - print and save weights and biases aside\n",
    "    if loss < lowest_loss:\n",
    "        print('New set of weights found, iteration:', iteration,\n",
    "              'loss:', loss, 'acc:', accuracy)\n",
    "        best_dense1_weights = dense1.weights.copy()\n",
    "        best_dense1_biases = dense1.biases.copy()\n",
    "        best_dense2_weights = dense2.weights.copy()\n",
    "        best_dense2_biases = dense2.biases.copy()\n",
    "        lowest_loss = loss\n",
    "    # Revert weights and biases\n",
    "    else:\n",
    "        dense1.weights = best_dense1_weights.copy()\n",
    "        dense1.biases = best_dense1_biases.copy()\n",
    "        dense2.weights = best_dense2_weights.copy()\n",
    "        dense2.biases = best_dense2_biases.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d55f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "# To je za TRAINING VALIDATION\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac8f946",
   "metadata": {},
   "source": [
    "Model smo izboljšali. Loss se je opazno zmanjšala in accuracy je opazno narastla.\n",
    "\n",
    "Vendar sta to loss in accuracy gledano na training dataset-u.\n",
    "\n",
    "Za bolj natančno predstavo si poglejmo rezultate na novem - testnem datasetu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa9d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "\n",
    "# Perform a forward pass of the training data through this layer\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c266abbf",
   "metadata": {},
   "source": [
    "Loss vrednost training dataseta je blizu loss vrednosti testing dataseta, kar pomeni, da je model dobro generaliziral svoje znanje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60fb2f",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e145d4f",
   "metadata": {},
   "source": [
    "Preverimo naš model na še bolj zapletenem datasetu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc2624",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Copyright (c) 2015 Andrej Karpathy\n",
    "# License: https://github.com/cs231n/cs231n.github.io/blob/master/LICENSE\n",
    "# Source: https://cs231n.github.io/neural-networks-case-study/\n",
    "def spiral_data(samples, classes):\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, samples)\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, samples) + np.random.randn(samples)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y\n",
    "\n",
    "X, y = spiral_data(100, 3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ff2ec4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "# <=== HERE ===>       \n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# <=== HERE ===>\n",
    "\n",
    "\n",
    "# Create model\n",
    "dense1 = Layer_Dense(2, 3)  # first dense layer, 2 inputs\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "# Helper variables\n",
    "lowest_loss = 9999999  # some initial value\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "for iteration in range(2_000):\n",
    "\n",
    "    # Generate a new set of weights for iteration\n",
    "    dense1.weights += 0.05 * np.random.randn(2, 3)\n",
    "    dense1.biases += 0.05 * np.random.randn(1, 3)\n",
    "    dense2.weights += 0.05 * np.random.randn(3, 3)\n",
    "    dense2.biases += 0.05 * np.random.randn(1, 3)\n",
    "\n",
    "    # Perform a forward pass of the training data through this layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # it takes the output of second dense layer here and returns loss\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    # If loss is smaller - print and save weights and biases aside\n",
    "    if loss < lowest_loss:\n",
    "        print('New set of weights found, iteration:', iteration,\n",
    "              'loss:', loss, 'acc:', accuracy)\n",
    "        best_dense1_weights = dense1.weights.copy()\n",
    "        best_dense1_biases = dense1.biases.copy()\n",
    "        best_dense2_weights = dense2.weights.copy()\n",
    "        best_dense2_biases = dense2.biases.copy()\n",
    "        lowest_loss = loss\n",
    "    # Revert weights and biases\n",
    "    else:\n",
    "        dense1.weights = best_dense1_weights.copy()\n",
    "        dense1.biases = best_dense1_biases.copy()\n",
    "        dense2.weights = best_dense2_weights.copy()\n",
    "        dense2.biases = best_dense2_biases.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN veziji\n",
    "'''\n",
    "\n",
    "# To je za TRAINING VALIDATION\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571a2bdf",
   "metadata": {},
   "source": [
    "Preverimo še testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Perform a forward pass of the training data through this layer\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "print(f\"Train Loss: {loss}, Train Accuracy: {accuracy}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17886c07",
   "metadata": {},
   "source": [
    "Model se ne odnese dobro na bolj zapletenem datsetu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3836c",
   "metadata": {},
   "source": [
    "Problem lahko rešimo tako, da parametre ne posodabljamo z random številko ampak jih posodabljamo glede na to kolikšen vpliv so imeli na loss funkcijo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d3828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b495c40c",
   "metadata": {},
   "source": [
    "# Partial derivatieves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16577fa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "O tem kakšen vpliv ima določena spremenljivka oziroma parameter na rezultat funkcije nam povejo parcialni odvodi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa74221b",
   "metadata": {},
   "source": [
    "## Partial derivative for 1 neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f7078e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Za začetek si bomo pogledali na primeru enega nevrona, ki je povezan na tri nevrone iz prejšnje plasti.\n",
    "\n",
    "Cilj je posodobiti weights in bias nevrona tako, da zmanjšamo vrednost njegove aktivacijske funkcije. To v praksi nima nobenega pomena, saj želimo zmanjšati loss funkcijo ampak je primer namenjen lažjemu razumevanju."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e0932",
   "metadata": {},
   "source": [
    "![Partial derivatives of 1 neuron](images/07.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f57bc72",
   "metadata": {},
   "source": [
    "Enačba enega neurona je sledeča:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c8da5",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "$\n",
    "\\Large output = ReLU(z) =  \n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\tz  & z > 0 \\\\\n",
    "        0  & z \\leq 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "\\\\\n",
    "\\Large z = w_0 \\cdot i_0 + w_1 \\cdot i_1 + w_2 \\cdot i_2 + b \\\\\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380aa9d4",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad11ea",
   "metadata": {},
   "source": [
    "Parcialni odvodi naših parametrov so sedaj:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b69080",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "$\n",
    "\\Large d\\_value \\cdot \\frac{dReLU}{dz} \\cdot \\frac{\\partial z}{\\partial p_j}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd94a583",
   "metadata": {},
   "source": [
    "* $d\\_value$ predstavlja vrednost, kako močno je ta neuron vplival na naslednjo plast\n",
    "* $p_j$ predstavlja parameter katerega vpliv iščemo. Lahko je to specifičnna weight oziroma bias, lahko pa je tudi input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf7e66",
   "metadata": {},
   "source": [
    "![d_value](images/08.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd71f9",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613716a",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "Parcialni odvod ReLu:\n",
    "    \n",
    "$\\Large \\frac{dReLU}{dz} =\n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t\\frac{d}{dz}z  & z > 0 \\\\\n",
    "        \\frac{d}{dz}0  & z \\leq 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "= \\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t1  & z > 0 \\\\\n",
    "        0  & z \\leq 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f692d4",
   "metadata": {},
   "source": [
    "![dReLU_dz](images/09.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94809d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0]  # input values\n",
    "w = [-3.0, -1.0, 2.0]  # weights\n",
    "b = 1.0  # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(\"z value: \", z)\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(\"ReLU value: \", y)\n",
    "\n",
    "# The derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(\"dReLU value: \", drelu_dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6a460",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df947f82",
   "metadata": {},
   "source": [
    "Parcialni odvod uteži:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941db252",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "Parcialni odvod uteži:\n",
    "    \n",
    "$\\Large \\frac{\\partial z}{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\cdot w_j i_j + \\frac{\\partial}{\\partial w_j} w_{j+1} i_{j+1} + \\frac{\\partial}{\\partial w_j} w_{j+2} i_{j+2} + \\frac{\\partial}{\\partial w_j} b = i_j + 0 + 0 + 0 = i_j $\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ade30",
   "metadata": {},
   "source": [
    "![dsum_dw](images/10.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f91f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0]  # input values\n",
    "w = [-3.0, -1.0, 2.0]  # weights\n",
    "b = 1.0  # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(\"z value: \", z)\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(\"ReLU value: \", y)\n",
    "\n",
    "# The derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(\"dReLU value: \", drelu_dz)\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Partial derivatives of the summation and multiplication, the chain rule\n",
    "dsum_dw0 = drelu_dz * x[0]\n",
    "dsum_dw1 = drelu_dz * x[1]\n",
    "dsum_dw2 = drelu_dz * x[2]\n",
    "\n",
    "print(\"dw0 value: \", dsum_dw0)\n",
    "print(\"dw1 value: \", dsum_dw1)\n",
    "print(\"dw2 value: \", dsum_dw2)\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22784093",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c94b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9aebc5a",
   "metadata": {},
   "source": [
    "Pacrialni odvod bias-a:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58992d69",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "Pacrialni odvod bias-a:\n",
    "    \n",
    "$\\Large \\frac{\\partial z}{\\partial b} = \\frac{\\partial}{\\partial b} \\cdot w_j i_j + \\frac{\\partial}{\\partial b} w_{j+1} i_{j+1} + \\frac{\\partial}{\\partial b} w_{j+2} i_{j+2} + \\frac{\\partial}{\\partial b} b = 0 + 0 + 0 + 1 = 1 $\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0439b",
   "metadata": {},
   "source": [
    "![dsum_db](images/11.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b5983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0]  # input values\n",
    "w = [-3.0, -1.0, 2.0]  # weights\n",
    "b = 1.0  # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(\"z value: \", z)\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(\"ReLU value: \", y)\n",
    "\n",
    "# The derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(\"dReLU value: \", drelu_dz)\n",
    "\n",
    "# Partial derivatives of the summation and multiplication, the chain rule\n",
    "dsum_dw0 = drelu_dz * x[0]\n",
    "dsum_dw1 = drelu_dz * x[1]\n",
    "dsum_dw2 = drelu_dz * x[2]\n",
    "\n",
    "print(\"dw0 value: \", dsum_dw0)\n",
    "print(\"dw1 value: \", dsum_dw1)\n",
    "print(\"dw2 value: \", dsum_dw2)\n",
    "\n",
    "# <=== HERE ===>\n",
    "dsum_db = 1\n",
    "print(\"db value: \", dsum_db)\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815045cf",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cb619a",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "Parcialni odvod inputa:\n",
    "\n",
    "$\\Large \\frac{\\partial z}{\\partial i_j} = \\frac{\\partial}{\\partial i_j} \\cdot w_j i_j + \\frac{\\partial}{\\partial i_j} w_{j+1} i_{j+1} + \\frac{\\partial}{\\partial i_j} w_{j+2} i_{j+2} + \\frac{\\partial}{\\partial i_j} b = w_j + 0 + 0 + 0 = w_j $\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b370d460",
   "metadata": {},
   "source": [
    "Za naš cilj je ta izračun nepotreben, saj posodabljamo le weights in bias našega neurona. Vendar pa parcialni odvodi glede na input spremenljivko predstavljajo $d\\_value$ vrednost pri računanju vrednosti naslednjih neuronov."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64846009",
   "metadata": {},
   "source": [
    "\n",
    "![dsum_i](images/12.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278deccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0]  # input values\n",
    "w = [-3.0, -1.0, 2.0]  # weights\n",
    "b = 1.0  # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(\"z value: \", z)\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(\"ReLU value: \", y)\n",
    "\n",
    "# The derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(\"dReLU value: \", drelu_dz)\n",
    "\n",
    "# Partial derivatives of the summation and multiplication, the chain rule\n",
    "dsum_dw0 = drelu_dz * x[0]\n",
    "dsum_dw1 = drelu_dz * x[1]\n",
    "dsum_dw2 = drelu_dz * x[2]\n",
    "\n",
    "print(\"dw0 value: \", dsum_dw0)\n",
    "print(\"dw1 value: \", dsum_dw1)\n",
    "print(\"dw2 value: \", dsum_dw2)\n",
    "\n",
    "dsum_db = 1\n",
    "print(\"db value: \", dsum_db)\n",
    "\n",
    "# <=== HERE ===>\n",
    "dsum_di0 = drelu_dz * w[0]\n",
    "dsum_di1 = drelu_dz * w[1]\n",
    "dsum_di2 = drelu_dz * w[2]\n",
    "\n",
    "print(\"di0 value: \", dsum_di0)\n",
    "print(\"di1 value: \", dsum_di1)\n",
    "print(\"di2 value: \", dsum_di2)\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771bbaec",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd22bc5",
   "metadata": {},
   "source": [
    "Sedaj, ko imamo izračunane naše vrednosti je potrebno le še posodobiti parametre. Pričakovan rezultat je, da se končna izhodna vrednost nevrona zmanjša.\n",
    "\n",
    "Parametre posodobimo tako, da jim prištejemo majhen del negativnega rezultata parcialnega odvoda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70677347",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "$ p_j = p_j + (-l_r \\cdot dp_j) $\n",
    "\n",
    "* $p_j$ je naš parameter katerega želimo posodobiti. Weights oziroma bias\n",
    "* $l_r$ je **learning rate**\n",
    "* $dp_j$ je vrednost parcialnega odvoda\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86a6f38",
   "metadata": {},
   "source": [
    "Prištejemo negativni rezultat saj želimo našo funkcijo zmanjšati do 0.\n",
    "\n",
    "**Learning rate** uporabimo, ker želimo parametre posodabljati le za neko majhno vrednost. V primeru prevelikega learning rate lahko zgrešimo 0 naše loss funkcije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef538588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0]  # input values\n",
    "w = [-3.0, -1.0, 2.0]  # weights\n",
    "b = 1.0  # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(\"z value: \", z)\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(\"ReLU value: \", y)\n",
    "\n",
    "# The derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(\"dReLU value: \", drelu_dz)\n",
    "\n",
    "# Partial derivatives of the summation and multiplication, the chain rule\n",
    "dsum_dw0 = drelu_dz * x[0]\n",
    "dsum_dw1 = drelu_dz * x[1]\n",
    "dsum_dw2 = drelu_dz * x[2]\n",
    "dsum_db = 1\n",
    "\n",
    "print(\"dw0 value: \", dsum_dw0)\n",
    "print(\"dw1 value: \", dsum_dw1)\n",
    "print(\"dw2 value: \", dsum_dw2)\n",
    "print(\"db value: \", dsum_db)\n",
    "\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Optimizing parameters\n",
    "w[0] += -0.001*dsum_dw0\n",
    "w[1] += -0.001*dsum_dw1\n",
    "w[2] += -0.001*dsum_dw2\n",
    "b += -0.001*dsum_db\n",
    "\n",
    "print()\n",
    "print(\"New weights: \", w)\n",
    "print(\"New bias: \", b)\n",
    "\n",
    "# Another forward pass. Excpecting a lower result now.\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(\"New z value: \", z)\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(\"New ReLU value: \", y)\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6822f9c7",
   "metadata": {},
   "source": [
    "Nova izhodna vrednost (ReLu value) je manjša kot začetna. Se pravi smo dosegli naš cilj."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9475e83d",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb56368a",
   "metadata": {},
   "source": [
    "Celotna koda za Dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90eb47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "        \n",
    "    # <=== HERE ===>\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "    # <=== HERE ===>\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # <=== HERE ===>\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "    # <=== HERE ===>\n",
    "\n",
    "        \n",
    "        \n",
    "inputs = np.array([[1.0, 2.0, 3.0, 2.5],\n",
    "                  [1.0, 2.0, 3.0, 2.5],\n",
    "                  [1.0, 2.0, 3.0, 2.5]])\n",
    "\n",
    "dense1 = Layer_Dense(4, 3)\n",
    "print(\"Dense 1 weights: \")\n",
    "print(dense1.weights)\n",
    "print(\"Dense 1 biases\")\n",
    "print(dense1.biases)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense1.forward(inputs)\n",
    "print(\"Dense 1 forward\")\n",
    "print(dense1.output)\n",
    "activation1.forward(dense1.output)\n",
    "print(\"ReLU 1 output:\")\n",
    "print(activation1.output)\n",
    "\n",
    "# <=== HERE ===>\n",
    "# d_values passed from next layer back to this layer\n",
    "d_values = np.array([[1.0, 1.0, 1.0],\n",
    "                    [1.0, 1.0, 1.0],\n",
    "                    [1.0, 1.0, 1.0]])\n",
    "\n",
    "activation1.backward(d_values)\n",
    "print(\"Activation 1 dinputs\")\n",
    "print(activation1.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "print(\"Dense 1 dweights\")\n",
    "print(dense1.dweights)\n",
    "print(\"Dense 1 dbias\")\n",
    "print(dense1.dbiases)\n",
    "print(\"Dense 1 dinputs\")\n",
    "print(dense1.dinputs)\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029058bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc2d2201",
   "metadata": {},
   "source": [
    "# Odvodi Loss FunctionS in Softmax layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7bbf47",
   "metadata": {},
   "source": [
    "Isti princip deluje na celotni mreži, le da takrat iščemo odvode glede na loss funkcijo.\n",
    "\n",
    "V našem primeru bomo še združili Softmax aktivacijsko funkcijo in Loss funkcijo, saj njuna odvoda skupaj vrneta preprosto enačbo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370c9d5a",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "$\\Large \\frac{\\partial L}{\\partial z_k} = \\hat{y_k} - y_k $\n",
    "\n",
    "* $L$ predstavlja categorical crossentropy loss funkcijo\n",
    "* $\\hat{y_k}$ predstavlja napovedano vrednost\n",
    "* $y_k$ predstavlja realno vrednost\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d7bbc",
   "metadata": {},
   "source": [
    "Vrednost izraza sedaj predstavlja $d\\_value$, ki je posredovana v neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    \n",
    "    # <=== HERE ===>\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    # <=== HERE ===>\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "# Let's see output of the first few samples:\n",
    "print(loss_activation.output[:5])\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)\n",
    "\n",
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "# Print gradients\n",
    "print(\"Partial derivatives\")\n",
    "print(\"Dense 1: dweights\")\n",
    "print(dense1.dweights)\n",
    "print(\"Dense 1: dbiases\")\n",
    "print(dense1.dbiases)\n",
    "print(\"Dense 2: dweights\")\n",
    "print(dense2.dweights)\n",
    "print(\"Dense 2: dbiases\")\n",
    "print(dense2.dbiases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8accdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b31dc227",
   "metadata": {},
   "source": [
    "# Optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3855a518",
   "metadata": {},
   "source": [
    "Ko imamo izračunane vse vrednosti lahko le-te uporabimo za posodobitev parametrov z namenom zmanjšanja loss funkcije.\n",
    "\n",
    "Za ta korak poskrbijo optimizerji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b1cb6",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD) Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e4fde0",
   "metadata": {},
   "source": [
    "Za začetek si bomo pogledali splošni SGD Optimizer. Pri tem parametre posodobimo tako, da jim odštejemo majhen del njihove vrednosti odvoda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffefd65",
   "metadata": {},
   "source": [
    "<div style=\"background: lightgreen\">\n",
    "$\\Large p_i = p_i - l_r \\cdot d\\_p_i$\n",
    "\n",
    "* $p_i$ je parameter katerega želimo posodobiti. Weights oziroma bias\n",
    "* $l_r$ je **learning rate**, ki nadzira kako velike posodobitve parametrev naredimo z enim korakom učenja\n",
    "* $d\\_p_i$ je parcialni odvod loss funkcije glede na parameter $p_i$\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4514fde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3f344",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e31572",
   "metadata": {},
   "source": [
    "Celotna koda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v  CLEAN verziji\n",
    "'''\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# <=== HERE ===>\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "# <=== HERE ===>\n",
    "        \n",
    "        \n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD()\n",
    "# <=== HERE ===>\n",
    "\n",
    "\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "# Let's see output of the first few samples:\n",
    "print(loss_activation.output[:5])\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)\n",
    "\n",
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "# Print gradients\n",
    "#print(dense1.dweights)\n",
    "#print(dense1.dbiases)\n",
    "#print(dense2.dweights)\n",
    "#print(dense2.dbiases)\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Update weights and biases\n",
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)\n",
    "\n",
    "\n",
    "\n",
    "# And if we make anothe forward pass with same data we should have lowered the loss\n",
    "\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "# Let's see output of the first few samples:\n",
    "print(loss_activation.output[:5])\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9974b55",
   "metadata": {},
   "source": [
    "Sedaj nam je uspelo sestaviti našo osnovno nevronsko mrežo.\n",
    "\n",
    "Opravimo lahko en forward pass, kjer dobimo naše napovedane vrednosti. Nato izračunamo našo vrednost loss funkcije.\n",
    "\n",
    "To vrednost nato uporabimo za posodabljanje parametrov nevronov. Za vsak parameter izračunamo njegov vpliv na vrednost loss funkcije in ga s pomočjo optimizerja posodobimo za majhen korak. Tako sedaj opravimo še **EN BACKWARD PASS** in s tem opravimo celoten postopek učenja naše nevronske mreže."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f3232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f5105ba",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42475718",
   "metadata": {},
   "source": [
    "# Training in epochos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc99c4",
   "metadata": {},
   "source": [
    "Naš model smo v prejšnjem primeru učili na celotnem naboru podatkov. Uspelo nam je zmanjšati loss vrednost, vendar končni rezultat še ni zadovoljiv.\n",
    "\n",
    "Kar lahko naredimo je, da model treniramo večkrat na istem datasetu.\n",
    "\n",
    "Vsakič, ko uporabilo celoten dataset za treniranje se temu reče **epoch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da9257",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    \n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.5f}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab59ab4e",
   "metadata": {},
   "source": [
    "Loss vrednost smo zmanjšali, vendar ne za veliko.\n",
    "\n",
    "Za izboljšanje modela je potrebno spreminjanje hiperparametrov (*angl.* **hyperparameter**) kot so število plasti, število nevronov, learning rate, itd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e286fe6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 128)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(128, 3)\n",
    "# <=== HERE ===>\n",
    "\n",
    "\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate=0.85)\n",
    "# <=== HERE ===>\n",
    "\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe11dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "461637f7",
   "metadata": {},
   "source": [
    "Poglejmo si še na testnem datasetu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3900e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=500, alpha=0.6)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"o\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d736eb88",
   "metadata": {},
   "source": [
    "Pri velikem številu plasti in nevronov je potrebno paziti, da si model ne prične zapomniti training dataseta temveč generalizira svoje znanje. Da si je model zapomnil podatke opazimo v veliki razliki med training validation in testing validation vrednostjo.\n",
    "\n",
    "Pri spreminjanju learning rate je potebno paziti kako le-ta vpliva na proces učenja. \n",
    "> S preveliko vrednostjo opravljamo večje posodobitve naših parametrov. To pomeni, da se bomo hitreje približevali ničli naše loss funkcije, vendar jo lahko zaradi prevelikih korakov zgrešimo oziroma nikoli ne dosežemo. Prevelik learning rate se opazi v nestabilnem učenju oziroma v skrajnih primerih v povečanju loss vrednosti.\n",
    "\n",
    "> S premajhno vrednostjo opravljamo majhne posodobitve naših parametrov. To pomeni, da se bomo počasi približevali ničli naše loss funkcije, kar podaljša naš čas učenja. Paziti je tudi potrebno, ker se z majhnim learning rate lahko ujamemo v lokalne minimume naše loss funkcije. Tako dosežemo neko majhno loss vrednost, vendar pa resnične ničle loss funkcije ne bomo dosegli.\n",
    "\n",
    "Za odpravljanje learning rate problemov se uporabljajo principi kot so **learning rate decay**, kjer treniranje modela pričnemo z visokim learning rate, katerega počasi zmanjšujemo. Pri optimizerjih se uporablja tudi princip **SGD with momentum**, kjer parametre posodabljamo še glede na to za koliko smo jih posodobili v prejšnjih korakih.\n",
    "\n",
    "Za kontroliranje vrednosti parametrov se uporabljajo še principi **L1 and L2 Regularization** kjer parametre z večjimi vrednostmi posodobimo za večje število. Tako se želi preprečiti eksplozijo vrednosti enega parametra in se skuša ohraniti vrednosti parametrov na istem nivoju. Na tak način pri učenju sodeluje večje število nevronov in generalizacija znanja je boljša. Za isti namen se uporablja tudi princip **Dropout**, kjer med učenjem ugasnemo določen majhen % nevronov v mreži."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc3798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "347d36a8",
   "metadata": {},
   "source": [
    "# Realni primer - uporaba Keras in Tensorflow knjižnjic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd36988f",
   "metadata": {},
   "source": [
    "Za naš primer bomo uporabili knjižnjici Tensorflow in Keras.\n",
    "\n",
    "Tensorflow je open source knjižnjica ustvarjena s strani Google Brain ekipe namenjena računanju s tensorji.\n",
    "> https://www.tensorflow.org/\n",
    "\n",
    "Keras je machine learning knjižnjica zgrajena na tensorflow knjižnjici namenjena lažjemu ustvarjanju nevronskih mrež.\n",
    "> https://keras.io/getting_started/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e9b6d",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32332410",
   "metadata": {},
   "source": [
    "Za začetek bomo uvozili knjižnjice.\n",
    "\n",
    "* Sequential opisuje model sestavljanja nevronske mreže\n",
    "* Dense predstavlja naš dense layer, kjer so vsi nevroni ene plasti povezani na nevrone naslednje plasti\n",
    "* Adam je naš optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f077915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first neural network with keras tutorial\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0047f8",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d2e562",
   "metadata": {},
   "source": [
    "Za naš primer bomo uporabili *Prima Indians onset of diabetes dataset*. Opisuje zdravstvene podatke pacientov Prima Indians porekla in napoveduje ali so imeli diabetes v roku 5 let ali ne.\n",
    "\n",
    "Dataset je namenjen binarni klasifikaciji:\n",
    "* **1** - So dobili diabetes\n",
    "* **0** - Niso dobili diabetes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaja v CLEAN verziji\n",
    "'''\n",
    "\n",
    "columns = [\n",
    "    \"Number of times pregnant\",\n",
    "    \"Plasma glucose concentration a 2 hours in an oral glucose tolerance test\",\n",
    "    \"Diastolic blood pressure (mm Hg)\",\n",
    "    \"Triceps skin fold thickness (mm)\",\n",
    "    \"2-Hour serum insulin (mu U/ml)\",\n",
    "    \"Body mass index (weight in kg/(height in m)^2)\",\n",
    "    \"Diabetes pedigree function\",\n",
    "    \"Age (years)\",\n",
    "    \"Class variable\"\n",
    "]\n",
    "data = pd.read_csv(\"data/pima-indians-diabetes.data.csv\", names=columns)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68929a8",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da0fe3",
   "metadata": {},
   "source": [
    "Poglejmo osnovne informacije o našem datasetu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f7e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9323e",
   "metadata": {},
   "source": [
    "Dataset nima null vrednosti, torej ga ni potrebno dodatno očistiti.\n",
    "\n",
    "Vrednosti so številčne, torej ni potrebno pretvarjati stringe.\n",
    "\n",
    "V informacijah o našem datasetu vidimo, da sta razreda razdeljena na sledeč način:\n",
    "* Vzorcev za razred **1** imamo 500, oziroma 65.1%\n",
    "* Vzorcev za razred **0** imamo 268, oziroma 34.9%\n",
    "\n",
    "To pomeni, da naš dataset **ni balanced**, kar bo lahko predstavljalo izvor slabega učenja. Model se lahko preprosto nauči zmeraj napovedati razred 1 in tako doseže 65.1% accuracy. Ker imamo premajhen dataset bomo pustil tako kot je."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d602f9",
   "metadata": {},
   "source": [
    "## Data scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f0abf",
   "metadata": {},
   "source": [
    "Nevronske mreže delujejo najboljše, če so vhodni podatki okoli 0 do 1 oziroma -1 do 1, zato bomo naše podatke skalirali\n",
    "\n",
    "Pri skaliranju je potrebno paziti, da uporabimo isto metodo skaliranja na **training dataset** in **testing dataset**. Pomembno je tudi, da **znanje o skaliranju izvira le iz training dataset-a**.\n",
    "> V praksi to pomeni sledeče - imamo training dataset z vhodnimi vrednostmi $\\Large [0,1,2,3,4,5]$ in testing dataset z vhodnimi vrednostmi $\\Large [0,2,4,6,8]$. Podatke želimo normirati, da se nahajajo znotraj intervala 0 in 1. Naša scaling metoda bo delovala po enačbi: $\\Large x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$, kjer sta $\\Large x_{min} = 0$ in $\\Large x_{max}=5$. Če bi uporabili vrednost $8$ bi tako nehote vpeljali znanje o testing datasetu v naše training podatke. Končne vrednost vhodnih podatkov se sedaj ne bodo nahajale med 0 in 1 ampak to ne bi smel biti velik problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80654993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_samples = data.shape[0]\n",
    "train_samples = 0.8 # % of how much samples to have in the training dataset\n",
    "\n",
    "X_train = data.iloc[:int(num_samples*train_samples), :-1]\n",
    "y_train = data.iloc[:int(num_samples*train_samples), -1]\n",
    "X_test = data.iloc[int(num_samples*train_samples):, :-1]\n",
    "y_test = data.iloc[int(num_samples*train_samples):, -1]\n",
    "\n",
    "scaling_min = X_train.min()\n",
    "scaling_max = X_train.max()\n",
    "\n",
    "def data_scaling(data, scaling_min, scaling_max):\n",
    "    return (data-scaling_min)/(scaling_max - scaling_min)\n",
    "\n",
    "X_train = data_scaling(X_train, scaling_min, scaling_max)\n",
    "X_test = data_scaling(X_test, scaling_min, scaling_max)\n",
    "\n",
    "X_train.describe() # More met vse min na 0 in vse max na 1\n",
    "# X_test.describe() ne bomo nč vn razbral ker ne rabi met od 0 do 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc1dcb9",
   "metadata": {},
   "source": [
    "## Creating Neural Network Model\n",
    "\n",
    "Za začetek ustvarimo *Sequence* model in nato dodajamo plasti eno za drugo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d6553",
   "metadata": {},
   "source": [
    "Kot prvo bomo ustvarili naš *input layer*, ki ima 8 nevronov, za naših 8 značilk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e453ec2",
   "metadata": {},
   "source": [
    "Za arhitekturo našega modela bomo uporabili 3 *dense layers*, kjer imata prvi 2 plasti aktivacijsko funkcijo *ReLU*, zadnja pa *Sigmoid*, ker imamo primer binarne klasifikacije. Vrednosti manjše od 0.5 bodo razred 0, vrednosti večje od 0.5 bodo razred 1.\n",
    "\n",
    "[Dense documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?hl=en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(32, input_dim=8, activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92555733",
   "metadata": {},
   "source": [
    "Nato bomo naš model zaključili s tem, da mu podamo *loss function* in *optimizer*.\n",
    "\n",
    "[Compile documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf018796",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ef9eb",
   "metadata": {},
   "source": [
    "Da vidimo celotno arhitekturo modela uporabimo *model.summary()* funkcijo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ecbc4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c18e2",
   "metadata": {},
   "source": [
    "Model treniramo s **fit()** metodo v kateri lahko definiramo velikost *batch* in število *epochs*.\n",
    "\n",
    "[Fit documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3196d0",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a937794",
   "metadata": {},
   "source": [
    "Znanje modela bomo preverili s pomočjo test dataset-a.\n",
    "\n",
    "[Evaluate documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78037f",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Model lahko uporabimo za predikcijo novih podatkov s pomočjo *predict()* metode, kjer dobimo vrednosti zadnje plasti.\n",
    "\n",
    "Oziroma v našem primeru lahko uporabimo *predict_classes()* metodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3814b0e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"{X_test.iloc[i, :]}.\")\n",
    "    print(30*\"-\")\n",
    "    print(f\"PREDICTION: {predictions[i][0]}, \\t REAL: {y_test.iloc[i]}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480947d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3353c6a4",
   "metadata": {},
   "source": [
    "# Save model\n",
    "\n",
    "Na koncu lahko model shranimo.\n",
    "\n",
    "Keras model je sestavljen iz večih kompoment:\n",
    "* Arhitektura, ki specificira sosledje plasti in število nevronov v plasteh ter način kako so povezane med seboj\n",
    "* Vrednosti weights in biases\n",
    "* Optimizer in njegovih vrednosti (kot so trenutna vrednost learning rate spremenljivke, itd.)\n",
    "* Vrednosti in stanje loss funkcije\n",
    "\n",
    "Shranimo lahko celotni model ali le določene dele. Standardna praksa je shraniti celotni model v TensorFlow SavedModel arhiv (oziroma v starejšem, Keras H5 formatu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/diabetes_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe633a",
   "metadata": {},
   "source": [
    "Da naložimo naš model uporabimo *model.load_model(\"location\")*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a161f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model2 = load_model(\"models/diabetes_model.h5\")\n",
    "\n",
    "\n",
    "predictions = model2.predict(X_test)\n",
    "predictions = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"{X_test.iloc[i, :]}.\")\n",
    "    print(30*\"-\")\n",
    "    print(f\"PREDICTION: {predictions[i][0]}, \\t REAL: {y_test.iloc[i]}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b0a4d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766faf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "118bcb15",
   "metadata": {},
   "source": [
    "Poglejmo si sedaj še druge, bolj napredne modele. Eden izmed takih je model za *Image Recognition*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51683107",
   "metadata": {},
   "source": [
    "# Image Recognition and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4305f1da",
   "metadata": {},
   "source": [
    "*Image Recognition* opisuje proces kjer neko sliko podamo v neural network model in ta za sliko izpiše določen label (pes, mačka, človek, avto, ...). Labels katere model lahko izpiše so v naprej definirani znotraj modela.\n",
    "\n",
    "Če imamo za sliko na izbiro samo 1 class temu rečemo *Image Recognition*. Če lahko izbiramo med večimi razredi temu rečemo *Image Classification*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efead49d",
   "metadata": {},
   "source": [
    "V našem primeru bomo uporabili knjižnjici TensorFlow in Keras, da bomo ustvarili model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1323a26c",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4263e8b",
   "metadata": {},
   "source": [
    "Da lahko naš model prepozna kaj je prikazano na sliki mora izvesti *feature extraction*. *Features* so elementi kateri naš model zares zanimajo. V našem primeru bi to bilo zaznavanje robov, zaznavanje splošnih oblik in likov, itd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab555b2d",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5261ade",
   "metadata": {},
   "source": [
    "Ta proces *feature extraction* lahko dosežemo z uporabo **Convolutional Layer** in tako dobimo *Convolutional Neural Network (CNN)*.\n",
    "\n",
    "Convolutional Layer deluje tako, da vzeme več pixlov naše slike naenkrat in nad njimi uporabi določen filter. Nato se filter premakne naprej po sliki in ponovno izvede filtracijo. Na koncu dobimo naš *feature map*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb96657",
   "metadata": {},
   "source": [
    "![CNN layer](images/conv-full-layer.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d93c303",
   "metadata": {},
   "source": [
    "Taka filtracija ni nič novega. Ponavadi so filtre izdelovali ročno, kot je recimo 3x3 filter za zaznavanje vertikalnih črt.\n",
    "\n",
    "```\n",
    "0, 1, 0\n",
    "0, 1, 0\n",
    "0, 1, 0\n",
    "```\n",
    "\n",
    "Z uporabo CNN pa se naš model sam nauči kateri filteri so najbolj primerni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d9422",
   "metadata": {},
   "source": [
    "Po končani filtraciji ponavadi sledi še **Pooling layer**, ki nam zmanjša število informacij, ki predstavljajo našo sliko. Ko recimo sami gledamo sliko nas ponavadi ne zanima kaj vse se dogaja v ozadju ampak smo fokusirani na glavni objekt slike (žival, človek, itd..). Podobno nam pomaga naš Pooling Layer, ki se nauči odstraniti nepotrebne dele naše slike. Tako napravi celoten model bolj fleksibilen in pomaga pri overfittingu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d883ed",
   "metadata": {},
   "source": [
    "![pooling layer](images/pooling_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4597d8b9",
   "metadata": {},
   "source": [
    "Obstaja več načinov pooling-a vendar je najbolj uporabljen **max pooling**, kjer obdržimo le maximalno vrednost znotraj našega filtra. Tako se efektivno znebimo 3/4 informacij (če uporabljamo 2x2 filter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd1e3b1",
   "metadata": {},
   "source": [
    "Nato sledijo še *Dense* plasti katere služijo združevanju različnih featurjev in v dejanskem prepoznavanju objektov. Zadnja plast nevronov nato predstavlja naše razrede in vrednost posameznega nevrona ponavadi predstavlja % kako siguren je model ali slika spada v določen razred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e110d702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cb8541d",
   "metadata": {},
   "source": [
    "# Image classification - CIFRA-10 example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb0b112",
   "metadata": {},
   "source": [
    "Pa si sedaj poglejmo to na dejanskem primeru.\n",
    "\n",
    "S pomočjo Tensorflow in Keras knjižnjice bomo ustvarili CNN model. Odločiti se moramo glede števila različnih plasti, glede števila nevronov v posamezni plasti, glede velikosti filtrov, itd. Optimalne poti pri izbiranju še ni, tako da to izbiranje pride z izkušnjami."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16d337",
   "metadata": {},
   "source": [
    "Za treniranje bomo potrebovali dataset slik, ki so pravilno označene. Mi bomo uporabili [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f1205",
   "metadata": {},
   "source": [
    "![CIFAR-10](images/cifar_10_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b589d5eb",
   "metadata": {},
   "source": [
    "CIFAR-10 vsebuje 60 000 slik in 10 različnih objekotv. Slike so velike 32 x 32 in vsebujejo RGB informacijo.\n",
    "\n",
    "Dataset pride skupaj z Keras knjižnjico zato ga ni potrebno prenašati lastnoročno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b89909",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086cadf",
   "metadata": {},
   "source": [
    "Za začetek importirajmo naše knjižnjice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bd0a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e6042",
   "metadata": {},
   "source": [
    "Dodajmo še random seed, da bodo rezultati ponovljivi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258c0940",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Koda se nahaj v CLEAN verziji\n",
    "'''\n",
    "seed = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd4251a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e300b1",
   "metadata": {},
   "source": [
    "Naložimo sedaj naš dataset in prikažimo nekaj slik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad811a1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (50000, 32, 32, 3) [[[ 59  62  63]\n",
      "  [ 43  46  45]\n",
      "  [ 50  48  43]\n",
      "  ...\n",
      "  [158 132 108]\n",
      "  [152 125 102]\n",
      "  [148 124 103]]\n",
      "\n",
      " [[ 16  20  20]\n",
      "  [  0   0   0]\n",
      "  [ 18   8   0]\n",
      "  ...\n",
      "  [123  88  55]\n",
      "  [119  83  50]\n",
      "  [122  87  57]]\n",
      "\n",
      " [[ 25  24  21]\n",
      "  [ 16   7   0]\n",
      "  [ 49  27   8]\n",
      "  ...\n",
      "  [118  84  50]\n",
      "  [120  84  50]\n",
      "  [109  73  42]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[208 170  96]\n",
      "  [201 153  34]\n",
      "  [198 161  26]\n",
      "  ...\n",
      "  [160 133  70]\n",
      "  [ 56  31   7]\n",
      "  [ 53  34  20]]\n",
      "\n",
      " [[180 139  96]\n",
      "  [173 123  42]\n",
      "  [186 144  30]\n",
      "  ...\n",
      "  [184 148  94]\n",
      "  [ 97  62  34]\n",
      "  [ 83  53  34]]\n",
      "\n",
      " [[177 144 116]\n",
      "  [168 129  94]\n",
      "  [179 142  87]\n",
      "  ...\n",
      "  [216 184 140]\n",
      "  [151 118  84]\n",
      "  [123  92  72]]]\n",
      "Y: (50000, 1) [6]\n"
     ]
    }
   ],
   "source": [
    "# Loading in the data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print(\"X:\", X_train.shape, X_train[0])\n",
    "print(\"Y:\", y_train.shape, y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6082d524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(15, 5))\n",
    "ax1.imshow(X_train[1])\n",
    "ax2.imshow(X_train[2])\n",
    "ax3.imshow(X_train[3])\n",
    "ax4.imshow(X_train[4])\n",
    "ax5.imshow(X_train[5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a70b3",
   "metadata": {},
   "source": [
    "Naši podatki so večinoma vredu in jih ne potrebujemo pretirano pre-procesirati. Kar bomo edino storili je, da jih normaliziramo.\n",
    "\n",
    "Ker so vrednosti pixlov shranjene kot številke od 0 do 255 jih bomo preprosto delili s 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51ca4b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the inputs from 0-255 to between 0 and 1 by dividing by 255\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b605c1",
   "metadata": {},
   "source": [
    "Dodatno bomo še one-hot encod-ali naše razrede in shranili število razredov, ker želimo toliko nevronov v naši zadnji plasti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e991f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "class_num = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28592e04",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69667c08",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba66442",
   "metadata": {},
   "source": [
    "Ustvarimo sedaj naš model.\n",
    "\n",
    "Keras ima veliko različnih načrtov za izdelavo modelov ampak med najbolj uporabljenimi je Sequential. Začeli bomo z njim in mu počasi dodajali eno in po eno plast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d33612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376bd3e0",
   "metadata": {},
   "source": [
    "Prva plast, ki jo bomo dodali je Convolution layer z 32 različnimi filtri in velikostjo filtra 3x3. `Padding=\"same\"` pove, da velikosti slik ne bomo spreminjali. Na koncu dodamo še activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e790cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))\n",
    "model.add(keras.layers.Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c120e3af",
   "metadata": {},
   "source": [
    "Nato bomo dodali Dropout Layer, ki nam pomaga preprečevati overfitting. Vrednost 0.2 pomeni, da tekom vsakega koraka treniranja ignoriramo random 20% inputov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71d7a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24734a1d",
   "metadata": {},
   "source": [
    "Nato dodamo še BatchNormalization Layer, ki normalizira inpute v naslednji layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33fe421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80662a8",
   "metadata": {},
   "source": [
    "Dodajmo sedaj še en set plasti. Praksa je, da globje kot gremo bolj se veča število filtrov v našem Conv2D layer-ju. S tem omogočamo modelu učenje bolj kompleksnih representacij v slikah.\n",
    "\n",
    "Dodatno tukaj dodajmo še MaxPooling2D layer vendar pazimo, da jih ne dodamo preveč, saj Pooling Layer odstrani informacije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75917e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D(2))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e338e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D(2))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "model.add(keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd293c6",
   "metadata": {},
   "source": [
    "Predno preidemo iz naših convolitonal plasti v Dense plasti moramo uporabiti Flatten plast, ki zagotovi, da bodo inputi v obliki enodimenzionalnega vektorja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bad003c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5516f5c4",
   "metadata": {},
   "source": [
    "Dodajmo sedaj našo Dense plast. Znotraj nje definiramo število nevronov in aktivacijko funkcijo. Spet dodajmo še Dropout in BatchNormalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d23f1941",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989f1c3",
   "metadata": {},
   "source": [
    "Naša zadnja plast bo napovedovala kateri objekt je na sliki. Število nevronov je kar število razredov in aktivacijska funkcija je *softmax*, ki bo predstavljala % gotovosti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9941a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(class_num, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca46ed7",
   "metadata": {},
   "source": [
    "Ko imamo model ustvarjen, ga moramo še *compil-at*. Tukaj izberemo še katero *loss function* bomo uporabili, kateri *optimizer* in katere metrike želimo spremljlati. Vzemimo najbolj splošne in najbolj uporabljene zadeve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78b5db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8b7fb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                262176    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 393,962\n",
      "Trainable params: 393,322\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49c081",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db3fe62",
   "metadata": {},
   "source": [
    "Še enkrat vse skupaj v eni celici:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec7d59ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                262176    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 393,962\n",
      "Trainable params: 393,322\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Koda je v CLEAN verziji\n",
    "'''\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, 3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D(2))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, 3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D(2))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "model.add(keras.layers.Conv2D(128, 3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(class_num, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f97a40",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866c1638",
   "metadata": {},
   "source": [
    "## Traning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591583f2",
   "metadata": {},
   "source": [
    "Sedaj lahko pričnemo s treniranjem našega modela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9fb040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9b2d232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "  5/782 [..............................] - ETA: 3:36 - loss: 2.7549 - accuracy: 0.1250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3d1f9f14bf5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "numpy.random.seed(seed)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=64)\n",
    "\n",
    "# Save model\n",
    "model.save(\"models/cifar_10.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b31242",
   "metadata": {},
   "source": [
    "Na žalost nimamo časa, da bi čakali, da se model dokončno nauči. Bomo pa uporabili že streniran model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "722828ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 16, 16, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 8, 8, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 8, 8, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                262176    \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 393,962\n",
      "Trainable params: 393,322\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "model2 = keras.models.load_model(\"models/cifar_10_25epoch.h5\")\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda2ba8d",
   "metadata": {},
   "source": [
    "Dodatno lahko prikažemo potek učenja (če bi ga učili dlje časa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fcd42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b7ce5",
   "metadata": {},
   "source": [
    "Training history:\n",
    "![Train history](images/cnn_train_history.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "566e741a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 10s 31ms/step - loss: 0.5083 - accuracy: 0.8370\n",
      "Accuracy: 83.70f%\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "scores = model2.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Accuracy: {scores[1]*100:.2f}f%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af78ea",
   "metadata": {},
   "source": [
    "Končni rezultat je kar dober. Kar bi nam sedaj preostalo je dodatno spreminjanje arhitekture našega modela in spreminjanje hiperparametrov v iskanju še boljšega modela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c8e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4331318",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd7595",
   "metadata": {},
   "source": [
    "Poglejmo si še predikcijo dejanske slike.\n",
    "\n",
    "Da naredimo predikcijo moramo naši sliki dodati en dimension, ker drugače dobimo error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5645b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer\n",
    "np.expand_dims(X_test[0], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c7a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dict_ = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    image = np.expand_dims(X_test[i], axis=0)\n",
    "    ax.imshow(X_test[i])\n",
    "    real_value = np.argmax(y_test[i])\n",
    "    prediction = np.argmax(model2.predict(image))\n",
    "    \n",
    "    ax.set_xlabel(f\"Prediction: {dict_[prediction]}\")\n",
    "    ax.set_title(f\"Real: {dict_[real_value]}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf809e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fcabfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a2d6b75",
   "metadata": {},
   "source": [
    "Naslednji bolj napredni model, ki si ga bomo pogledali, je Recurrent Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c8252",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cacac6",
   "metadata": {},
   "source": [
    "S pomočjo Recurrent Neural Network-a lahko naš model generira besedila. Naš model se bo učil na Harry Potter knjigah in nato generiral tekst glede na začetno besedilo katerega mu bomo mi podali."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f32b8",
   "metadata": {},
   "source": [
    "Posplošeno povedano, RNN - Recurrent Neural Network, procesira zaporedja, kot so cene delnic, stavki ali zaporedne meritve, po en element naenkrat. Hkrati pa ohranja spomin zaporedja.\n",
    "\n",
    "Recurrent pomeni, da trenutni izhod postane del vhoda pri naslednjem koraku."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979fe14",
   "metadata": {},
   "source": [
    "![RNN](images/rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f92bc",
   "metadata": {},
   "source": [
    "Ta spomin omogoča modelu, da se nauči daljših odvisnosti kar pomeni, da lahko pri napovedovanju upošteva celoten kontekst, naj bo to naslednja beseda v stavku, *sentement analysis* ali naslednja meritev temperature.\n",
    "\n",
    "Za primer vzemimo sledeči stavek:\n",
    ">The concert was boring for the first 15 minutes while the band warmed up but then was terribly exciting.\n",
    "\n",
    "Model kateri upošteva samo besede, ne pa tudi kontekst, bi po vsej verjetnosti napovedal, da ima stavek negativni pomen. RNN pa bi moral biti sposoben razumeti besede kot so *but* in *terribly exciting* in videti, da to spremeni pomen stavka iz negativnega v pozitivnega. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421af353",
   "metadata": {},
   "source": [
    "Glavna komponenta RNN modela je plast, ki implementira spomin. Med najbolj uporabljenimi in popularnimi je LSTM (Long Short-Term Memory) plast. Na kratko povedano, LSTM ima *input gate* za trenutno vhodno informacijo, *forget gate*, ki odvrže irelevantne informacije in *output gate*, ki predstavlja izhodno informacjo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9a38e9",
   "metadata": {},
   "source": [
    "![anatomy_of_lstm](images/anatomy_of_lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14346c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93f4caf7",
   "metadata": {},
   "source": [
    "## Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3ad48",
   "metadata": {},
   "source": [
    "Za naše namene bomo RNN uporabljali tako, da bomo napovedovali naslednjo besedo, ki naj bi se pojavila.\n",
    "\n",
    "Ko bomo natrenirali naš model bomo vanj poslali začetno besedilo in pridobili naslednjo besedo. S to besedo bomo nato posodobili naše začetno besedilo in ga ponovn poslali v naš model in pridobili naslednjo besedo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dc7a7d",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c0b6f5",
   "metadata": {},
   "source": [
    "Kot naše podatke bomo uporabili Harry Potter knjige. Vzeli bomo le prvo knjigo (ker moj računalnik ne zdrži večjih tekstov)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fcd3e8",
   "metadata": {},
   "source": [
    "[GitHub repo](https://github.com/formcept/whiteboard/tree/master/nbviewer/notebooks/data/harrypotter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca7bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "with open(\"data/Book 1 - The Philosopher's Stone.txt\") as f:\n",
    "    for line in f.readlines()[:]:\n",
    "        text = text + line.replace(\"\\n\", \"\")\n",
    "\n",
    "text = [text] # if it is just a long string it will encode each character\n",
    "print(text[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7520e6d",
   "metadata": {},
   "source": [
    "Naš tekst bomo sedaj *tokanizirali* - besede bomo pretvorili v številčne vrednosti. Za to bomo uporabili *Tokanizer* class v **keras** knjižnjici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e2a925",
   "metadata": {},
   "source": [
    "Tokanizer se znebi vseh ločil in vse črke spremeni v male črke. Nato pretvori vsako besedo v številčno vrednost. Če želimo ohraniti ločila lahko uporabimo drugačen **filters** parameter.\n",
    "\n",
    "Katera številka predstavlja katero vrednost lahko vidimo znotraj **.word_index** dictionary-ja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a0286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokanizer = Tokenizer(filters='!\"$#%&()*+,-./:;<=>?@[\\\\]^_`{|}\\t\\n', lower=True, split=\" \")\n",
    "tokanizer.fit_on_texts(text)\n",
    "for i, key in enumerate(tokanizer.word_index.keys()):\n",
    "    if i > 5:\n",
    "        break\n",
    "    print(key, \"\\t\", tokanizer.word_index[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89836dc",
   "metadata": {},
   "source": [
    "S pomočjo metode **texts_to_sequences(text)** bomo sedaj spremenili naše tekst besedilo v številčno sekvenco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792506ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokanizer.texts_to_sequences(text)\n",
    "sequence[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ebdfd4",
   "metadata": {},
   "source": [
    "Da številke pretvorimo nazaj uporabimo **.index_word** dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6661cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of indexes to words\n",
    "idx_word = tokanizer.index_word\n",
    "\n",
    "\" \".join(idx_word[w] for w in sequence[0][:51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4412d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of indexes to words\n",
    "idx_word = tokanizer.index_word\n",
    "word_idx = tokanizer.word_index\n",
    "print(len(idx_word))\n",
    "print(len(word_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0562b19b",
   "metadata": {},
   "source": [
    "## Features and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c1459a",
   "metadata": {},
   "source": [
    "Naslednji korak je, da ustvarimo naš nabor *features* in *labels*. V našem primeru bomo vzeli 50 besed (naši *features*) in nato naš model trenirali, da napove naslednjo 51. besedo (naš *label*).\n",
    "\n",
    "Da ustvarimo naše labels in features bomo sedaj uporabili prvih 50 besed in napovedali naslednjo. Nato bomo vzeli besede od 1 do 51 in napovedovali 52, in tako dalje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52638096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "training_length = 50\n",
    "\n",
    "seq = sequence[0]\n",
    "for i in range(training_length, len(seq)):\n",
    "    # Extract the features and label\n",
    "    extract = seq[i - training_length:i + 1]\n",
    "\n",
    "    # Set the features and label\n",
    "    features.append(extract[:-1])\n",
    "    labels.append(extract[-1])\n",
    "        \n",
    "features = np.array(features)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fccaf8",
   "metadata": {},
   "source": [
    "Sedaj imamo okoli 850'000 \"podatkov\" s katerimi bomo učili naš model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6069aca7",
   "metadata": {},
   "source": [
    "Nato bomo še *one-hot encoded* naše labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c653fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words ni vocabulary\n",
    "num_words = len(idx_word) + 1\n",
    "\n",
    "# Empty array to hold labels\n",
    "label_array = np.zeros((len(features), num_words))\n",
    "\n",
    "# One hot encode the labels\n",
    "for example_index, word_index in enumerate(labels):\n",
    "    label_array[example_index, word_index] = 1\n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e0dc5",
   "metadata": {},
   "source": [
    "Naš prvi input in out sta sedaj:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a6dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naš INPUT\")\n",
    "\" \".join([idx_word[i] for i in features[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1acc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naš OUTPUT\")\n",
    "idx_word[np.argmax(label_array[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00eec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "243b8e58",
   "metadata": {},
   "source": [
    "Za konec bomo naše podatke še razdelili na *training* in pa *validating* set. Uporabili bomo tudi **shuffle** funkcijo, da se bodo podatki premešali in ne bodo v točnem zaporedju knjige."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f32abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def create_train_valid(features,\n",
    "                       labels,\n",
    "                       num_words,\n",
    "                       train_fraction=0.75,\n",
    "                       RANDOM_STATE = 123):\n",
    "    \"\"\"Create training and validation features and labels.\"\"\"\n",
    "    \n",
    "    # Randomly shuffle features and labels\n",
    "    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Decide on number of samples for training\n",
    "    train_end = int(train_fraction * len(labels))\n",
    "\n",
    "    train_features = np.array(features[:train_end])\n",
    "    valid_features = np.array(features[train_end:])\n",
    "\n",
    "    train_labels = labels[:train_end]\n",
    "    valid_labels = labels[train_end:]\n",
    "\n",
    "    # Convert to arrays\n",
    "    X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
    "\n",
    "    # Using int8 for memory savings\n",
    "    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
    "    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
    "\n",
    "    # One hot encoding of labels\n",
    "    for example_index, word_index in enumerate(train_labels):\n",
    "        y_train[example_index, word_index] = 1\n",
    "\n",
    "    for example_index, word_index in enumerate(valid_labels):\n",
    "        y_valid[example_index, word_index] = 1\n",
    "    \n",
    "    # Memory management\n",
    "    import gc\n",
    "    gc.enable()\n",
    "    del features, labels, train_features, valid_features, train_labels, valid_labels\n",
    "    gc.collect()\n",
    "\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = create_train_valid(features, labels, num_words)\n",
    "X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac8c970",
   "metadata": {},
   "source": [
    "## Building RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef6cc2",
   "metadata": {},
   "source": [
    "Naš model bomo zgradili s pomočjo **Keras** knjižnjice, ki deluje na **Tensorflow** knjižnjici."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa7fac",
   "metadata": {},
   "source": [
    "Uporabili bomo **Sequential** model, kar pomeni, da plasti dodajamo eno in po eno.\n",
    "\n",
    "* Embedding - nam mapira besede v 100dimenzionalni vektor. Pozicija besede v tem vektorskem prostoru je njen *embedding*. Embeding Layer lahko tudi treniramo in tako izboljsamo relacije med besedami, lahko pa ga pustimo pri miru (ker moj računalnik ne zmore vsega tega).\n",
    "* Masking - je način kako našemu RNN povemo, če kakšne besede manjkajo naj jih izpusti. Če treniramo naš embedding layer potem Maskinga ne uporabljamo. V nasprotnem primeru pa ja.\n",
    "* LSTM - Plast, ki skrbi za ohranjanje spomina\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced5dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# EMbedding layer\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                     input_length = training_length,\n",
    "                     output_dim=100,\n",
    "                     trainable=True, #False\n",
    "                     #mask_zero=True\n",
    "                   ))\n",
    "\n",
    "# Masking layer for pre-trained embeddings\n",
    "#model.add(Masking(mask_value=0.0))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(LSTM(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "#Droput for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#Output layer\n",
    "model.add(Dense(num_words, activation=\"softmax\"))\n",
    "\n",
    "#compile model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce75a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d74e222",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae69732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Create callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint('models/harry_potter.h5', save_best_only=True, save_weights_only=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e71525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train,  y_train, \n",
    "                    batch_size=64,#2048, \n",
    "                    epochs=55,#150,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c435d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/hp_save.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd97c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = load_model(\"models/harry_potter_best.h5\")\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4029e25",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a1bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed_length=50\n",
    "diversity=1\n",
    "\n",
    "seed = sequence[0][:50]\n",
    "print(\" \".join([idx_word[x] for x in seed]))\n",
    "print(\"---->\")\n",
    "for i in range(30):\n",
    "    seed = sequence[0][i:50+i]\n",
    "    # Make a prediction from the seed\n",
    "    preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(np.float64)\n",
    "    predictions = preds.argsort()[-10:][::-1]\n",
    "\n",
    "    index_ = random.choice(predictions) # np.argmax(preds)# če je sam argmax ponavad vn dobiš sam the the the\n",
    "\n",
    "    print(idx_word[index_], end=\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d69d79",
   "metadata": {},
   "source": [
    "Vidimo, da je naš model dokaj neuspešen.\n",
    "\n",
    "Da bi model izbojšali bi lahko uporabili tekste vseh knjig in podaljšali čas učenja. (vendar moj računalnik ni dovolj zmogljiv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49457dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97ac2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e90aed8e",
   "metadata": {},
   "source": [
    "Lahko pa uporabimo že natrenirane modele."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d51e193",
   "metadata": {},
   "source": [
    "# GPT-2 and GPT-J Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5214b7",
   "metadata": {},
   "source": [
    "V smeri generiranja texta so bili večji premiki, ko je OpenAI izdal članek o [GPT-2](https://openai.com/blog/gpt-2-1-5b-release/) modelu. Učen je bil na okoli 40GB texta oziroma teksta iz 8milijon random spletnih strani. Njegov namen je bil preprosto napovedati naslednjo besedo.\n",
    "\n",
    "Naslednji večji premik je bil, ko je OpenAI objavil [GPT-3 članek](https://arxiv.org/abs/2005.14165). Na žalost pa niso javno objavili modelovih uteži in tako ne moremo sami uporabiti njihovega modela.\n",
    "\n",
    "So pa se pojavili modeli, ki se po sposobnosti približajo GPT-3 in imajo javne uteži. Tak model bomo uporabili tudi mi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8490d7",
   "metadata": {},
   "source": [
    "V naši nalogi bomo uporabili [GPT-J model](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/), katerega so ustvarili pri ElutherAI. Model ima 6 miljard parametrov, 28 plasti z 4096 dimenzijami.\n",
    "\n",
    "Učen je bil na [The Pile dataset](https://arxiv.org/abs/2101.00027), ki je velik 825GB, vsebuje 22 pod-datasetov, ki vključujejo celotno angleško Wikipedijo, GitHub, Stack Exchange, ArXiv in še več."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3670dc",
   "metadata": {},
   "source": [
    "V spodnji tabelici lahko vidimo kakšne modele lahko s pomočjo knjižnjice uporabljamo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc9c77",
   "metadata": {},
   "source": [
    "<table style=\"border-collapse: collapse; width: 55.8987%; height: 107px; margin-left: auto; margin-right: auto;\" border=\"1\">\n",
    "<tbody>\n",
    "<tr style=\"height: 18px;\">\n",
    "<td style=\"width: 39.1738%; text-align: center; height: 18px;\"><span style=\"font-family: helvetica, arial, sans-serif; font-size: 12pt;\">Model</span></td>\n",
    "<td style=\"width: 30.6178%; height: 18px; text-align: center;\"><span style=\"font-family: helvetica, arial, sans-serif; font-size: 12pt;\">Number of Parameters</span></td>\n",
    "<td style=\"width: 21.5188%; height: 18px; text-align: center;\"><span style=\"font-family: helvetica, arial, sans-serif; font-size: 12pt;\">Size</span></td>\n",
    "</tr>\n",
    "<tr style=\"height: 18px;\">\n",
    "<td style=\"width: 39.1738%; height: 18px; text-align: center;\"><strong><span style=\"font-family: 'andale mono', monospace;\"><code><span style=\"font-size: 12pt;\">gpt2</span></code></span></strong></td>\n",
    "<td style=\"width: 30.6178%; height: 18px; text-align: center;\"><strong><span style=\"font-family: helvetica, arial, sans-serif; font-size: 12pt;\">124M</span></strong></td>\n",
    "<td style=\"width: 21.5188%; height: 18px; text-align: center;\"><span style=\"font-family: 'andale mono', monospace;\"><strong><span style=\"font-size: 12pt;\">523MB</span></strong></span></td>\n",
    "</tr>\n",
    "<tr style=\"height: 18px;\">\n",
    "<td style=\"width: 39.1738%; height: 18px; text-align: center;\"><span style=\"font-family: 'andale mono', monospace;\"><code><span style=\"font-size: 12pt;\">EleutherAI/gpt-neo-125M</span></code></span></td>\n",
    "<td style=\"width: 30.6178%; height: 18px; text-align: center;\"><span style=\"font-family: helvetica, arial, sans-serif; font-size: 12pt;\">125M</span></td>\n",
    "<td style=\"width: 21.5188%; text-align: center; height: 18px;\"><span style=\"font-family: 'andale mono', monospace; font-size: 12pt;\">502MB</span></td>\n",
    "</tr>\n",
    "<tr style=\"height: 18px;\">\n",
    "<td style=\"width: 39.1738%; text-align: center; height: 18px;\"><span style=\"font-family: 'andale mono', monospace;\"><code><span style=\"font-size: 12pt;\">EleutherAI/gpt-neo-1.3B</span></code></span></td>\n",
    "<td style=\"width: 30.6178%; text-align: center; height: 18px;\"><span style=\"font-family: helvetica, arial, sans-serif; font-size: 12pt;\">1.3B</span></td>\n",
    "<td style=\"width: 21.5188%; text-align: center; height: 18px;\"><span style=\"font-family: 'andale mono', monospace; font-size: 12pt;\">4.95GB</span></td>\n",
    "</tr>\n",
    "<tr style=\"height: 17px;\">\n",
    "<td style=\"width: 39.1738%; height: 17px; text-align: center;\"><span style=\"font-family: 'andale mono', monospace;\"><code><span style=\"font-size: 12pt;\">EleutherAI/gpt-neo-2.7B</span></code></span></td>\n",
    "<td style=\"width: 30.6178%; height: 17px; text-align: center;\"><span style=\"font-family: helvetica, arial, sans-serif; font-size: 12pt;\">2.7B</span></td>\n",
    "<td style=\"width: 21.5188%; height: 17px; text-align: center;\"><span style=\"font-family: 'andale mono', monospace; font-size: 12pt;\">9.94GB</span></td>\n",
    "</tr>\n",
    "<tr style=\"height: 18px;\">\n",
    "<td style=\"width: 39.1738%; text-align: center; height: 18px;\"><strong><span style=\"font-family: 'andale mono', monospace;\"><code><span style=\"font-size: 12pt;\">EleutherAI/gpt-j-6B</span></code></span></strong></td>\n",
    "<td style=\"width: 30.6178%; text-align: center; height: 18px;\"><strong><span style=\"font-family: helvetica, arial, sans-serif; font-size: 12pt;\">6B</span></strong></td>\n",
    "<td style=\"width: 21.5188%; text-align: center; height: 18px;\"><span style=\"font-family: 'andale mono', monospace;\"><strong><span style=\"font-size: 12pt;\">22.5GB</span></strong></span></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a49390",
   "metadata": {},
   "source": [
    "## Installing library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e00732",
   "metadata": {},
   "source": [
    "Za začetek naložimo python knjižnjico:\n",
    "\n",
    "```python\n",
    "$ pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e62c93",
   "metadata": {},
   "source": [
    "## Uporaba knjižnjice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d684904d",
   "metadata": {},
   "source": [
    "(Da zdownloada GPT2 model tud traja neki cajta...). Na Ubuntu se zadeve shranijo v `~/.cache/huggingface/transformers/`.\n",
    "\n",
    "https://huggingface.co/docs/datasets/v1.12.0/cache.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3b6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# download & load GPT-2 model\n",
    "gpt2_generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c541254c",
   "metadata": {},
   "source": [
    "Prvo uporabimo GPT-2 model, da zgeneriramo 3 različne stavke iz top 50 kandidatov."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe15c8",
   "metadata": {},
   "source": [
    "S pomočjo parametra **top_k** povemo, da želimo izbirati izmed 50 najbolj primernih besed. S parametrom **temperature** pa definiramo kako greedy želimo biti pri izbiranju besed. Vrednost 0 pomeni, da želimo vzeti najbolj primerno besedo, vendar to lahko hitro privede do ponavljanja.\n",
    "\n",
    "S prametrom **max_length** določimo končno dolžino našega teksta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb307c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Harry wandered over to the Restricted Section. He had been wondering for a while if\"\n",
    "sentences = gpt2_generator(prompt, \n",
    "                           do_sample=True, \n",
    "                           top_k=50, \n",
    "                           temperature=0.6, \n",
    "                           max_length=50, \n",
    "                           num_return_sequences=3)\n",
    "for sentence in sentences:\n",
    "    print(sentence[\"generated_text\"])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ab153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2302b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download & load GPT-J model! It's 22.5GB in size\n",
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-125M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c59a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sentences with TOP-K sampling\n",
    "sentences = generator(\"To be honest, robots will\", do_sample=True, top_k=100, temperature=0.8, max_length=50, num_return_sequences=3)\n",
    "for sentence in sentences:\n",
    "    print(sentence[\"generated_text\"])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e02d91",
   "metadata": {},
   "source": [
    "Ker je bil model streniran na The Pile datasetu lahko generira več kot samo angleška besedila:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fece0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Python Code!\n",
    "print(generator(\n",
    "\"\"\"\n",
    "import os\n",
    "# make a list of all african countries\n",
    "\"\"\",\n",
    "    do_sample=True, top_k=10, temperature=0.05, max_length=256)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Python Code!\n",
    "print(generator(\n",
    "\"\"\"\n",
    "import datetime\n",
    "# make a list of all holiday datetimes\n",
    "\"\"\",\n",
    "    do_sample=True, top_k=10, temperature=0.05, max_length=256)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250c9d35",
   "metadata": {},
   "source": [
    "https://6b.eleuther.ai/ - online prompt za GPT-J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ad852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
