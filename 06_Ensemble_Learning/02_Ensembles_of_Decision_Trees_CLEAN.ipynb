{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca0a212",
   "metadata": {},
   "source": [
    "# Ensembles of Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e62a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d796e1d",
   "metadata": {},
   "source": [
    "## Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7b56a",
   "metadata": {},
   "source": [
    "\n",
    "**A random forest is essentially a collection of decision trees, where each tree is slightly different from\n",
    "the others.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.helpers_tree import randomized_tree_interactive\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4, random_state=0, cluster_std=1.0)\n",
    "\n",
    "randomized_tree_interactive(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ed0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77726ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78a2c267",
   "metadata": {},
   "source": [
    "### Building random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a8d38",
   "metadata": {},
   "source": [
    "1. To build a tree, we first take what is called a bootstrap sample of our data.\n",
    "2. A decision tree is built based on this newly created dataset.\n",
    "3. The bootstrap sampling leads to each decision tree in the random forest being built on a slightly different dataset.\n",
    "\n",
    "A critical parameter in this process is `max_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2097ef",
   "metadata": {},
   "source": [
    "### Analyzing random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb9f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f296e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=5, random_state=2)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f5d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plot_interactive_tree import plot_tree_partition\n",
    "from helpers.plot_2d_separator import plot_2d_separator\n",
    "from helpers.plot_helpers import discrete_scatter\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "\n",
    "for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\n",
    "    ax.set_title(f\"Tree {i}\")\n",
    "    plot_tree_partition(X_train, y_train, tree, ax=ax)\n",
    "    plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1],alpha=.4)\n",
    "    \n",
    "axes[-1, -1].set_title(\"Random Forest\")\n",
    "discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91816a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy on training set: {forest.score(X_train, y_train):.3f}\")\n",
    "print(f\"Accuracy on test set: {forest.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ceb584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances_cancer(model):\n",
    "    plt.figure(figsize=(5, 7))\n",
    "    n_features = cancer.data.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), cancer.feature_names)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importances_cancer(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d00d8",
   "metadata": {},
   "source": [
    "### Strengths, weaknesses, and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8292503",
   "metadata": {},
   "source": [
    "- **Random forests for regression and classification are currently among the most widely used machine learning methods.**\n",
    "- They are:\n",
    "    - **very powerful**, \n",
    "    - often **work well without heavy tuning** of the parameters\n",
    "    - **don’t require scaling of the data**.\n",
    "    - Random forests usually work well even on very large datasets, and training can easily be parallelized over many CPU cores within a powerful computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3631e2c7",
   "metadata": {},
   "source": [
    "The important parameters to adjust are `n_estimators`, `max_features`, and possibly pre-pruning options like `max_depth`.\n",
    "- `n_estimators`, larger is always better (there are diminishing returns, and more trees need more memory and more time to train.)\n",
    "- `max_features` determines how random each tree is, and a smaller max_features reduces overfitting (it’s a good rule of thumb to use the default values: `max_features=sqrt(n_features)` for classification and `max_features=n_features` for regression.)\n",
    "    - It can also drastically reduce space and time requirements for training and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69fce05",
   "metadata": {},
   "source": [
    "### Example: Random Forest for Classifying Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b280d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3fc300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the figure\n",
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# plot the digits: each image is 8x8 pixels\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    \n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3294f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target, random_state=0)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=1000)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypred = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b20cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mat = confusion_matrix(ytest, ypred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc5890",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f2e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(200)\n",
    "\n",
    "def model(x, sigma=0.3):\n",
    "    fast_oscillation = np.sin(5 * x)\n",
    "    slow_oscillation = np.sin(0.5 * x)\n",
    "    noise = sigma * rng.randn(len(x))\n",
    "\n",
    "    return slow_oscillation + fast_oscillation + noise\n",
    "\n",
    "y = model(x)\n",
    "plt.errorbar(x, y, 0.3, fmt='o');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor(200)\n",
    "forest.fit(x[:, None], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = forest.predict(xfit[:, None])\n",
    "ytrue = model(xfit, sigma=0)\n",
    "\n",
    "plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)\n",
    "plt.plot(xfit, yfit, '-r');\n",
    "plt.plot(xfit, ytrue, '-k', alpha=0.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e77166",
   "metadata": {},
   "source": [
    "## Gradient boosted regression trees (gradient boosting machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d60db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy on training set: {gbrt.score(X_train, y_train):.3f}\")\n",
    "print(f\"Accuracy on test set: {gbrt.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc43158",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy on training set: {gbrt.score(X_train, y_train):.3f}\")\n",
    "print(f\"Accuracy on test set: {gbrt.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f729efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy on training set: {gbrt.score(X_train, y_train):.3f}\")\n",
    "print(f\"Accuracy on test set: {gbrt.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4674f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "plot_feature_importances_cancer(gbrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e53019d",
   "metadata": {},
   "source": [
    "### Strengths, weaknesses, and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4300f6",
   "metadata": {},
   "source": [
    "Strengths:\n",
    "- Gradient boosted decision trees are among **the most powerful and widely used models for supervised learning**. \n",
    "- Similarly to other tree-based models, the algorithm **works well without scaling** and on a mixture of binary and continuous features.\n",
    "- **Works well on tabular (structured) data**\n",
    "- Fast\n",
    "\n",
    "Weaknesses:\n",
    "- Their main drawback is that they **require careful tuning of the parameters** and may take a long time to train. \n",
    "- As with other tree-based models, it also **often does not work well on high-dimensional sparse data**.\n",
    "- Not racommended for unstructured data (images, audio, text)\n",
    "\n",
    "The main parameters of gradient boosted tree models are:\n",
    "- the number of trees, `n_estimators`\n",
    "- the `learning_rate`, which controls the degree to which each tree is allowed to correct the mistakes of the previous trees. \n",
    "\n",
    "These two parameters are highly interconnected, as a lower `learning_rate` means that more trees are needed to build\n",
    "a model of similar complexity. \n",
    "\n",
    "- In contrast to random forests, where a higher `n_estimators` value is always better, increasing `n_estimators` in gradient boosting leads to a more complex model, which may lead to overfitting. \n",
    "- A common practice is to fit `n_estimators` depending on the time and memory budget, and then search over different\n",
    "`learning_rates`.\n",
    "- Another important parameter is `max_depth` (or alternatively max_leaf_nodes), to reduce the complexity of each tree. \n",
    "    - Usually `max_depth` is set very low for gradient boosted models, often not deeper than five splits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
