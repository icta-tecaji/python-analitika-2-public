{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e6a662",
   "metadata": {},
   "source": [
    "# Working with Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e7c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a899ac9",
   "metadata": {},
   "source": [
    "## Types of Data Represented as Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0356050b",
   "metadata": {},
   "source": [
    "## Example Application: Sentiment Analysis of Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c2ceaa",
   "metadata": {},
   "source": [
    "> Prenos podatkov s strani: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2bd432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdata/aclImdb\u001b[00m\r\n",
      "├── \u001b[01;34mtest\u001b[00m\r\n",
      "│   ├── \u001b[01;34mneg\u001b[00m\r\n",
      "│   └── \u001b[01;34mpos\u001b[00m\r\n",
      "└── \u001b[01;34mtrain\u001b[00m\r\n",
      "    ├── \u001b[01;34mneg\u001b[00m\r\n",
      "    └── \u001b[01;34mpos\u001b[00m\r\n",
      "\r\n",
      "6 directories\r\n"
     ]
    }
   ],
   "source": [
    "!tree -dL 2 data/aclImdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebac2b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'data/aclImdb/train/unsup': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm -r data/aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff2ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a8616b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 25000\n",
      "text_train[1]:\n",
      "b'Words can\\'t describe how bad this movie is. I can\\'t explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many clich\\xc3\\xa9s, mistakes (and all other negative things you can imagine) here that will just make you cry. To start with the technical first, there are a LOT of mistakes regarding the airplane. I won\\'t list them here, but just mention the coloring of the plane. They didn\\'t even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original Boeing livery. Very bad. The plot is stupid and has been done many times before, only much, much better. There are so many ridiculous moments here that i lost count of it really early. Also, I was on the bad guys\\' side all the time in the movie, because the good guys were so stupid. \"Executive Decision\" should without a doubt be you\\'re choice over this one, even the \"Turbulence\"-movies are better. In fact, every other movie in the world is better than this one.'\n"
     ]
    }
   ],
   "source": [
    "reviews_train = load_files(\"data/aclImdb/train/\")\n",
    "\n",
    "# load_files returns a bunch, containing training texts and training labels\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print(\"type of text_train: {}\".format(type(text_train)))\n",
    "print(\"length of text_train: {}\".format(len(text_train)))\n",
    "print(\"text_train[1]:\\n{}\".format(text_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "761198d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca43ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class (training): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Samples per class (training): {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "704476f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "reviews_test = load_files(\"data/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
    "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb908973",
   "metadata": {},
   "source": [
    "## Representing Text Data as a Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e83918",
   "metadata": {},
   "source": [
    "<img src=\"images/bag.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cebcda",
   "metadata": {},
   "source": [
    "### Applying Bag-of-Words to a Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "bards_words =[\"The fool doth think he is wise,\",\n",
    "              \"but the the wise man knows himself to be a fool\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71b0e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eac3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ed1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "\n",
    "print(\"bag_of_words: {}\".format(repr(bag_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c929601",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86f949",
   "metadata": {},
   "source": [
    "### Bag-of-Words for Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448450d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(f\"X_train:\\n{repr(X_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0017f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=10000), X_train, y_train, cv=5, n_jobs=-1, verbose=1)\n",
    "print(f\"Mean cross-validation accuracy: {np.mean(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "print(\"{:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(\"X_train with min_df: {}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3944bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "print(\"First 50 features:\\n{}\".format(feature_names[:50]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 700th feature:\\n{}\".format(feature_names[::700]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best cross-validation score: {grid.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de857e",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250946e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying stop_words=\"english\" uses the built-in list.\n",
    "# We could also augment it and pass our own.\n",
    "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
    "\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with stop words:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf23509",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best cross-validation score: {grid.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db7cb5d",
   "metadata": {},
   "source": [
    "## Rescaling the Data with tf–idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb713b3e",
   "metadata": {},
   "source": [
    "    tfidf(w, d) = tf log((N + 1) / (Nw + 1)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5986f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None), LogisticRegression(max_iter=10000))\n",
    "\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid.fit(text_train, y_train)\n",
    "\n",
    "print(f\"Best cross-validation score: {grid.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57868db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "\n",
    "# transform the training dataset\n",
    "X_train = vectorizer.transform(text_train)\n",
    "\n",
    "# find maximum value for each of the features over the dataset\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "\n",
    "# get feature names\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(f\"Features with lowest tfidf:\\n{feature_names[sorted_by_tfidf[:20]]}\")\n",
    "print(f\"Features with highest tfidf: \\n{feature_names[sorted_by_tfidf[-20:]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a292b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "\n",
    "print(f\"Features with lowest idf:\\n{feature_names[sorted_by_idf[:100]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1513ad",
   "metadata": {},
   "source": [
    "## Investigating Model Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc06cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import tools\n",
    "\n",
    "tools.visualize_coefficients(grid.best_estimator_.named_steps[\"logisticregression\"].coef_, feature_names, n_top_features=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fbd824",
   "metadata": {},
   "source": [
    "## Bag-of-Words with More Than One Word (n-Grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119567f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"bards_words:\\n{bards_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\n",
    "\n",
    "print(f\"Vocabulary size: {len(cv.vocabulary_)}\")\n",
    "print(f\"Vocabulary:\\n{cv.get_feature_names_out()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7211c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "\n",
    "print(f\"Vocabulary size: {len(cv.vocabulary_)}\")\n",
    "print(f\"Vocabulary:\\n{cv.get_feature_names_out()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914de039",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(3, 3)).fit(bards_words)\n",
    "\n",
    "print(f\"Vocabulary size: {len(cv.vocabulary_)}\")\n",
    "print(f\"Vocabulary:\\n{cv.get_feature_names_out()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627332a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.transform(bards_words).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)\n",
    "\n",
    "print(f\"Vocabulary size: {len(cv.vocabulary_)}\")\n",
    "print(f\"Vocabulary:\\n{cv.get_feature_names_out()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871dc202",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
    "\n",
    "# running the grid search takes a long time because of the\n",
    "# relatively large grid and the inclusion of trigrams\n",
    "\n",
    "# lahko dodamo triagrame in biagrame samo predolgo traja\n",
    "param_grid = {\"logisticregression__C\": [1, 10, 100], \"tfidfvectorizer__ngram_range\": [(1, 2)]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid.fit(text_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e500f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature names and coefficients\n",
    "vect = grid.best_estimator_.named_steps['tfidfvectorizer']\n",
    "\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "coef = grid.best_estimator_.named_steps['logisticregression'].coef_\n",
    "tools.visualize_coefficients(coef, feature_names, n_top_features=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb6bbc",
   "metadata": {},
   "source": [
    "## Advanced Tokenization, Stemming, and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17020c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3996a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy's English-language models\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "# instantiate nltk's Porter stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# define function to compare lemmatization in spacy with stemming in nltk\n",
    "def compare_normalization(doc):\n",
    "    # tokenize document in spacy\n",
    "    doc_spacy = en_nlp(doc)\n",
    "    # print lemmas found by spacy\n",
    "    print(\"Lemmatization:\")\n",
    "    print([token.lemma_ for token in doc_spacy])\n",
    "    # print tokens found by Porter stemmer\n",
    "    print(\"Stemming:\")\n",
    "    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b4900",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_normalization(\"Our meeting today was worse than yesterday, I'm scared of meeting the clients tomorrow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c90f02",
   "metadata": {},
   "source": [
    "## Topic Modeling and Document Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af12cf6",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af04b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "\n",
    "X = vect.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246025c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method=\"batch\", max_iter=25, random_state=0, n_jobs=-1)\n",
    "\n",
    "# We build the model and transform the data in one step\n",
    "# Computing transform takes some time,\n",
    "# and we can save time by doing both at once\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3922353",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef894f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic (a row in the components_), sort the features (ascending)\n",
    "# Invert rows with [:, ::-1] to make sorting descending\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "\n",
    "# Get the feature names from the vectorizer\n",
    "feature_names = np.array(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db8781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.tools import print_topics\n",
    "\n",
    "#Print out the 10 topics:\n",
    "print_topics(topics=range(10), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda100 = LatentDirichletAllocation(n_components=100, learning_method=\"batch\", max_iter=25, random_state=0, n_jobs=-1)\n",
    "document_topics100 = lda100.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = np.array([7, 16, 24, 25, 28, 36, 37, 45, 51, 53, 54, 63, 89, 97])\n",
    "\n",
    "sorting = np.argsort(lda100.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "print_topics(topics=topics, feature_names=feature_names, sorting=sorting, topics_per_chunk=7, n_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74302eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by weight of \"music\" topic 45\n",
    "music = np.argsort(document_topics100[:, 45])[::-1]\n",
    "\n",
    "# print the five documents where the topic is most important\n",
    "for i in music[:10]:\n",
    "    # pshow first two sentences\n",
    "    print(b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177131d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "topic_names = [\"{:>2} \".format(i) + \" \".join(words) for i, words in enumerate(feature_names[sorting[:, :2]])]\n",
    "\n",
    "# two column bar chart:\n",
    "for col in [0, 1]:\n",
    "    start = col * 50\n",
    "    end = (col + 1) * 50\n",
    "    ax[col].barh(np.arange(50), np.sum(document_topics100, axis=0)[start:end])\n",
    "    ax[col].set_yticks(np.arange(50))\n",
    "    ax[col].set_yticklabels(topic_names[start:end], ha=\"left\", va=\"top\")\n",
    "    ax[col].invert_yaxis()\n",
    "    ax[col].set_xlim(0, 2000)\n",
    "    yax = ax[col].get_yaxis()\n",
    "    yax.set_tick_params(pad=130)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
