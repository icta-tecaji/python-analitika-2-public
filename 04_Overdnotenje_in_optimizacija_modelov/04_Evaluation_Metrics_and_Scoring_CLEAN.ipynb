{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d7abe83",
   "metadata": {},
   "source": [
    "# Evaluation Metrics and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e03503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8d5db",
   "metadata": {},
   "source": [
    "## Machine learning application goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b35ad8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16611e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b91b3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df29ef38",
   "metadata": {},
   "source": [
    "## Metrics for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d00f91d",
   "metadata": {},
   "source": [
    "### Kinds of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4837691e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9297a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb7e684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b000aeb",
   "metadata": {},
   "source": [
    "### Imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "y = digits.target == 9\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f6bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "pred_most_frequent = dummy_majority.predict(X_test)\n",
    "\n",
    "print(f\"Unique predicted labels: {np.unique(pred_most_frequent)}\")\n",
    "print(f\"Test score: {dummy_majority.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\n",
    "pred_tree = tree.predict(X_test)\n",
    "\n",
    "print(f\"Test score: {tree.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e55913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"stratified\", random_state=42).fit(X_train, y_train)\n",
    "pred_dummy = dummy.predict(X_test)\n",
    "print(f\"dummy score: {dummy.score(X_test, y_test):.2f}\")\n",
    "\n",
    "logreg = LogisticRegression(C=0.1, max_iter=10000, solver=\"liblinear\").fit(X_train, y_train)\n",
    "pred_logreg = logreg.predict(X_test)\n",
    "print(f\"logreg score: {logreg.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40613752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1861f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0153d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b670c68f",
   "metadata": {},
   "source": [
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a085a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(y_test, pred_logreg)\n",
    "\n",
    "print(f\"Confusion matrix:\\n{confusion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81cd847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plot_metrics import plot_confusion_matrix_illustration\n",
    "\n",
    "plot_confusion_matrix_illustration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plot_metrics import plot_binary_confusion_matrix\n",
    "\n",
    "plot_binary_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae46bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most frequent class:\")\n",
    "print(confusion_matrix(y_test, pred_most_frequent))\n",
    "print(\"\\nDummy model:\")\n",
    "print(confusion_matrix(y_test, pred_dummy))\n",
    "print(\"\\nDecision tree:\")\n",
    "print(confusion_matrix(y_test, pred_tree))\n",
    "print(\"\\nLogistic Regression\")\n",
    "print(confusion_matrix(y_test, pred_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d943176",
   "metadata": {},
   "source": [
    "**Relation to accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f797dba5",
   "metadata": {},
   "source": [
    "    \n",
    "    Accuracy = TP+TN / (TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dff7cf",
   "metadata": {},
   "source": [
    "`Accuracy is the number of correct predictions (TP and TN) divided by the number of all samples (all entries of the confusion matrix summed up)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0d6be0",
   "metadata": {},
   "source": [
    "**Precision, recall, and f-score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81977f6c",
   "metadata": {},
   "source": [
    "`Precision measures how many of the samples predicted as positive are actually positive:`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c136add1",
   "metadata": {},
   "source": [
    "    Precision = TP / (TP+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77386257",
   "metadata": {},
   "source": [
    "`Recall, on the other hand, measures how many of the positive samples are captured by the positive predictions:`\n",
    "\n",
    "    Recall = TP / (TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecb4795",
   "metadata": {},
   "source": [
    "`f-score or f-measure, which is with the harmonic mean of precision and recall`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3895f",
   "metadata": {},
   "source": [
    "    F = 2 * (precision * recall) / (precision + recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6814b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"f1 score most frequent: {:.2f}\".format(f1_score(y_test, pred_most_frequent)))\n",
    "print(\"f1 score dummy: {:.2f}\".format(f1_score(y_test, pred_dummy)))\n",
    "print(\"f1 score tree: {:.2f}\".format(f1_score(y_test, pred_tree)))\n",
    "print(\"f1 score logistic regression: {:.2f}\".format(f1_score(y_test, pred_logreg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c40520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, pred_most_frequent, target_names=[\"not nine\", \"nine\"], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc859c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_dummy, target_names=[\"not nine\", \"nine\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fce281",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_logreg, target_names=[\"not nine\", \"nine\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca43ae52",
   "metadata": {},
   "source": [
    "### Taking uncertainty into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_blobs(n_samples=(400, 50), cluster_std=[7.0, 2], random_state=22)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "svc = SVC(gamma=.05).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c62026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plot_metrics import plot_decision_threshold\n",
    "\n",
    "plot_decision_threshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a312aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc471a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lower_threshold = svc.decision_function(X_test) > -.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e9375",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_lower_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82229f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61307db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222254fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "487a7cd6",
   "metadata": {},
   "source": [
    "### Precision-recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840de0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, svc.decision_function(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96192277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use more data points for a smoother curve\n",
    "X, y = make_blobs(n_samples=(4000, 500), cluster_std=[7.0, 2], random_state=22)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "svc = SVC(gamma=.05).fit(X_train, y_train)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, svc.decision_function(X_test))\n",
    "\n",
    "# find threshold closest to zero\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "plt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10, label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n",
    "plt.plot(precision, recall, label=\"precision recall curve\")\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82974001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0, max_features=2)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# RandomForestClassifier has predict_proba, but not decision_function\n",
    "precision_rf, recall_rf, thresholds_rf = precision_recall_curve(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "plt.plot(precision, recall, label=\"svc\")\n",
    "plt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,label=\"threshold zero svc\", fillstyle=\"none\", c='k', mew=2)\n",
    "plt.plot(precision_rf, recall_rf, label=\"rf\")\n",
    "\n",
    "close_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\n",
    "plt.plot(precision_rf[close_default_rf], recall_rf[close_default_rf], '^', c='k', markersize=10, label=\"threshold 0.5 rf\", fillstyle=\"none\", mew=2)\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b0363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"f1_score of random forest: {f1_score(y_test, rf.predict(X_test)):.3f}\")\n",
    "print(f\"f1_score of svc: {f1_score(y_test, svc.predict(X_test)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b871ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "ap_rf = average_precision_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "ap_svc = average_precision_score(y_test, svc.decision_function(X_test))\n",
    "\n",
    "print(f\"Average precision of random forest: {ap_rf:.3f}\")\n",
    "print(f\"Average precision of svc: {ap_svc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d30a981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd835c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab3721a9",
   "metadata": {},
   "source": [
    "### Receiver operating characteristics (ROC) and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8699a32a",
   "metadata": {},
   "source": [
    "    FPR = FP / (FP+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893501da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, svc.decision_function(X_test))\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "\n",
    "# find threshold closest to zero\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10, label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3950cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve SVC\")\n",
    "plt.plot(fpr_rf, tpr_rf, label=\"ROC Curve RF\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "\n",
    "plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10, label=\"threshold zero SVC\", fillstyle=\"none\", c='k', mew=2)\n",
    "\n",
    "close_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\n",
    "\n",
    "plt.plot(fpr_rf[close_default_rf], tpr[close_default_rf], '^', markersize=10, label=\"threshold 0.5 RF\", fillstyle=\"none\", c='k', mew=2)\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "svc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\n",
    "\n",
    "print(f\"AUC for Random Forest: {rf_auc:.3f}\")\n",
    "print(f\"AUC for SVC: {svc_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5541e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = digits.target == 9\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, y, random_state=0)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for gamma in [1, 0.05, 0.01]:\n",
    "    svc = SVC(gamma=gamma).fit(X_train, y_train)\n",
    "    accuracy = svc.score(X_test, y_test)\n",
    "    auc = roc_auc_score(y_test, svc.decision_function(X_test))\n",
    "    fpr, tpr, _ = roc_curve(y_test , svc.decision_function(X_test))\n",
    "    print(f\"gamma = {gamma:.2f} accuracy = {accuracy:.2f} AUC = {auc:.2f}\")\n",
    "    plt.plot(fpr, tpr, label=\"gamma={:.3f}\".format(gamma))\n",
    "\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlim(-0.01, 1)\n",
    "plt.ylim(0, 1.02)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a07a8da",
   "metadata": {},
   "source": [
    "**RocCurveDisplay function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a819b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a ROC curve for a fitted support vector machine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "X, y = load_wine(return_X_y=True)\n",
    "y = y == 2  # make binary\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "svc = SVC(random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "svc_disp = RocCurveDisplay.from_estimator(svc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "ax = plt.gca()\n",
    "rfc_disp = RocCurveDisplay.from_estimator(rfc, X_test, y_test, ax=ax, alpha=0.8)\n",
    "svc_disp.plot(ax=ax, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd03b7",
   "metadata": {},
   "source": [
    "## Metrics for Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1824c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, random_state=0)\n",
    "\n",
    "lr = LogisticRegression(max_iter=10000, solver=\"liblinear\").fit(X_train, y_train)\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, pred):.3f}\")\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(y_test, pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10dab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.tools import heatmap\n",
    "\n",
    "scores_image = heatmap(confusion_matrix(y_test, pred), \n",
    "                       xlabel='Predicted label', \n",
    "                       ylabel='True label', \n",
    "                       xticklabels=digits.target_names, \n",
    "                       yticklabels=digits.target_names, \n",
    "                       cmap=plt.cm.gray_r, \n",
    "                       fmt=\"%d\")\n",
    "\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c414a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f01115",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Micro average f1 score: {f1_score(y_test, pred, average='micro'):.3f}\")\n",
    "print(f\"Macro average f1 score: {f1_score(y_test, pred, average='macro'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d432b748",
   "metadata": {},
   "source": [
    "\n",
    "The most important values for the scoring parameter for **classification** are:\n",
    "- accuracy (the default); \n",
    "- `roc_auc` for the area under the ROC curve; \n",
    "- `average_precision` for the area under the precision-recall curve; \n",
    "- `f1`, `f1_macro`, `f1_micro`, and `f1_weighted` for the binary f1-score and the different weighted variants. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac7480",
   "metadata": {},
   "source": [
    "## Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91defd15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507414d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00054027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e013f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51c0ed23",
   "metadata": {},
   "source": [
    "\n",
    "For **regression**, the most commonly used values are:\n",
    "- r2 for the R2 score, \n",
    "- `mean_squared_error` for mean squared error\n",
    "- `mean_absolute_error` for mean absolute error. \n",
    "\n",
    "You can find a full list of supported arguments in the documentation:\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11764f5e",
   "metadata": {},
   "source": [
    "## Validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc01a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ce0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57deaa12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "subset_mask = np.isin(y, [1, 2])  # binary classification: 1 vs 2\n",
    "\n",
    "X, y = X[subset_mask], y[subset_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_range = np.logspace(-6, -1, 5)\n",
    "\n",
    "# use the default 5-fold cross validation,\n",
    "train_scores, test_scores = validation_curve(\n",
    "    SVC(),\n",
    "    X,\n",
    "    y,\n",
    "    param_name=\"gamma\",\n",
    "    param_range=param_range,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=2,\n",
    ")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed332c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Validation Curve with SVM\")\n",
    "plt.xlabel(r\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(\n",
    "    param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", lw=lw\n",
    ")\n",
    "plt.fill_between(\n",
    "    param_range,\n",
    "    train_scores_mean - train_scores_std,\n",
    "    train_scores_mean + train_scores_std,\n",
    "    alpha=0.2,\n",
    "    color=\"darkorange\",\n",
    "    lw=lw,\n",
    ")\n",
    "plt.semilogx(\n",
    "    param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", lw=lw\n",
    ")\n",
    "plt.fill_between(\n",
    "    param_range,\n",
    "    test_scores_mean - test_scores_std,\n",
    "    test_scores_mean + test_scores_std,\n",
    "    alpha=0.2,\n",
    "    color=\"navy\",\n",
    "    lw=lw,\n",
    ")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3007ea3",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599a994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadb3cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5)\n",
    "\n",
    "train_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91cd9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b24acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d635385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import LearningCurveDisplay\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "\n",
    "LearningCurveDisplay.from_estimator(\n",
    "   SVC(kernel=\"linear\"), X, y, train_sizes=[50, 80, 110], cv=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6085b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "naive_bayes = GaussianNB()\n",
    "svc = SVC(kernel=\"rbf\", gamma=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96635bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LearningCurveDisplay, ShuffleSplit\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 6), sharey=True)\n",
    "\n",
    "common_params = {\n",
    "    \"X\": X,\n",
    "    \"y\": y,\n",
    "    \"train_sizes\": np.linspace(0.1, 1.0, 5),\n",
    "    \"cv\": ShuffleSplit(n_splits=50, test_size=0.2, random_state=0),\n",
    "    \"score_type\": \"both\",\n",
    "    \"n_jobs\": 4,\n",
    "    \"line_kw\": {\"marker\": \"o\"},\n",
    "    \"std_display_style\": \"fill_between\",\n",
    "    \"score_name\": \"Accuracy\",\n",
    "}\n",
    "\n",
    "for ax_idx, estimator in enumerate([naive_bayes, svc]):\n",
    "    LearningCurveDisplay.from_estimator(estimator, **common_params, ax=ax[ax_idx])\n",
    "    handles, label = ax[ax_idx].get_legend_handles_labels()\n",
    "    ax[ax_idx].legend(handles[:2], [\"Training Score\", \"Test Score\"])\n",
    "    ax[ax_idx].set_title(f\"Learning Curve for {estimator.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef5227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5805489f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9ea5885",
   "metadata": {},
   "source": [
    "## Metrics and scoring in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd00f42",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed412c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfea68c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b560c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63e002c6",
   "metadata": {},
   "source": [
    "## Example: Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets\n",
    "\n",
    "X,Y  = datasets.load_breast_cancer(return_X_y=True)\n",
    "\n",
    "print('Dataset Size : ',X.shape,Y.shape)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    train_size=0.80, test_size=0.20,\n",
    "                                                    stratify=Y,\n",
    "                                                    random_state=1)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print('Train/Test Size : ', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=123)\n",
    "\n",
    "log_reg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cf9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_preds = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e042dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(Y_preds[:15])\n",
    "print(Y_test[:15])\n",
    "\n",
    "print('Test Accuracy     : {:.3f}'.format(accuracy_score(Y_test, Y_preds)))\n",
    "print('Test Accuracy     : {:.3f}'.format(log_reg.score(X_test, Y_test))) ## Score method also evaluates accuracy for classification models.\n",
    "print('Training Accuracy : {:.3f}'.format(log_reg.score(X_train, Y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d862d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(Y_test, Y_preds)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81745c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "\n",
    "print('Precision                                   : %.3f'%precision_score(Y_test, Y_preds))\n",
    "print('Recall                                      : %.3f'%recall_score(Y_test, Y_preds))\n",
    "print('F1-Score                                    : %.3f'%f1_score(Y_test, Y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b58dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPrecision Recall F1-Score Support Per Class : \\n',precision_recall_fscore_support(Y_test, Y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c802ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nClassification Report : ')\n",
    "print(classification_report(Y_test, Y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7850e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, log_reg.decision_function(X_test))\n",
    "\n",
    "auc = roc_auc_score(Y_test, log_reg.decision_function(X_test))\n",
    "\n",
    "acc = log_reg.score(X_test, Y_test)\n",
    "\n",
    "print(\"False Positive Rates : {}\".format(fpr))\n",
    "print(\"True  Positive Rates : {}\".format(tpr))\n",
    "print(\"Threshols            : {}\".format(thresholds))\n",
    "print(\"Accuracy             : {:.3f}\".format(acc))\n",
    "print(\"AUC                  : {:.3f}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56145332",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_disp = RocCurveDisplay.from_estimator(log_reg, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc,average_precision_score\n",
    "\n",
    "#precision, recall, thresholds = precision_recall_curve(Y_test, log_reg.predict_proba(X_test)[:,1])\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, log_reg.decision_function(X_test))\n",
    "\n",
    "acc = log_reg.score(X_test, Y_test)\n",
    "\n",
    "p_auc = auc(recall, precision)\n",
    "\n",
    "print(\"Accuracy  : {:.3f}\".format(acc))\n",
    "print(\"AUC       : {:.3f}\".format(p_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9305a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context(('ggplot', 'seaborn')):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(recall, precision, c='blue')\n",
    "    plt.plot(recall, precision, label=\"Accuray:%.2f, AUC:%.2f\" % (acc, p_auc), linewidth=2, c='red')\n",
    "    plt.hlines(0.5,0.0,1.0, linestyle='dashed', colors=['orange'])\n",
    "    plt.xlabel(\"Recall (Sensitivity)\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title('Precision Recall Curve')\n",
    "    plt.legend(loc='best');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a71488",
   "metadata": {},
   "source": [
    "## Using Evaluation Metrics in Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89cce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "# default scoring for classification is accuracy\n",
    "print(f\"Default scoring: {cross_val_score(SVC(), digits.data, digits.target == 9, cv=3)}\")\n",
    "\n",
    "# providing scoring=\"accuracy\" doesn't change the results\n",
    "explicit_accuracy = cross_val_score(SVC(), digits.data, digits.target == 9, scoring=\"accuracy\", cv=3)\n",
    "print(f\"Explicit accuracy scoring: {explicit_accuracy}\")\n",
    "\n",
    "roc_auc = cross_val_score(SVC(), digits.data, digits.target == 9, scoring=\"roc_auc\", cv=3)\n",
    "print(f\"AUC scoring: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97cfc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target == 9, random_state=0)\n",
    "\n",
    "# we provide a somewhat bad grid to illustrate the point:\n",
    "param_grid = {'gamma': [0.0001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "# using the default scoring of accuracy:\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Grid-Search with accuracy\")\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(f\"Best cross-validation score (accuracy)): {grid.best_score_:.3f}\")\n",
    "print(f\"Test set AUC: {roc_auc_score(y_test, grid.decision_function(X_test)):.3f}\")\n",
    "print(f\"Test set accuracy: {grid.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using AUC scoring instead:\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, scoring=\"roc_auc\")\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"\\nGrid-Search with AUC\")\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(f\"Best cross-validation score (AUC): {grid.best_score_:.3f}\")\n",
    "print(f\"Test set AUC: {roc_auc_score(y_test, grid.decision_function(X_test)):.3f}\")\n",
    "print(f\"Test set accuracy: {grid.score(X_test, y_test):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
