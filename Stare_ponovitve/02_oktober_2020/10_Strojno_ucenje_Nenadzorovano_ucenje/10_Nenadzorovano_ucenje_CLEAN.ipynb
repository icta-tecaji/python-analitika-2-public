{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Del 10: Strojno učenje: Nenadzorovano učenje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges in Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Kinds of Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import (StandardScaler, MinMaxScaler, Normalizer,\n",
    "                                   RobustScaler)\n",
    "from plot_helpers import plot_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [StandardScaler(), \n",
    "           RobustScaler(), \n",
    "           MinMaxScaler(), \n",
    "           Normalizer(norm='l2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scaling(scalers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                    random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset properties before and after scaling\n",
    "print(f\"transformed shape: {X_train_scaled.shape}\")\n",
    "print(f\"per-feature minimum before scaling:\\n {X_train.min(axis=0)}\")\n",
    "print(f\"per-feature maximum before scaling:\\n {X_train.max(axis=0)}\")\n",
    "print(f\"per-feature minimum after scaling:\\n {X_train_scaled.min(axis=0)}\")\n",
    "print(f\"per-feature maximum after scaling:\\n {X_train_scaled.max(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print test data properties after scaling\n",
    "print(f\"per-feature minimum after scaling:\\n{X_test_scaled.min(axis=0)}\")\n",
    "print(f\"per-feature maximum after scaling:\\n{X_test_scaled.max(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Training and Test Data the Same Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from plot_helpers import cm2\n",
    "\n",
    "# make synthetic data\n",
    "X, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\n",
    "# split it into training and test sets\n",
    "X_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n",
    "\n",
    "# plot the training and test sets\n",
    "fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "axes[0].scatter(X_train[:, 0], X_train[:, 1],\n",
    "                color=cm2(0), label=\"Training set\", s=60)\n",
    "axes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^',\n",
    "                color=cm2(1), label=\"Test set\", s=60)\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].set_title(\"Original Data\")\n",
    "\n",
    "# scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# visualize the properly scaled data\n",
    "axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
    "                color=cm2(0), label=\"Training set\", s=60)\n",
    "axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',\n",
    "                color=cm2(1), label=\"Test set\", s=60)\n",
    "axes[1].set_title(\"Scaled Data\")\n",
    "\n",
    "# rescale the test set separately\n",
    "# so test set min is 0 and test set max is 1\n",
    "# DO NOT DO THIS! For illustration purposes only.\n",
    "test_scaler = MinMaxScaler()\n",
    "test_scaler.fit(X_test)\n",
    "X_test_scaled_badly = test_scaler.transform(X_test)\n",
    "\n",
    "# visualize wrongly scaled data\n",
    "axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
    "                color=cm2(0), label=\"training set\", s=60)\n",
    "axes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],\n",
    "                marker='^', color=cm2(1), label=\"test set\", s=60)\n",
    "axes[2].set_title(\"Improperly Scaled Data\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# calling fit and transform in sequence (using method chaining)\n",
    "X_scaled = scaler.fit(X).transform(X)\n",
    "\n",
    "# same result, but more efficient computation\n",
    "X_scaled_d = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Effect of Preprocessing on Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "svm = SVC(C=100)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Test set accuracy: {svm.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "# preprocessing using 0-1 scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# learning an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# scoring on the scaled test set\n",
    "print(f\"Scaled test set accuracy: {svm.score(X_test_scaled, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [StandardScaler(), \n",
    "           RobustScaler(), \n",
    "           MinMaxScaler(), \n",
    "           Normalizer(norm='l2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scaler in scalers:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "    \n",
    "    scaler_ = scaler\n",
    "    scaler_.fit(X_train)\n",
    "    \n",
    "    X_train_scaled = scaler_.transform(X_train)\n",
    "    X_test_scaled = scaler_.transform(X_test)\n",
    "    \n",
    "    svm = SVC(C=100)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(f\"Scaler: {type(scaler).__name__}. SVM test accuracy: {svm.score(X_test_scaled, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction, Feature Extraction, and Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_helpers import plot_pca_illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_illustration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_helpers import cm3\n",
    "\n",
    "fig, axes = plt.subplots(15, 2, figsize=(10, 20))\n",
    "\n",
    "malignant = cancer.data[cancer.target == 0]\n",
    "benign = cancer.data[cancer.target == 1]\n",
    "\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(30):\n",
    "    _, bins = np.histogram(cancer.data[:, i], bins=50)\n",
    "    ax[i].hist(malignant[:, i], bins=bins, color=cm3(0), alpha=.5)\n",
    "    ax[i].hist(benign[:, i], bins=bins, color=cm3(2), alpha=.5)\n",
    "    ax[i].set_title(cancer.feature_names[i])\n",
    "    ax[i].set_yticks(())\n",
    "    \n",
    "ax[0].set_xlabel(\"Feature magnitude\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[0].legend([\"malignant\", \"benign\"], loc=\"best\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(cancer.data)\n",
    "\n",
    "X_scaled = scaler.transform(cancer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# keep the first two principal components of the data\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# fit PCA model to breast cancer data\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# transform data onto the first two principal components\n",
    "X_pca = pca.transform(X_scaled)\n",
    "\n",
    "print(f\"Original shape: {str(X_scaled.shape)}\")\n",
    "print(f\"Reduced shape: {str(X_pca.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=cancer.target, legend=False)\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")\n",
    "#plt.legend(cancer.target_names, loc=\"best\")\n",
    "plt.legend(loc='upper right', labels=['Malignat', 'Benign'])\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PCA component shape: {pca.components_.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(pca.components_, cmap='viridis')\n",
    "plt.yticks([0, 1], [\"First component\", \"Second component\"])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(cancer.feature_names)),\n",
    "cancer.feature_names, rotation=60, ha='left')\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Principal components\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying NMF to synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_helpers import plot_nmf_illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nmf_illustration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold Learning with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5),subplot_kw={'xticks':(), 'yticks': ()})\n",
    "\n",
    "for ax, img in zip(axes.ravel(), digits.images):\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a PCA model\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca.fit(digits.data)\n",
    "\n",
    "# transform the digits data onto the first two principal components\n",
    "digits_pca = pca.transform(digits.data)\n",
    "\n",
    "colors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\", \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\", \"#535D8E\"]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\n",
    "plt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\n",
    "for i in range(len(digits.data)):\n",
    "    # actually plot the digits as text instead of using scatter\n",
    "    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]), color = colors[digits.target[i]], fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(random_state=42)\n",
    "\n",
    "# use fit_transform instead of fit, as TSNE has no transform method\n",
    "digits_tsne = tsne.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\n",
    "plt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\n",
    "for i in range(len(digits.data)):\n",
    "    # actually plot the digits as text instead of using scatter\n",
    "    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]), color = colors[digits.target[i]], fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "plt.xlabel(\"t-SNE feature 0\")\n",
    "plt.xlabel(\"t-SNE feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Is Clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Clustering Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Partitional clustering\n",
    "2. Hierarchical clustering\n",
    "3. Density-based clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitional Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density-Based Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the K-Means Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img loading=\"lazy\" class=\"img-fluid mx-auto d-block \" src=\"https://files.realpython.com/media/kmeans-algorithm.a94498a7ecd2.png\" width=\"1186\" height=\"332\" srcset=\"https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/kmeans-algorithm.a94498a7ecd2.png&amp;w=296&amp;sig=efa46cf1bfa03d5dac763ccfd1e1ed573afdc86a 296w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/kmeans-algorithm.a94498a7ecd2.png&amp;w=593&amp;sig=5d5c01463b76abb809eb35ae90ca916bb34d2bb7 593w, https://files.realpython.com/media/kmeans-algorithm.a94498a7ecd2.png 1186w\" sizes=\"75vw\" alt=\"k means algorithm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img loading=\"lazy\" class=\"img-fluid mx-auto d-block w-50\" src=\"https://files.realpython.com/media/centroids_iterations.247379590275.gif\" width=\"576\" height=\"576\" srcset=\"https://files.realpython.com/media/centroids_iterations.247379590275.gif 144w, https://files.realpython.com/media/centroids_iterations.247379590275.gif 288w, https://files.realpython.com/media/centroids_iterations.247379590275.gif 576w\" sizes=\"75vw\" alt=\"k means centroids iterations\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying k-means with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# generate synthetic two-dimensional data\n",
    "X, y = make_blobs(random_state=1)\n",
    "\n",
    "# build the clustering model\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cluster memberships:\\n{kmeans.labels_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_helpers import discrete_scatter\n",
    "\n",
    "discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')\n",
    "discrete_scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2], markers='o', markeredgewidth=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# using two cluster centers:\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "assignments = kmeans.labels_\n",
    "discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[0])\n",
    "\n",
    "# using five cluster centers:\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(X)\n",
    "assignments = kmeans.labels_\n",
    "discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure cases of k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_varied, y_varied = make_blobs(n_samples=200, cluster_std=[1.0, 2.5, 0.5], random_state=170)\n",
    "\n",
    "y_pred = KMeans(n_clusters=3, random_state=0).fit_predict(X_varied)\n",
    "\n",
    "discrete_scatter(X_varied[:, 0], X_varied[:, 1], y_pred)\n",
    "\n",
    "plt.legend([\"cluster 0\", \"cluster 1\", \"cluster 2\"], loc='best')\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_helpers import cm3\n",
    "\n",
    "# generate some random cluster data\n",
    "X, y = make_blobs(random_state=170, n_samples=600)\n",
    "rng = np.random.RandomState(74)\n",
    "\n",
    "# transform the data to be stretched\n",
    "transformation = rng.normal(size=(2, 2))\n",
    "X = np.dot(X, transformation)\n",
    "\n",
    "# cluster the data into three clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "# plot the cluster assignments and cluster centers\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=cm3)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='^', c='y', s=100, linewidth=2, cmap=cm3)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer: Enostaven Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, true_labels = make_blobs(n_samples=200, centers=3, cluster_std=2.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters=3,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = kmeans.predict(scaled_features)\n",
    "discrete_scatter(scaled_features[:, 0], scaled_features[:, 1], y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lowest SSE value\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final locations of the centroid\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of iterations required to converge\n",
    "kmeans.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Appropriate Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The elbow method\n",
    "kmeans_kwargs = {\"init\": \"random\", \"n_init\": 10, \"max_iter\": 300,\"random_state\": 42,}\n",
    "\n",
    "# A list holds the SSE values for each k\n",
    "sse = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(scaled_features)\n",
    "    sse.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1, 11), sse)\n",
    "plt.xticks(range(1, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = KneeLocator(range(1, 11), sse, curve=\"convex\", direction=\"decreasing\")\n",
    "kl.elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list holds the silhouette coefficients for each k\n",
    "silhouette_coefficients = []\n",
    "\n",
    "# Notice you start at 2 clusters for silhouette coefficient\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(scaled_features)\n",
    "    score = silhouette_score(scaled_features, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(2, 11), silhouette_coefficients)\n",
    "plt.xticks(range(2, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer:  K-Means Clustering Pipeline in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a K-Means Clustering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uci_tcga_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00401/\"\n",
    "archive_name = \"TCGA-PANCAN-HiSeq-801x20531.tar.gz\"\n",
    "\n",
    "def download_data(uci_tcga_url, archive_name, path='data'):\n",
    "    save_path = f\"{path}/{archive_name}\"\n",
    "    \n",
    "    # Build the url\n",
    "    full_download_url = urllib.parse.urljoin(uci_tcga_url, archive_name)\n",
    "\n",
    "    # Download the file\n",
    "    r = urllib.request.urlretrieve (full_download_url, save_path)\n",
    "\n",
    "    # Extract the data from the archive\n",
    "    with tarfile.open(save_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Izbrišemo podatke preden damo na git!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(uci_tcga_url, archive_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = \"data/TCGA-PANCAN-HiSeq-801x20531/data.csv\"\n",
    "labels_file = \"data/TCGA-PANCAN-HiSeq-801x20531/labels.csv\"\n",
    "\n",
    "data = np.genfromtxt(datafile, delimiter=\",\", usecols=range(1, 20532), skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label_names = np.genfromtxt(labels_file, delimiter=\",\", usecols=(1,), skip_header=1, dtype=\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = label_encoder.fit_transform(true_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "        (\"pca\", PCA(n_components=2, random_state=42)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = Pipeline(\n",
    "   [\n",
    "       (\n",
    "           \"kmeans\",\n",
    "           KMeans(\n",
    "               n_clusters=n_clusters,\n",
    "               init=\"k-means++\",\n",
    "               n_init=50,\n",
    "               max_iter=500,\n",
    "               random_state=42,\n",
    "           ),\n",
    "       ),\n",
    "   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"clusterer\", clusterer)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = pipe[\"preprocessor\"].transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = pipe[\"clusterer\"][\"kmeans\"].labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(preprocessed_data, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_rand_score(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcadf = pd.DataFrame(\n",
    "    pipe[\"preprocessor\"].transform(data),\n",
    "    columns=[\"component_1\", \"component_2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcadf[\"predicted_cluster\"] = pipe[\"clusterer\"][\"kmeans\"].labels_\n",
    "pcadf[\"true_label\"] = label_encoder.inverse_transform(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "scat = sns.scatterplot(\n",
    "    x = \"component_1\",\n",
    "    y= \"component_2\",\n",
    "    s=50,\n",
    "    data=pcadf,\n",
    "    hue=\"predicted_cluster\",\n",
    "    style=\"true_label\",\n",
    "    palette=\"Set2\",\n",
    ")\n",
    "\n",
    "scat.set_title(\n",
    "    \"Clustering results from TCGA Pan-Cancer\\nGene Expression Data\"\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning a K-Means Clustering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists to hold evaluation metrics\n",
    "silhouette_scores = []\n",
    "ari_scores = []\n",
    "for n in range(2, 11):\n",
    "    # This set the number of components for pca,\n",
    "    # but leaves other steps unchanged\n",
    "    pipe[\"preprocessor\"][\"pca\"].n_components = n\n",
    "    pipe.fit(data)\n",
    "\n",
    "    silhouette_coef = silhouette_score(\n",
    "        pipe[\"preprocessor\"].transform(data),\n",
    "        pipe[\"clusterer\"][\"kmeans\"].labels_,\n",
    "    )\n",
    "    ari = adjusted_rand_score(\n",
    "        true_labels,\n",
    "        pipe[\"clusterer\"][\"kmeans\"].labels_,\n",
    "    )\n",
    "\n",
    "    # Add metrics to their lists\n",
    "    silhouette_scores.append(silhouette_coef)\n",
    "    ari_scores.append(ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(\n",
    "    range(2, 11),\n",
    "    silhouette_scores,\n",
    "    c=\"#008fd5\",\n",
    "    label=\"Silhouette Coefficient\",\n",
    ")\n",
    "\n",
    "plt.plot(range(2, 11), ari_scores, c=\"#fc4f30\", label=\"ARI\")\n",
    "\n",
    "plt.xlabel(\"n_components\")\n",
    "plt.legend()\n",
    "plt.title(\"Clustering Performance as a Function of n_components\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from plot_helpers import discrete_scatter\n",
    "\n",
    "X, y = make_blobs(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "assignment = agg.fit_predict(X)\n",
    "\n",
    "discrete_scatter(X[:, 0], X[:, 1], assignment)\n",
    "\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical clustering and dendrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dendrogram function and the ward clustering function from SciPy\n",
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "\n",
    "X, y = make_blobs(random_state=0, n_samples=12)\n",
    "\n",
    "# Apply the ward clustering to the data array X\n",
    "# The SciPy ward function returns an array that specifies the distances\n",
    "# bridged when performing agglomerative clustering\n",
    "linkage_array = ward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we plot the dendrogram for the linkage_array containing the distances\n",
    "# between clusters\n",
    "dendrogram(linkage_array)\n",
    "\n",
    "# Mark the cuts in the tree that signify two or three clusters\n",
    "ax = plt.gca()\n",
    "bounds = ax.get_xbound()\n",
    "ax.plot(bounds, [7.25, 7.25], '--', c='k')\n",
    "ax.plot(bounds, [4, 4], '--', c='k')\n",
    "ax.text(bounds[1], 7.25, ' two clusters', va='center', fontdict={'size': 15})\n",
    "ax.text(bounds[1], 4, ' three clusters', va='center', fontdict={'size': 15})\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "assignment = agg.fit_predict(X)\n",
    "\n",
    "discrete_scatter(X[:, 0], X[:, 1], assignment)\n",
    "\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X, y = make_blobs(random_state=0, n_samples=12)\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X)\n",
    "print(f\"Cluster memberships:\\n{clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_helpers import plot_dbscan\n",
    "plot_dbscan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons\n",
    "from plot_helpers import cm2\n",
    "\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# rescale the data to zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# plot the cluster assignments\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=cm2, s=60)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing and Evaluating Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating clustering with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from plot_helpers import cm3\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# rescale the data to zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of algorithms to use\n",
    "algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2), DBSCAN()]\n",
    "\n",
    "# create a random cluster assignment for reference\n",
    "random_state = np.random.RandomState(seed=0)\n",
    "random_clusters = random_state.randint(low=0, high=2, size=len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(15, 3),\n",
    "subplot_kw={'xticks': (), 'yticks': ()})\n",
    "\n",
    "# plot random assignment\n",
    "axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters, cmap=cm3, s=60)\n",
    "axes[0].set_title(f\"Random assignment - ARI: {adjusted_rand_score(y, random_clusters):.2f}\")\n",
    "\n",
    "\n",
    "for ax, algorithm in zip(axes[1:], algorithms):\n",
    "    # plot the cluster assignments and cluster centers\n",
    "    clusters = algorithm.fit_predict(X_scaled)\n",
    "    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=cm3, s=60)\n",
    "    ax.set_title(f\"{algorithm.__class__.__name__} - ARI: {adjusted_rand_score(y, clusters):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# these two labelings of points correspond to the same clustering\n",
    "clusters1 = [0, 0, 1, 1, 0]\n",
    "clusters2 = [1, 1, 0, 0, 1]\n",
    "\n",
    "# accuracy is zero, as none of the labels are the same\n",
    "print(f\"Accuracy: {accuracy_score(clusters1, clusters2):.2f}\")\n",
    "# adjusted rand score is 1, as the clustering is exactly the same\n",
    "print(f\"ARI: {adjusted_rand_score(clusters1, clusters2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating clustering without ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import silhouette_score\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# rescale the data to zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random cluster assignment for reference\n",
    "random_state = np.random.RandomState(seed=0)\n",
    "random_clusters = random_state.randint(low=0, high=2, size=len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(15, 3),\n",
    "subplot_kw={'xticks': (), 'yticks': ()})\n",
    "\n",
    "# plot random assignment\n",
    "axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters, cmap=cm3, s=60)\n",
    "axes[0].set_title(f\"Random assignment: {silhouette_score(X_scaled, random_clusters):.2f}\")\n",
    "\n",
    "\n",
    "for ax, algorithm in zip(axes[1:], algorithms):\n",
    "    # plot the cluster assignments and cluster centers\n",
    "    clusters = algorithm.fit_predict(X_scaled)\n",
    "    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=cm3, s=60)\n",
    "    ax.set_title(f\"{algorithm.__class__.__name__}: {silhouette_score(X_scaled, clusters):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
