{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Kaj-so-nevronske-mreže\" data-toc-modified-id=\"Kaj-so-nevronske-mreže-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Kaj so nevronske mreže</a></span></li><li><span><a href=\"#Nevron\" data-toc-modified-id=\"Nevron-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Nevron</a></span></li><li><span><a href=\"#Plast-nevronov\" data-toc-modified-id=\"Plast-nevronov-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Plast nevronov</a></span></li><li><span><a href=\"#Activation-functions\" data-toc-modified-id=\"Activation-functions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Activation functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-Activation-Function\" data-toc-modified-id=\"Step-Activation-Function-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Step Activation Function</a></span></li><li><span><a href=\"#Sigmoid-Activation-Function\" data-toc-modified-id=\"Sigmoid-Activation-Function-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Sigmoid Activation Function</a></span></li><li><span><a href=\"#Rectified-Linear-Units---ReLU\" data-toc-modified-id=\"Rectified-Linear-Units---ReLU-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Rectified Linear Units - ReLU</a></span></li><li><span><a href=\"#Uporaba-aktivacijske-funkcije-v-naši-neuronski-merži\" data-toc-modified-id=\"Uporaba-aktivacijske-funkcije-v-naši-neuronski-merži-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Uporaba aktivacijske funkcije v naši neuronski merži</a></span></li><li><span><a href=\"#Classification-dataset\" data-toc-modified-id=\"Classification-dataset-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Classification dataset</a></span></li><li><span><a href=\"#Softmax-Activation-Function\" data-toc-modified-id=\"Softmax-Activation-Function-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Softmax Activation Function</a></span></li></ul></li><li><span><a href=\"#Loss-Function-and-Accuracy\" data-toc-modified-id=\"Loss-Function-and-Accuracy-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Loss Function and Accuracy</a></span><ul class=\"toc-item\"><li><span><a href=\"#Categorical-Cross-Entropy-Loss\" data-toc-modified-id=\"Categorical-Cross-Entropy-Loss-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Categorical Cross-Entropy Loss</a></span></li></ul></li><li><span><a href=\"#Optimizing\" data-toc-modified-id=\"Optimizing-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Optimizing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random-search\" data-toc-modified-id=\"Random-search-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Random search</a></span></li><li><span><a href=\"#Fraction-Change\" data-toc-modified-id=\"Fraction-Change-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Fraction Change</a></span></li></ul></li><li><span><a href=\"#Partial-derivatieves\" data-toc-modified-id=\"Partial-derivatieves-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Partial derivatieves</a></span><ul class=\"toc-item\"><li><span><a href=\"#Partial-derivative-for-1-neuron\" data-toc-modified-id=\"Partial-derivative-for-1-neuron-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Partial derivative for 1 neuron</a></span></li></ul></li><li><span><a href=\"#Odvodi-Loss-FunctionS-in-Softmax-layer\" data-toc-modified-id=\"Odvodi-Loss-FunctionS-in-Softmax-layer-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Odvodi Loss FunctionS in Softmax layer</a></span></li><li><span><a href=\"#Optimizers\" data-toc-modified-id=\"Optimizers-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Optimizers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stochastic-Gradient-Descent-(SGD)-Optimizer\" data-toc-modified-id=\"Stochastic-Gradient-Descent-(SGD)-Optimizer-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Stochastic Gradient Descent (SGD) Optimizer</a></span></li></ul></li><li><span><a href=\"#Training-in-epochos\" data-toc-modified-id=\"Training-in-epochos-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Training in epochos</a></span></li><li><span><a href=\"#Realni-primer---uporaba-Keras-in-Tensorflow-knjižnjic\" data-toc-modified-id=\"Realni-primer---uporaba-Keras-in-Tensorflow-knjižnjic-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Realni primer - uporaba Keras in Tensorflow knjižnjic</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Load data</a></span></li><li><span><a href=\"#Data-preprocessing\" data-toc-modified-id=\"Data-preprocessing-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>Data preprocessing</a></span></li><li><span><a href=\"#Data-scaling\" data-toc-modified-id=\"Data-scaling-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>Data scaling</a></span></li><li><span><a href=\"#Creating-Neural-Network-Model\" data-toc-modified-id=\"Creating-Neural-Network-Model-11.4\"><span class=\"toc-item-num\">11.4&nbsp;&nbsp;</span>Creating Neural Network Model</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-11.5\"><span class=\"toc-item-num\">11.5&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Evaluating-the-model\" data-toc-modified-id=\"Evaluating-the-model-11.6\"><span class=\"toc-item-num\">11.6&nbsp;&nbsp;</span>Evaluating the model</a></span></li><li><span><a href=\"#Making-predictions\" data-toc-modified-id=\"Making-predictions-11.7\"><span class=\"toc-item-num\">11.7&nbsp;&nbsp;</span>Making predictions</a></span></li></ul></li><li><span><a href=\"#TO-DO---Save-model\" data-toc-modified-id=\"TO-DO---Save-model-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>TO DO - Save model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaj so nevronske mreže\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevronske mreže so bile narejene kot računalniški model delovanja možganov. Z njihovo pomočjo poizkušamo aproksimirati nelinearne funkcije.\n",
    "\n",
    "Uporabljajo se pri avtonomni vožnji, diagnosticiranju bolezni, napovedovanju cen, napovedovanju 3D oblike proteinov, itd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevronske mreže so zgrajene iz večih plasti medseboj povezanih nevronov, kjer ima vsak nevron določene parametre katere lahko spreminjamo. \n",
    "\n",
    "Začetna plast nevronov sprejme vhodne vrednosti - kot so slika okolice, kemična sestava molekule, itd. - jih spremeni glede na določeno funkcijo in svoje parametre ter te vrednosti posreduje naslednji plasti. Tako vrednosti potujejo do zadnje plasti, ki nam poda končno napoved - ali avto vidi sosednje avte ali ne, način kako se bo protein oblikoval, itd. Če so parametri nevronov pravilno izbrani potem so izhodne vrednosti nevronske mreže blizu resničnim vrednostim. \n",
    "\n",
    "Kako blizu smo resničnim vrednostim lahko matematično izračunamo. Dobljeno vrednost lahko nato uporabimo, da z njeno pomočjo posodobimo parametre nevronov in se tako bolj približamo resnični vrednosti. Na tak način se nevronska mreža uči."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za začetek si poglejmo kako je sestavljen 1 nevron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nevron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevron je sestavljen iz :\n",
    "* vhodnih signalov (*angl.* **inputs**), \n",
    "* uteži (*angl.* **weights**),\n",
    "* praga (*angl.* **bias**)\n",
    "\n",
    "**Inputs** predstavljajo vrednosti, ki so posredovane v neuron. To so lahko značilke (teža, kvadratna površina stanovanja, barva pixla, itd..), lahko pa so vrednosti neuronov iz prednodnje plasti.\n",
    "\n",
    "**Weights** so parametri, ki predstavljajo jakost posameznega input-a. Vsak input v nevron ima svoj specifičen weight.\n",
    "\n",
    "**Bias** je dodaten parameter, specifičen za vsak nevron.\n",
    "\n",
    "Tekom treniranja naše mreže je cilj spreminjati parametre nevronov, tako da, čim bolj natančno napovemo izhodne vrednosti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vrednost enega nevrona se izračuna po enačbi:\n",
    "\n",
    "$\\Large o = \\sum_{j=0}^{n}(i_j w_j) + b$\n",
    "\n",
    "* $i_j$ je specifičen input\n",
    "* $w_j$ je weight specifičnega inputa\n",
    "* $b$   je bias nevrona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za primer bomo sprogramirali delovanje enega nevrona. \n",
    "* V nevron vstopajo 3 input vrednosti: 1,2,3\n",
    "* Vsak input ima svoj specifični weight: 0.2, 0.8, -0.5\n",
    "* Nevron ima svoj specifični bias: 2\n",
    "\n",
    "![Nevron](notebook_images/01.PNG)\n",
    "\n",
    "Izhodno vrednost dobimo po zgornji enačbi:\n",
    "\n",
    "$\\Large o = \\sum_{j=0}^{n}(i_j w_j) + b = i_0 \\cdot w_0 + i_1 \\cdot w_1 + i_2 \\cdot w_2 + b = 1 \\cdot 0.2 + 2 \\cdot 0.8 + 3 \\cdot (-0.5) + 2 = 2.3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1,2,3]\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "bias = 2\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V primeru večih inputov nevron dobi dodatne weights - dodaten weight za vsak nov input. Bias ostane samo eden saj je le-ta specifičen za nevron.\n",
    "\n",
    "![Nevron - 4 inputs](notebook_images/02.PNG)\n",
    "\n",
    "$o = \\sum_{j=0}^{n}(i_j w_j) + b = i_0 \\cdot w_0 + i_1 \\cdot w_1 + i_2 \\cdot w_2 + i_3 \\cdot w_3 + b = 1 \\cdot 0.2 + 2 \\cdot 0.8 + 3 \\cdot (-0.5) + 2.5 \\cdot 1.0 + 2 = 4.8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1,2,3,2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za hitrejše računanje se uporablja matematične operacije nad tensorji.\n",
    "\n",
    "Za izračun vrednosti neurona se lahko uporabi **dot product**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dot product:**\n",
    "\n",
    "$\\Large \\vec{a}^{\\,}\\cdot \\vec{b}^{\\,} = [1,2,3]\\cdot [2,3,4] = 1\\cdot 2 + 2\\cdot 3 + 3\\cdot 4 = 20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer računanja s tensorji za en neuron\n",
    "import numpy as np\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2.0\n",
    "\n",
    "\n",
    "output = np.dot(weights, inputs) + bias\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plast nevronov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevronske mreže so večinoma sestavljene iz večih plasti (*angl.* **layer**). Plast je preprosto skupina neuronov.\n",
    "\n",
    "V osnovi poznamo vhodno plast (*angl.* **input layer**), ki prejme značilke. Izhodno plast (*angl.* **output layer**), ki vrne napovedano vrednost. Ostale plasti so skrite plasti (*angl.* **hidden layer**). Če ima mreža več kot 1 skrito plast se imenuje **deep neural network**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za primer si poglejmo del nevronske mreže kjer imamo vhodno plast sestavljeno iz 4 nevronov, ki so nato povezani z naslednjo plastjo, ki ima 3 nevrone.\n",
    "\n",
    "Vsak izmed štirih nevronov je povezan na vse nevrone naslednje plasti, kar pomeni, da imamo **dense layer**. Razlika med nevroni v isti plasti so njihove weights in bias.\n",
    "\n",
    "![Dense layer](notebook_images/03.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3, 2.5]                \n",
    "weights1 = [0.2, 0.8, -0.5, 1.0]       # uteži neurona1\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]    # uteži neurona2\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]  # uteži neurona3\n",
    "\n",
    "bias1 = 2    # bias neurona1\n",
    "bias2 = 3    # bias neurona2\n",
    "bias3 = 0.5  # bias neurona3\n",
    "\n",
    "outputs = [\n",
    "    # Output Neuron 1:\n",
    "    inputs[0]*weights1[0] +\n",
    "    inputs[1]*weights1[1] +\n",
    "    inputs[2]*weights1[2] +\n",
    "    inputs[3]*weights1[3] + bias1,\n",
    "\n",
    "    # Output Neuron 2:\n",
    "    inputs[0]*weights2[0] +\n",
    "    inputs[1]*weights2[1] +\n",
    "    inputs[2]*weights2[2] +\n",
    "    inputs[3]*weights2[3] + bias2,\n",
    "\n",
    "    # Output Neuron 3:\n",
    "    inputs[0]*weights3[0] +\n",
    "    inputs[1]*weights3[1] +\n",
    "    inputs[2]*weights3[2] +\n",
    "    inputs[3]*weights3[3] + bias3\n",
    "]\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Ista koda, zapisana z **numpy.dot()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "layer_outputs = np.dot(weights, inputs) + biases\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sedaj dodamo še en dense layer 3eh nevronov. Inputs te so outputs prejšnje dense layer.\n",
    "\n",
    "![2 Dense layers](notebook_images/04.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "\n",
    "# Dense layer 1\n",
    "weights = [[ 0.20,  0.80, -0.50,  1.00],\n",
    "           [ 0.50, -0.91,  0.26, -0.50],\n",
    "           [-0.26, -0.27,  0.17,  0.87]]\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "# Dense layer 2\n",
    "weights2 = [[ 0.10, -0.14,  0.50],\n",
    "            [-0.50,  0.12, -0.33],\n",
    "            [-0.44,  0.73, -0.13]]\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "\n",
    "layer1_outputs = np.dot(weights, inputs) + biases\n",
    "layer2_outputs = np.dot(weights2, layer1_outputs) + biases2\n",
    "\n",
    "print(\"Layer 1: \", layer1_outputs)\n",
    "print(\"Layer 2: \", layer2_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodo sedaj spremenimo v python class.\n",
    "\n",
    "Weights parametre inicializiramo naključno.\n",
    "\n",
    "Bias parametre inicializiramo na vrednost 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_neurons, n_inputs)\n",
    "        print(\"Weights: \")\n",
    "        print(self.weights)\n",
    "        self.biases = np.zeros(n_neurons)\n",
    "        print(\"Bias: \")\n",
    "        print(self.biases)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(self.weights, inputs) + self.biases\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "\n",
    "print(\"Creating DENSE 1\")\n",
    "dense1 = Layer_Dense(4, 3)\n",
    "print(\"Creating DENSE 2\")\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "dense1.forward(inputs)\n",
    "dense2.forward(dense1.output)\n",
    "\n",
    "print(\"Dense 1 output: \")\n",
    "print(dense1.output)\n",
    "print(\"Dense 2 outut: \")\n",
    "print(dense2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuronske mreže ne treniramo z vsakim vzorcem posebej, vendar jih treniramo z večimi vzorci naenkrat. Na tak način se zagotavlja, da se model ne prilagaja vsakemu vzorcu posebej ampak se prilagaja na več vzorcev naenkrat. Tako je model sposoben znanje posplošiti na še ne videne podatke in se izogibamo overfitting-u.\n",
    "\n",
    "Dobra stran treniranja na večih vzorcih naenkrat je tudi zmožnost uporabljanja večih procesorjev kar dodatno pohitri čas učenja. Za ta namen se uporablja GPUs oziroma TPUs (tensor processing units)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za primer vzemimo batch, kjer imamo 3 vzorce:\n",
    "```python\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],  # sample 1\n",
    "          [2.0, 5.0, -1.0, 2.0], # sample 2\n",
    "          [-1.5, 2.7, 3.3, -0.8]]# sample 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "layer_outputs = np.dot(np.array(inputs), np.array(weights)) + biases\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sedaj dobimo error, saj se dimenzije inputs in weights ne poravnajo za izvedbo računanja.\n",
    "\n",
    "Wights želimo v obliki:\n",
    "```python\n",
    "[[ 0.2 ,  0.5 , -0.26],\n",
    "[ 0.8 , -0.91, -0.27],\n",
    "[-0.5 ,  0.26,  0.17],\n",
    "[ 1.  , -0.5 ,  0.87]]\n",
    "```\n",
    "\n",
    "Kjer je stolpec 1 weights nevrona 1, stolpec 2 weights nevrona 2, stolpec 3 weights nevrona 3.\n",
    "\n",
    "Sedaj lahko uporabimo matrični račun:\n",
    "\n",
    "![Matrix calculation](notebook_images/05.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],  # Sample 1\n",
    "          [2.0, 5.0, -1.0, 2.0], # Sample 2\n",
    "          [-1.5, 2.7, 3.3, -0.8]]# Sample 3\n",
    "\n",
    "#          # N1    # N2    # N3\n",
    "weights = [[ 0.2 ,  0.5 , -0.26],\n",
    "           [ 0.8 , -0.91, -0.27],\n",
    "           [-0.5 ,  0.26,  0.17],\n",
    "           [ 1.  , -0.5 ,  0.87]]\n",
    "\n",
    "#        # N1   # N2  # N3\n",
    "biases = [2.0,   3.0,  0.5]\n",
    "\n",
    "layer_outputs = np.dot(np.array(inputs), np.array(weights)) + biases\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posodobimo kodo v Layer_Dense() razredu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        # <=== HERE ===>\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        # <=== HERE ===>\n",
    "        print(\"Weights: \")\n",
    "        print(self.weights)\n",
    "        # <=== HERE ===>\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # <=== HERE ===>\n",
    "        print(\"Bias: \")\n",
    "        print(self.biases)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        # <=== HERE ===>\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        # <=== HERE ===>\n",
    "        \n",
    "inputs = np.array([[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]])\n",
    "print(\"Inputs:\")\n",
    "print(inputs)\n",
    "\n",
    "dense1 = Layer_Dense(4,3)\n",
    "dense1.forward(inputs)\n",
    "\n",
    "print(dense1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def linear_dataset(samples=50):\n",
    "    X = np.linspace(-1, 1, num=samples)\n",
    "    X = np.reshape(X, newshape=(X.shape[-1], 1))\n",
    "    y = 2*X + 1.5\n",
    "    y += np.random.uniform(low=-0.5, high=0.5, size=(y.shape))\n",
    "    return X, y\n",
    "\n",
    "X, y = linear_dataset(samples=50)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO ... trying to make interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.widgets import Slider  # import the Slider widget\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(2020)\n",
    "\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        print(\"Weights: \")\n",
    "        print(self.weights)\n",
    "        # <=== HERE ===>\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # <=== HERE ===>\n",
    "        print(\"Bias: \")\n",
    "        print(self.biases)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "# =================================================================\n",
    "# =================================================================\n",
    "# =================================================================\n",
    "\n",
    "\n",
    "X, y = linear_dataset(samples=5)\n",
    "print(\"Inputs:\")\n",
    "print(X)\n",
    "\n",
    "a_min = 0    # the minimial value of the paramater a\n",
    "a_max = 10   # the maximal value of the paramater a\n",
    "a_init = 1   # the value of the parameter a to be used initially, when the graph is created\n",
    "\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "\n",
    "# first we create the general layount of the figure\n",
    "# with two axes objects: one for the plot of the function\n",
    "# and the other for the slider\n",
    "sin_ax = plt.axes([0.1, 0.2, 0.8, 0.65])\n",
    "slider_ax = plt.axes([0.1, 0.05, 0.8, 0.05])\n",
    "\n",
    "\n",
    "# in plot_ax we plot the function with the initial value of the parameter a\n",
    "plt.axes(sin_ax) # select sin_ax\n",
    "plt.title('y = sin(ax)')\n",
    "sin_plot, = plt.plot(X[:, 0], y[:, 0], 'r')\n",
    "\n",
    "# here we create the slider\n",
    "a_slider = Slider(slider_ax,      # the axes object containing the slider\n",
    "                  'a',            # the name of the slider parameter\n",
    "                  a_min,          # minimal value of the parameter\n",
    "                  a_max,          # maximal value of the parameter\n",
    "                  valinit=a_init  # initial value of the parameter\n",
    "                 )\n",
    "\n",
    "# Next we define a function that will be executed each time the value\n",
    "# indicated by the slider changes. The variable of this function will\n",
    "# be assigned the value of the slider.\n",
    "def update(a):\n",
    "    X, y = linear_dataset(samples=5)\n",
    "    print(y)\n",
    "    sin_plot.plt(X[:, 0], y[:, 0]) # set new y-coordinates of the plotted points\n",
    "    fig.canvas.draw_idle()          # redraw the plot\n",
    "\n",
    "# the final step is to specify that the slider needs to\n",
    "# execute the above function when its value changes\n",
    "a_slider.on_changed(update)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.widgets import Slider  # import the Slider widget\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "a_min = 0    # the minimial value of the paramater a\n",
    "a_max = 10   # the maximal value of the paramater a\n",
    "a_init = 1   # the value of the parameter a to be used initially, when the graph is created\n",
    "\n",
    "x = np.linspace(0, 2*pi, 500)\n",
    "\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "\n",
    "# first we create the general layount of the figure\n",
    "# with two axes objects: one for the plot of the function\n",
    "# and the other for the slider\n",
    "sin_ax = plt.axes([0.1, 0.2, 0.8, 0.65])\n",
    "slider_ax = plt.axes([0.1, 0.05, 0.8, 0.05])\n",
    "\n",
    "\n",
    "# in plot_ax we plot the function with the initial value of the parameter a\n",
    "plt.axes(sin_ax) # select sin_ax\n",
    "plt.title('y = sin(ax)')\n",
    "sin_plot, = plt.plot(x, np.sin(a_init*x), 'r')\n",
    "plt.xlim(0, 2*pi)\n",
    "plt.ylim(-1.1, 1.1)\n",
    "\n",
    "# here we create the slider\n",
    "a_slider = Slider(slider_ax,      # the axes object containing the slider\n",
    "                  'a',            # the name of the slider parameter\n",
    "                  a_min,          # minimal value of the parameter\n",
    "                  a_max,          # maximal value of the parameter\n",
    "                  valinit=a_init  # initial value of the parameter\n",
    "                 )\n",
    "\n",
    "# Next we define a function that will be executed each time the value\n",
    "# indicated by the slider changes. The variable of this function will\n",
    "# be assigned the value of the slider.\n",
    "def update(a):\n",
    "    sin_plot.set_ydata(np.sin(a*x)) # set new y-coordinates of the plotted points\n",
    "    fig.canvas.draw_idle()          # redraw the plot\n",
    "\n",
    "# the final step is to specify that the slider needs to\n",
    "# execute the above function when its value changes\n",
    "a_slider.on_changed(update)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO lahko se da nek SSSIIIIIMMMPPLEEE linearni dataset in se gleda preprost accuracy -> kolk jih pravilno napovemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testiranje na exponential datasetu\n",
    "> Forward step\n",
    "> mogl bi it uredu. Ročno igranje s parametri..\n",
    "> Problem je, da lahko aproksimiramo samo linearno funkcijo\n",
    "> potrebno je uvest AKTIVACIJSKE FUNKCIJE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "\n",
    "Problem je, da lahko z našim trenutnim modelom aproksimiramo samo linearne funkcije.\n",
    "\n",
    "Da rešim to problem se vsakemu nevronu doda aktivacijsko funkcijo (*angl.* **activation function**). To je funkcija, ki sprejme seštevek vhodov, weights in bias-a in vrne neko novo vrednost. Izbira aktivacijske funkcije je pomembna saj le-ta vpliva na vrednosti izhodov, na hitrost učenja in zmožnostjo konvergiranja.\n",
    "\n",
    "Praksa je, da imajo vsi nevroni v isti plasti isto aktivacijsko funkcijo, ni pa potrebno, da je tako.\n",
    "\n",
    "![Activation function](notebook_images/06.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Activation Function\n",
    "\n",
    "Za začetek si poglejmo preprosto **step** aktivacijsko funkcijo. Njena formula je:\n",
    "\n",
    "$\\Large \n",
    "y = \n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t1  & x > 0 \\\\\n",
    "        0  & x \\leq 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def step_function(x):\n",
    "    return 1 if x>0 else 0\n",
    "\n",
    "X = np.arange(-10, 10, 0.01)\n",
    "y = [step_function(x) for x in X]\n",
    "\n",
    "plt.plot(X, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step funkcija je bila v uporabi v preteklosti, vendar se večinoma ne uporablja več.\n",
    "\n",
    "Problem se je pojavil, saj funkcija sama po sebi ni dovolj informativna:\n",
    "* Velike številke vrnejo isto vrednost kot pozitivne številke blizu ničle - vrednost 1\n",
    "* Isto velja za velike negativne številke in negativne številke blizu 0 - vrnejo vrednost 0.\n",
    "\n",
    "Pri aktivacijski funkciji želimo večjo granularnost saj tako lažje posodabljamo parametre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zato se je začelo uporabljati **Sigmoid activation function**. Njena definicija je:\n",
    "\n",
    "$\\Large y = \\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / ( 1 + np.exp(-x))\n",
    "\n",
    "X = np.arange(-10, 10, 0.01)\n",
    "y = [sigmoid(x) for x in X]\n",
    "\n",
    "plt.plot(X, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid vrne vrednosti:\n",
    "* 0 do 0.5 za negativna števila \n",
    "* 0.5 do 1 za pozitivna števila \n",
    "\n",
    "Funkcija je dosti bolj granularna in se veliko uporablja kot aktivacijska funkcija. Vendar pa je tudi dosti bolj kompleksna, kar poveča čas učenja. Funkcija ima tudi problem izginjajočega gradienta kar pomeni, da se pri izredno velikih ali majhnih vhodnih vrednosti izhodna vrednost skoraj ne spremeni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified Linear Units - ReLU\n",
    "\n",
    "Za reševanje teh problemov se je začela uporabljati ReLU funkcija katere definicija je:\n",
    "\n",
    "$\\Large \n",
    "y = \n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\tx  & x > 0 \\\\\n",
    "        0  & x \\leq 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return x if x>0 else 0\n",
    "\n",
    "X = np.arange(-10, 10, 0.01)\n",
    "y = [relu(x) for x in X]\n",
    "\n",
    "plt.plot(X, y)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uporaba aktivacijske funkcije v naši neuronski merži"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output valzes from input\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "n_samples = 4\n",
    "neurons = 3\n",
    "X = np.random.normal(size=(n_samples,neurons))\n",
    "print(\"Inputs:\")\n",
    "print(X)\n",
    "\n",
    "activation = Activation_ReLU()\n",
    "activation.forward(X)\n",
    "print(\"Output\")\n",
    "print(activation.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celotna koda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        print(\"Weights: \")\n",
    "        print(self.weights)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        print(\"Bias: \")\n",
    "        print(self.biases)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# <=== HERE ===>\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output valzes from input\n",
    "        self.output = np.maximum(0, inputs)\n",
    "# <=== HERE ===>\n",
    "        \n",
    "        \n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "dense1 = Layer_Dense(4, 3)\n",
    "# <=== HERE ===>\n",
    "activation1 = Activation_ReLU()\n",
    "# <=== HERE ===>\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "dense1.forward(inputs)\n",
    "# <=== HERE ===>\n",
    "activation1.forward(dense1.output)\n",
    "# <=== HERE ===>\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "print(\"Dense 1 output: \")\n",
    "print(dense1.output)\n",
    "# <=== HERE ===>\n",
    "print(\"ReLU 1 output:\")\n",
    "print(activation1.output)\n",
    "# <=== HERE ===>\n",
    "print(\"Dense 2 outut: \")\n",
    "print(dense2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO ... dodat en nelinearen primer in da se ročno igrajo s parametri?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification dataset\n",
    "\n",
    "V nadaljnem bomo uporabili težji dataset ter izvajali klasifikacijo treh različnih razredov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def vertical_data(samples, classes):\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        X[ix] = np.c_[np.random.randn(samples)*.1 + (class_number)/3, np.random.randn(samples)*.1 + 0.5]\n",
    "        y[ix] = class_number\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation Function\n",
    "\n",
    "Kot aktivacijsko funkcijo zadnje plasti bomo uporabili **Softmax**. Tako bomo kot napovedane izhodne vrednosti dobili % kako verjetno je, da vhodni podatki spadajo v določen razred - *confidence score*\n",
    "> Primer: Če bodo naše izhodne vrednosti [0.7, 0.2, 0.1] potem model s 70% natančnostjo verjame, da vhodni podatki pripadajo razredu 0. Z 20% natančnostjo, da so razreda 1 in 10% natančnostjo, da so podatki razreda 2.\n",
    "\n",
    "Enačba softmax funkcije je:\n",
    "\n",
    "$\\Large S_j = \\frac{e^{o_j}}{\\sum_{l=0}^{L}e^{o_l}}$\n",
    "\n",
    "* $S_j$ je confidence score $j$ razreda\n",
    "* $o_j$ je izhodna vrednost neurona\n",
    "* $\\sum_{l=0}^{L}e^{o_l}$ je seštevek $e^o$ vseh izhodnih vrednosti neuronov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(inputs):\n",
    "    exp_values = np.exp(layer_outputs)\n",
    "    return exp_values / np.sum(exp_values)\n",
    "\n",
    "layer_outputs = [1.0, 2.0, 3.0, 2.5]\n",
    "\n",
    "softmax_output = softmax(layer_outputs)\n",
    "print(softmax_output)\n",
    "print(sum(softmax_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs)\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "layer_outputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "softmax = Activation_Softmax()\n",
    "softmax.forward(layer_outputs)\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem se lahko pojavi pri prevelikih številkah, saj hitro lahko pride do errorja. Zato bomo pred začetkom izračuna Softmax vrednosti odšteli največjo vrednost od vhodnih vrednosti. S tem, ko odštejemo največjo vrednost končnega rezultata ne spremenimo. Spremenimo pa, da se vhodne vrednosti gibljejo od 1 - kar nam s potenciranjem vrne vrednost $e$ - do izredno majhne številke - kar nam s potenciranjem vrne vrednost blizu 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "        \n",
    "layer_outputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "softmax = Activation_Softmax()\n",
    "softmax.forward(layer_outputs)\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax koda in dataset dodana celotni kodi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "# <=== HERE ===>      \n",
    "        \n",
    "# <=== HERE ===>\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "# <=== HERE ===>\n",
    "\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "# <=== HERE ===>\n",
    "activation2 = Activation_Softmax()\n",
    "# <=== HERE ===>\n",
    "\n",
    "# <=== HERE ===>\n",
    "dense1.forward(X[:5])\n",
    "# <=== HERE ===>\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "# <=== HERE ===>\n",
    "activation2.forward(dense2.output)\n",
    "# <=== HERE ===>\n",
    "\n",
    "print(\"Dense 1 output: \")\n",
    "print(dense1.output)\n",
    "print(\"ReLU 1 output:\")\n",
    "print(activation1.output)\n",
    "print(\"Dense 2 outut: \")\n",
    "print(dense2.output)\n",
    "# <=== HERE ===>\n",
    "print(\"Softmax output - PREDICTION\")\n",
    "print(activation2.output)\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sedaj, ko imamo zgrajen model lahko specifične vhodne podatke pretvorimo v napovedano vrednost.\n",
    "> Na podlagi kvadrature stanovanja in lokacije lahko napovemo ceno\n",
    "\n",
    "> Iz dolžnie in širine lista lahko napovemo vrsto rastline\n",
    "\n",
    "\n",
    "Kako uspešen je model pri napvedovanju izračunamo s pomočjo **loss function** (drugo ime je tudi *cost function*). Vrednost loss function nam pove kako blizu resnični vrednosti je bila napovedana vrednost. Idealno bi želeli, da je vrednost loss function enaka 0.\n",
    "\n",
    "Model ocenjujemo tudi z vrednostjo **accuracy**, ki nam pove koliko vrednosti smo pravilno napovedali."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Cross-Entropy Loss\n",
    "\n",
    "Za naš primer klasifikacije bomo za izračun napake uporabili funkcijo **categorical cross-entropy loss**. Funkcija se uporablja za primerjavo resničnih vrednosti (*ground-truth* vrednosti) z napovedano distribucijo (softmax predictions).\n",
    "\n",
    "Enačba je sledeča:\n",
    "\n",
    "$\\Large L = - \\sum_{j}y_j log(\\hat{y_j})$\n",
    "\n",
    "* $L$ je **loss** vrednost\n",
    "* $y_j$ je resnična vrednost\n",
    "* $\\hat{y_j}$ je napovedana vrednost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za primer uzemimo sledeče vrednosti - model, kjer imamo kot zadnjo funkcijo Softmax, je napovedal sledeč *confidence score* [0.7, 0.1, 0.2]. S tem bi lahko našo točko ocenili, da spada v razred 0.\n",
    "\n",
    "Resnične vrednosti so sledeče [1, 0, 0], kar pomeni, da naša točka spada v razred 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [0.7, 0.1, 0.2]\n",
    "real_values =  [1, 0, 0]\n",
    "\n",
    "loss = -(real_values[0]*np.log(inputs[0]) +\n",
    "         real_values[1]*np.log(inputs[1]) +\n",
    "         real_values[2]*np.log(inputs[2]))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za naš specifičen primer lahko naredimo nekaj posplošitev.\n",
    "\n",
    "Ker lahko naša točka spada samo v 1 razred, vemo, da ima iskan razred vedno vrednost 1 in preostala dva razreda vedno vrednost 0. Zato lahko našo loss funkcijo posplošimo:\n",
    "\n",
    "$\\Large L = - \\sum_{j}y_j log(\\hat{y_j}) = \\\\ \n",
    "\\Large -( y_0 log(\\hat{y_0}) + y_1 log(\\hat{y_1}) + y_2 log(\\hat{y_2})) = \\\\ \n",
    "\\Large -(1 \\cdot log(\\hat{y_0}) + 0 \\cdot log(\\hat{y_1}) + 0 \\cdot log(\\hat{y_2})) = \\\\\n",
    "\\Large - log(\\hat{y_0}) = - log(\\hat{y_k})$\n",
    "\n",
    "* $k$ - index pravilnega razreda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [0.7, 0.1, 0.2]\n",
    "real_values =  [1, 0, 0]\n",
    "\n",
    "correct_class_index = real_values.index(1)\n",
    "print(\"Correct class index: \", correct_class_index)\n",
    "loss = -np.log(inputs[correct_class_index])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodaten problem se pojavi, če model za resnično vrednost napove 0, saj tako dobimo izraz:\n",
    "\n",
    "$log( 0 )$\n",
    "\n",
    "vredonst katerega je $-\\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [0.7, 0.0, 0.3]\n",
    "real_values =  [0, 1, 0]\n",
    "\n",
    "correct_class_index = real_values.index(1)\n",
    "print(\"Correct class index: \", correct_class_index)\n",
    "loss = -np.log(inputs[correct_class_index])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem rešimo tako, da najmanjšo vrednost povečamo za neko minimalno številko. Na tak način se znebimo problema z ničlo in na končni loss rezultat bistveno ne vplivamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-np.log(0.1)) # ne-korigiran rezultat\n",
    "print(-np.log(0.1+1e-7)) # korigiran rezultat\n",
    "# končna razlika med rezultatoma je skoraj neopazna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodaten problem se sedaj pojavi, če model napove vrednost 1, kjer imamo sedaj negativno vrednost loss funkcije. Pravilna vrednost pa bi morala biti 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-np.log(1+1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da se rešimo tega problema bomo največjo številko pomanjšal za neko minimalno vrednost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-np.log(1)) # pravilna in željena vrednost\n",
    "print(-np.log(1-1e-7)) # korigirana vrednost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obe dve rešitvi najdemo znotraj numpy paketa v funkciji *clip()*, ki vrednosti v listu, ki so zunaj specificiranega intervala postavi na vrednosti mej intervala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1.7, 0.0, 0.3]\n",
    "print(\"Inputs:\")\n",
    "print(inputs)\n",
    "cor_inputs = np.clip(inputs, 1e-7, 1-1e-7)\n",
    "print(\"Corrected inputs:\")\n",
    "print(cor_inputs)\n",
    "\n",
    "real_values =  [0, 1, 0]\n",
    "\n",
    "correct_class_index = real_values.index(1)\n",
    "print(\"Correct class index: \", correct_class_index)\n",
    "loss = -np.log(cor_inputs[correct_class_index])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stvar bomo sedaj zapisali v python razrede.\n",
    "\n",
    "Sprva ustvarimo splošni Loss razred. Ker izračunamo **loss** vrednosti večih sampl-ov naeankrat je potrebno končno vrnjeno loss vrednost povprečiti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nato sledi razred specifične loss funkcije. V našem primeru je to Categorical Crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "        print(\"Correct confidencesa:\")\n",
    "        print(correct_confidences)\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "#class_targets = np.array([0, 2, 2])\n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 1, 0]])\n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celotna koda do sedaj.\n",
    "\n",
    "Izračunati znamo vrednosti nevronov, dodamo jim lahko aktivacijsko funkcijo in na koncu lahko dobimo izhodni rezultat. Vsem korakom skupaj se reče **ONE FORWARD PASS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "# <=== HERE ===>\n",
    "\n",
    "        \n",
    "\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "# <=== HERE ===>\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# <=== HERE ===>\n",
    "\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "\n",
    "# Perform a forward pass through loss function\n",
    "# it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za konec dodajmo še preverjanje natančnosti (*angl.* **accuracy**). To je preprosto % vrednost koliko pravilnih napovedi je napravil model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "\n",
    "# Perform a forward pass through loss function\n",
    "# it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "\n",
    "\n",
    "# <=== HERE ===>\n",
    "\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=200)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"v\", s=100, edgecolors=\"black\")\n",
    "plt.show()\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sedaj znamo ustvariti model, kateri se inicializira z random weights in biases z vrednostimi 0. Znamo ocenit rezultate našega modela.\n",
    "\n",
    "Naslednji korak je izboljšati naš model. To pomeni spremeniti naše weights in biases tako, da bo končna loss vrednost bližje 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ustvarili bomo model z random inicializiranimi weights in biases\n",
    "2. Izračunali bomo njegovo loss vrednost\n",
    "3. Ustvarili bomo nov model z random inicializiranimi weights in biases\n",
    "4. Izračunali bomo loss vrednost novega modela\n",
    "5. 1. Če ima manjšo loss vrednost bomo obdržali novi model\n",
    "5. 2. Če ima večjo loss vrednost bomo obdržali stari model\n",
    "6. Ponovimo od 3. koraka naprej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "        \n",
    "# Create dataset\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "\n",
    "# Create model\n",
    "dense1 = Layer_Dense(2, 3)  # first dense layer, 2 inputs\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Helper variables\n",
    "lowest_loss = 9999999  # some initial value\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "for iteration in range(2_000):\n",
    "\n",
    "    # Generate a new set of weights for iteration\n",
    "    dense1.weights = 0.05 * np.random.randn(2, 3)\n",
    "    dense1.biases = 0.05 * np.random.randn(1, 3)\n",
    "    dense2.weights = 0.05 * np.random.randn(3, 3)\n",
    "    dense2.biases = 0.05 * np.random.randn(1, 3)\n",
    "\n",
    "    # Perform a forward pass of the training data through this layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # it takes the output of second dense layer here and returns loss\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    # If loss is smaller - print and save weights and biases aside\n",
    "    if loss < lowest_loss:\n",
    "        print('New set of weights found, iteration:', iteration,\n",
    "              'loss:', loss, 'acc:', accuracy)\n",
    "        best_dense1_weights = dense1.weights.copy()\n",
    "        best_dense1_biases = dense1.biases.copy()\n",
    "        best_dense2_weights = dense2.weights.copy()\n",
    "        best_dense2_biases = dense2.biases.copy()\n",
    "        lowest_loss = loss\n",
    "# <=== HERE ===>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=200)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"v\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidimo, da takšno iskanje optimalni parametrov ni učinkovito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraction Change\n",
    "\n",
    "Namesto, da spremenimo vse weights in biases bomo vsak parameter spremenili za neko majhno število. Če se bo model izkazal za boljšega bomo to spremembo obdržali, če se bo model izkazal za slabšega bomo to spremembo zavrgli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "        \n",
    "# Create dataset\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "\n",
    "# Create model\n",
    "dense1 = Layer_Dense(2, 3)  # first dense layer, 2 inputs\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "# Helper variables\n",
    "lowest_loss = 9999999  # some initial value\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "for iteration in range(2_000):\n",
    "    # <=== HERE ===>\n",
    "    # Generate a new set of weights for iteration\n",
    "    dense1.weights += 0.05 * np.random.randn(2, 3)\n",
    "    dense1.biases += 0.05 * np.random.randn(1, 3)\n",
    "    dense2.weights += 0.05 * np.random.randn(3, 3)\n",
    "    dense2.biases += 0.05 * np.random.randn(1, 3)\n",
    "    # <=== HERE ===>\n",
    "\n",
    "    # Perform a forward pass of the training data through this layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # it takes the output of second dense layer here and returns loss\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    # If loss is smaller - print and save weights and biases aside\n",
    "    if loss < lowest_loss:\n",
    "        print('New set of weights found, iteration:', iteration,\n",
    "              'loss:', loss, 'acc:', accuracy)\n",
    "        best_dense1_weights = dense1.weights.copy()\n",
    "        best_dense1_biases = dense1.biases.copy()\n",
    "        best_dense2_weights = dense2.weights.copy()\n",
    "        best_dense2_biases = dense2.biases.copy()\n",
    "        lowest_loss = loss\n",
    "    # Revert weights and biases\n",
    "    else:\n",
    "        dense1.weights = best_dense1_weights.copy()\n",
    "        dense1.biases = best_dense1_biases.copy()\n",
    "        dense2.weights = best_dense2_weights.copy()\n",
    "        dense2.biases = best_dense2_biases.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To je za TRAINING VALIDATION\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=200)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"v\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model smo izboljšali. Loss se je opazno zmanjšala in accuracy je opazno narastla.\n",
    "\n",
    "Vendar sta to loss in accuracy gledano na training dataset-u.\n",
    "\n",
    "Za bolj natančno predstavo si poglejmo rezultate na novem - testnem datasetu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "\n",
    "# Perform a forward pass of the training data through this layer\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "print(f\"Train Loss: {loss}, Train Accuracy: {accuracy}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=200)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"v\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss vrednost training dataseta je blizu loss vrednosti testing dataseta, kar pomeni, da je model dobro generaliziral svoje znanje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preverimo naš model na še bolj zapletenem datasetu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Copyright (c) 2015 Andrej Karpathy\n",
    "# License: https://github.com/cs231n/cs231n.github.io/blob/master/LICENSE\n",
    "# Source: https://cs231n.github.io/neural-networks-case-study/\n",
    "def spiral_data(samples, classes):\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, samples)\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, samples) + np.random.randn(samples)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y\n",
    "\n",
    "X, y = spiral_data(100, 3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "        \n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create model\n",
    "dense1 = Layer_Dense(2, 3)  # first dense layer, 2 inputs\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3, 3)  # second dense layer, 3 inputs, 3 outputs\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "# Helper variables\n",
    "lowest_loss = 9999999  # some initial value\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "for iteration in range(2_000):\n",
    "\n",
    "    # Generate a new set of weights for iteration\n",
    "    dense1.weights += 0.05 * np.random.randn(2, 3)\n",
    "    dense1.biases += 0.05 * np.random.randn(1, 3)\n",
    "    dense2.weights += 0.05 * np.random.randn(3, 3)\n",
    "    dense2.biases += 0.05 * np.random.randn(1, 3)\n",
    "\n",
    "    # Perform a forward pass of the training data through this layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # it takes the output of second dense layer here and returns loss\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    # If loss is smaller - print and save weights and biases aside\n",
    "    if loss < lowest_loss:\n",
    "        print('New set of weights found, iteration:', iteration,\n",
    "              'loss:', loss, 'acc:', accuracy)\n",
    "        best_dense1_weights = dense1.weights.copy()\n",
    "        best_dense1_biases = dense1.biases.copy()\n",
    "        best_dense2_weights = dense2.weights.copy()\n",
    "        best_dense2_biases = dense2.biases.copy()\n",
    "        lowest_loss = loss\n",
    "    # Revert weights and biases\n",
    "    else:\n",
    "        dense1.weights = best_dense1_weights.copy()\n",
    "        dense1.biases = best_dense1_biases.copy()\n",
    "        dense2.weights = best_dense2_weights.copy()\n",
    "        dense2.biases = best_dense2_biases.copy()\n",
    "\n",
    "\n",
    "# To je za TRAINING VALIDATION\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=200)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"v\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preverimo še testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Perform a forward pass of the training data through this layer\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "print(f\"Train Loss: {loss}, Train Accuracy: {accuracy}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=200)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"v\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model se ne odnese dobro na bolj zapletenem datsetu.\n",
    "\n",
    "Problem lahko rešimo tako, da parametre ne posodabljamo z random številko ampak jih posodabljamo glede na to kolikšen vpliv so imeli na loss funkcijo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO --- pogledat če lahko un graf pobarvan nrdim...\n",
    "\n",
    "LINKS:\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/04.04-density-and-contour-plots.html\n",
    "\n",
    "https://www.tutorialspoint.com/matplotlib/matplotlib_contour_plot.htm\n",
    "\n",
    "plt.contourf(x1, x2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "X_conturef = np.arange(0, 1, 0.01)\n",
    "X_data = np.empty(shape=(0, 2))\n",
    "print(X_data.shape)\n",
    "for x1 in X_conturef:\n",
    "    for x2 in X_conturef:\n",
    "        X_data = np.append(X_data, [[x1, x2]], axis=0)\n",
    "print(X_data.shape)\n",
    "print(X_data)\n",
    "\n",
    "# Perform a forward pass of the training data through this layer\n",
    "dense1.forward(X_data)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "print(f\"Train Loss: {loss}, Train Accuracy: {accuracy}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "#ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=200)\n",
    "ax.contourf(X_conturef, X_conturef, predictions, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial derivatieves\n",
    "\n",
    "O tem kakšen vpliv ima določena spremenljivka oziroma parameter na rezultat funkcije nam povejo parcialni odvodi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial derivative for 1 neuron\n",
    "\n",
    "Za začetek si bomo pogledali na primeru enega nevrona, ki je povezan na tri nevrone iz prejšnje plasti.\n",
    "\n",
    "Cilj je posodobiti weights in bias nevrona tako, da zmanjšamo vrednost njegove aktivacijske funkcije. To v praksi nima nobenega pomena, saj želimo zmanjšati loss funkcijo ampak je primer namenjen lažjemu razumevanju."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Partial derivatives of 1 neuron](notebook_images/07.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enačba enega neurona je sledeča:\n",
    "\n",
    "$\n",
    "\\Large output = ReLU(z) =  \n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\tz  & z > 0 \\\\\n",
    "        0  & z \\leq 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "\\\\\n",
    "\\Large z = w_0 \\cdot i_0 + w_1 \\cdot i_1 + w_2 \\cdot i_2 + b \\\\\n",
    "$\n",
    "<hr>\n",
    "Parcialni odvodi naših parametrov so sedaj:\n",
    "\n",
    "$\n",
    "\\Large d\\_value \\cdot \\frac{dReLU}{dz} \\cdot \\frac{\\partial z}{\\partial p_j}\n",
    "$\n",
    "\n",
    "* $d\\_value$ predstavlja vrednost, kako močno je ta neuron vplival na naslednjo plast\n",
    "* $p_j$ predstavlja parameter katerega vpliv iščemo. Lahko je to specifičnna weight oziroma bias, lahko pa je tudi input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![d_value](notebook_images/08.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large \\frac{dReLU}{dz} =\n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t\\frac{d}{dz}z  & z > 0 \\\\\n",
    "        \\frac{d}{dz}0  & z \\leq 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "= \\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t1  & z > 0 \\\\\n",
    "        0  & z \\leq 0\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dReLU_dz](notebook_images/09.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0]  # input values\n",
    "w = [-3.0, -1.0, 2.0]  # weights\n",
    "b = 1.0  # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(\"z value: \", z)\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(\"ReLU value: \", y)\n",
    "\n",
    "# The derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(\"dReLU value: \", drelu_dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large \\frac{\\partial z}{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\cdot w_j i_j + \\frac{\\partial}{\\partial w_j} w_{j+1} i_{j+1} + \\frac{\\partial}{\\partial w_j} w_{j+2} i_{j+2} + \\frac{\\partial}{\\partial w_j} b = i_j + 0 + 0 + 0 = i_j $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dsum_dw](notebook_images/10.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0]  # input values\n",
    "w = [-3.0, -1.0, 2.0]  # weights\n",
    "b = 1.0  # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(\"z value: \", z)\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(\"ReLU value: \", y)\n",
    "\n",
    "# The derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(\"dReLU value: \", drelu_dz)\n",
    "\n",
    "# Partial derivatives of the summation and multiplication, the chain rule\n",
    "dsum_dw0 = drelu_dz * x[0]\n",
    "dsum_dw1 = drelu_dz * x[1]\n",
    "dsum_dw2 = drelu_dz * x[2]\n",
    "\n",
    "print(\"dw0 value: \", dsum_dw0)\n",
    "print(\"dw1 value: \", dsum_dw1)\n",
    "print(\"dw2 value: \", dsum_dw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large \\frac{\\partial z}{\\partial b} = \\frac{\\partial}{\\partial b} \\cdot w_j i_j + \\frac{\\partial}{\\partial b} w_{j+1} i_{j+1} + \\frac{\\partial}{\\partial b} w_{j+2} i_{j+2} + \\frac{\\partial}{\\partial b} b = 0 + 0 + 0 + 1 = 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dsum_db](notebook_images/11.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0]  # input values\n",
    "w = [-3.0, -1.0, 2.0]  # weights\n",
    "b = 1.0  # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(\"z value: \", z)\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(\"ReLU value: \", y)\n",
    "\n",
    "# The derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(\"dReLU value: \", drelu_dz)\n",
    "\n",
    "# Partial derivatives of the summation and multiplication, the chain rule\n",
    "dsum_dw0 = drelu_dz * x[0]\n",
    "dsum_dw1 = drelu_dz * x[1]\n",
    "dsum_dw2 = drelu_dz * x[2]\n",
    "\n",
    "dsum_db = 1\n",
    "\n",
    "print(\"dw0 value: \", dsum_dw0)\n",
    "print(\"dw1 value: \", dsum_dw1)\n",
    "print(\"dw2 value: \", dsum_dw2)\n",
    "print(\"db value: \", dsum_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large \\frac{\\partial z}{\\partial i_j} = \\frac{\\partial}{\\partial i_j} \\cdot w_j i_j + \\frac{\\partial}{\\partial i_j} w_{j+1} i_{j+1} + \\frac{\\partial}{\\partial i_j} w_{j+2} i_{j+2} + \\frac{\\partial}{\\partial i_j} b = w_j + 0 + 0 + 0 = w_j $\n",
    "\n",
    "Za naš cilj je ta izračun nepotreben, saj posodabljamo le weights in bias našega neurona. Vendar pa parcialni odvodi glede na input spremenljivko predstavljajo $d\\_value$ vrednost pri računanju vrednosti naslednjih neuronov.\n",
    "\n",
    "![dsum_i](notebook_images/12.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sedaj, ko imamo izračunane naše vrednosti je potrebno le še posodobiti parametre. Pričakovan rezultat je, da se končna izhodna vrednost nevrona zmanjša.\n",
    "\n",
    "Parametre posodobimo tako, da jim prištejemo majhen del negativnega rezultata parcialnega odvoda.\n",
    "\n",
    "$ p_j = p_j + (-l_r \\cdot dp_j) $\n",
    "\n",
    "* $p_j$ je naš parameter katerega želimo posodobiti. Weights oziroma bias\n",
    "* $l_r$ je **learning rate**\n",
    "* $dp_j$ je vrednost parcialnega odvoda\n",
    "\n",
    "Prištejemo negativni rezultat saj želimo našo funkcijo zmanjšati do 0.\n",
    "\n",
    "**Learning rate** uporabimo, ker želimo parametre posodabljati le za neko majhno vrednost. V primeru prevelikega learning rate lahko zgrešimo 0 naše funkcije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0]  # input values\n",
    "w = [-3.0, -1.0, 2.0]  # weights\n",
    "b = 1.0  # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(\"z value: \", z)\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(\"ReLU value: \", y)\n",
    "\n",
    "# The derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(\"dReLU value: \", drelu_dz)\n",
    "\n",
    "# Partial derivatives of the summation and multiplication, the chain rule\n",
    "dsum_dw0 = drelu_dz * x[0]\n",
    "dsum_dw1 = drelu_dz * x[1]\n",
    "dsum_dw2 = drelu_dz * x[2]\n",
    "dsum_db = 1\n",
    "\n",
    "print(\"dw0 value: \", dsum_dw0)\n",
    "print(\"dw1 value: \", dsum_dw1)\n",
    "print(\"dw2 value: \", dsum_dw2)\n",
    "print(\"db value: \", dsum_db)\n",
    "\n",
    "# Optimizing parameters\n",
    "w[0] += -0.001*dsum_dw0\n",
    "w[1] += -0.001*dsum_dw1\n",
    "w[2] += -0.001*dsum_dw2\n",
    "b += -0.001*dsum_db\n",
    "\n",
    "print()\n",
    "print(\"New weights: \", w)\n",
    "print(\"New bias: \", b)\n",
    "\n",
    "# Another forward pass. Excpecting a lower result now.\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(\"New z value: \", z)\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(\"New ReLU value: \", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celotna koda za Dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "        \n",
    "        \n",
    "inputs = np.array([[1.0, 2.0, 3.0, 2.5],\n",
    "                  [1.0, 2.0, 3.0, 2.5],\n",
    "                  [1.0, 2.0, 3.0, 2.5]])\n",
    "\n",
    "dense1 = Layer_Dense(4, 3)\n",
    "print(\"Dense 1 weights: \")\n",
    "print(dense1.weights)\n",
    "print(\"Dense 1 biases\")\n",
    "print(dense1.biases)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense1.forward(inputs)\n",
    "print(\"Dense 1 forward\")\n",
    "print(dense1.output)\n",
    "activation1.forward(dense1.output)\n",
    "print(\"ReLU 1 output:\")\n",
    "print(activation1.output)\n",
    "\n",
    "# d_values passed from next layer back to this layer\n",
    "d_values = np.array([[1.0, 1.0, 1.0],\n",
    "                    [1.0, 1.0, 1.0],\n",
    "                    [1.0, 1.0, 1.0]])\n",
    "\n",
    "activation1.backward(d_values)\n",
    "print(\"Activation 1 dinputs\")\n",
    "print(activation1.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "print(\"Dense 1 dweights\")\n",
    "print(dense1.dweights)\n",
    "print(\"Dense 1 dbias\")\n",
    "print(dense1.dbiases)\n",
    "print(\"Dense 1 dinputs\")\n",
    "print(dense1.dinputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odvodi Loss FunctionS in Softmax layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isti princip deluje na celotni mreži, le da takrat iščemo odvode glede na loss funkcijo.\n",
    "\n",
    "V našem primeru bomo še združili Softmax aktivacijsko funkcijo in Loss funkcijo, saj njuna odvoda skupaj vrneta preprosto enačbo:\n",
    "\n",
    "$\\Large \\frac{\\partial L}{\\partial z_k} = \\hat{y_k} - y_k $\n",
    "\n",
    "* $L$ predstavlja categorical crossentropy loss funkcijo\n",
    "* $\\hat{y_k}$ predstavlja napovedano vrednost\n",
    "* $y_k$ predstavlja realno vrednost\n",
    "\n",
    "Vrednost izraza sedaj predstavlja $d\\_value$, ki je posredovana v neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "# Let's see output of the first few samples:\n",
    "print(loss_activation.output[:5])\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)\n",
    "\n",
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "# Print gradients\n",
    "print(\"Partial derivatives\")\n",
    "print(\"Dense 1: dweights\")\n",
    "print(dense1.dweights)\n",
    "print(\"Dense 1: dbiases\")\n",
    "print(dense1.dbiases)\n",
    "print(\"Dense 2: dweights\")\n",
    "print(dense2.dweights)\n",
    "print(\"Dense 2: dbiases\")\n",
    "print(dense2.dbiases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ko imamo izračunane vse vrednosti lahko le-te uporabimo za posodobitev parametrov z namenom zmanjšanja loss funkcije.\n",
    "\n",
    "Za ta korak poskrbijo optimizerji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD) Optimizer\n",
    "\n",
    "Za začetek si bomo pogledali splošni SGD Optimizer. Pri tem parametre posodobimo tako, da jim odštejemo majhen del njihove vrednosti odvoda.\n",
    "\n",
    "$\\Large p_i = p_i - l_r \\cdot d\\_p_i$\n",
    "\n",
    "* $p_i$ je parameter katerega želimo posodobiti. Weights oziroma bias\n",
    "* $l_r$ je **learning rate**, ki nadzira kako velike posodobitve parametrev naredimo z enim korakom učenja\n",
    "* $d\\_p_i$ je parcialni odvod loss funkcije glede na parameter $p_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celotna koda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "\n",
    "        \n",
    "        \n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "# Let's see output of the first few samples:\n",
    "print(loss_activation.output[:5])\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)\n",
    "\n",
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "# Print gradients\n",
    "#print(dense1.dweights)\n",
    "#print(dense1.dbiases)\n",
    "#print(dense2.dweights)\n",
    "#print(dense2.dbiases)\n",
    "\n",
    "# Update weights and biases\n",
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# And if we make anothe forward pass with same data we should have lowered the loss\n",
    "\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "# Let's see output of the first few samples:\n",
    "print(loss_activation.output[:5])\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sedaj nam je uspelo sestaviti našo osnovno nevronsko mrežo.\n",
    "\n",
    "Opravimo lahko en forward pass, kjer dobimo naše napovedane vrednosti. Nato izračunamo našo vrednost loss funkcije.\n",
    "\n",
    "To vrednost nato uporabimo za posodabljanje parametrov nevronov. Za vsak parameter izračunamo njegov vpliv na vrednost loss funkcije in ga s pomočjo optimizerja posodobimo za majhen korak. Tako sedaj opravimo še **EN BACKWARD PASS** in s tem opravimo celoten postopek učenja naše nevronske mreže."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training in epochos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naš model smo v prejšnjem primeru učili na celotnem naboru podatkov. Uspelo nam je zmanjšati loss vrednost, vendar končni rezultat še ni zadovoljiv.\n",
    "\n",
    "Kar lahko naredimo je, da model treniramo večkrat na istem datasetu.\n",
    "\n",
    "Vsakič, ko uporabilo celoten dataset za treniranje se temu reče **epoch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    \n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.5f}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=200)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"v\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss vrednost smo zmanjšali, vendar ne za veliko.\n",
    "\n",
    "Za izboljšanje modela je potrebno spreminjanje hiperparametrov (*angl.* **hyperparameter**) kot so število plasti, število nevronov, learning rate, itd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 128)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(128, 3)\n",
    "# <=== HERE ===>\n",
    "\n",
    "\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "# <=== HERE ===>\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate=0.85)\n",
    "# <=== HERE ===>\n",
    "\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=200)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"v\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"brg\", marker=\"o\", s=200)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=predictions, cmap=\"brg\", marker=\"v\", s=100, edgecolors=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pri velikem številu plasti in nevronov je potrebno paziti, da si model ne prične zapomniti training dataseta temveč generalizira svoje znanje. Da si je model zapomnil podatke opazimo v veliki razliki med training validation in testing validation vrednostjo.\n",
    "\n",
    "Pri spreminjanju learning rate je potebno paziti kako le-ta vpliva na proces učenja. \n",
    "> S preveliko vrednostjo opravljamo večje posodobitve naših parametrov. To pomeni, da se bomo hitreje približevali ničli naše loss funkcije, vendar jo lahko zaradi prevelikih korakov zgrešimo oziroma nikoli ne dosežemo. Prevelik learning rate se opazi v nestabilnem učenju oziroma v skrajnih primerih v povečanju loss vrednosti.\n",
    "\n",
    "> S premajhno vrednostjo opravljamo majhne posodobitve naših parametrov. To pomeni, da se bomo počasi približevali ničli naše loss funkcije, kar podaljša naš čas učenja. Paziti je tudi potrebno, ker se z majhnim learning rate lahko ujamemo v lokalne minimume naše loss funkcije. Tako dosežemo neko majhno loss vrednost, vendar pa resnične ničle loss funkcije ne bomo dosegli.\n",
    "\n",
    "Za odpravljanje learning rate problemov se uporabljajo principi kot so **learning rate decay**, kjer treniranje modela pričnemo z visokim learning rate, katerega počasi zmanjšujemo. Pri optimizerjih se uporablja tudi princip **SGD with momentum**, kjer parametre posodabljamo še glede na to za koliko smo jih posodobili v prejšnjih korakih.\n",
    "\n",
    "Za kontroliranje vrednosti parametrov se uporabljajo še principi **L1 and L2 Regularization** kjer parametre z večjimi vrednostmi posodobimo za večje število. Tako se želi preprečiti eksplozijo vrednosti enega parametra in se skuša ohraniti vrednosti parametrov na istem nivoju. Na tak način pri učenju sodeluje večje število nevronov in generalizacija znanja je boljša. Za isti namen se uporablja tudi princip **Dropout**, kjer med učenjem ugasnemo določen majhen % nevronov v mreži."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realni primer - uporaba Keras in Tensorflow knjižnjic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za naš primer bomo uporabili knjižnjici Tensorflow in Keras.\n",
    "\n",
    "Tensorflow je python knjižnjica namenjena računanju s tensorji.\n",
    "> https://www.tensorflow.org/\n",
    "\n",
    "Keras je machine learning knjižnjica zgrajena na tensorflow knjižnjici namenjena ustvarjanju nevronskih mrež.\n",
    "> https://keras.io/getting_started/\n",
    "\n",
    "Za inštalacijo je ukaz:\n",
    "> pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za začetek bomo uvozili knjižnjice.\n",
    "\n",
    "Sequential opisuje model sestavljanja nevronske mreže. Dense predstavlja naš dense layer, kjer so vsi nevroni ene plasti povezani na nevrone naslednje plasti. Adam je naš optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first neural network with keras tutorial\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za naš primer bomo uporabili *Prima Indians onset of diabetes dataset*. Opisuje zdravstvene podatke pacientov Prima Indians porekla in napoveduje ali so imeli diabetes v roku 5 let ali ne.\n",
    "\n",
    "Dataset je namenjen binarni klasifikaciji:\n",
    "* **1** - So dobili diabetes\n",
    "* **0** - Niso dobili diabetes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of times pregnant</th>\n",
       "      <th>Plasma glucose concentration a 2 hours in an oral glucose tolerance test</th>\n",
       "      <th>Diastolic blood pressure (mm Hg)</th>\n",
       "      <th>Triceps skin fold thickness (mm)</th>\n",
       "      <th>2-Hour serum insulin (mu U/ml)</th>\n",
       "      <th>Body mass index (weight in kg/(height in m)^2)</th>\n",
       "      <th>Diabetes pedigree function</th>\n",
       "      <th>Age (years)</th>\n",
       "      <th>Class variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of times pregnant  \\\n",
       "0                         6   \n",
       "1                         1   \n",
       "2                         8   \n",
       "3                         1   \n",
       "4                         0   \n",
       "\n",
       "   Plasma glucose concentration a 2 hours in an oral glucose tolerance test  \\\n",
       "0                                                148                          \n",
       "1                                                 85                          \n",
       "2                                                183                          \n",
       "3                                                 89                          \n",
       "4                                                137                          \n",
       "\n",
       "   Diastolic blood pressure (mm Hg)  Triceps skin fold thickness (mm)  \\\n",
       "0                                72                                35   \n",
       "1                                66                                29   \n",
       "2                                64                                 0   \n",
       "3                                66                                23   \n",
       "4                                40                                35   \n",
       "\n",
       "   2-Hour serum insulin (mu U/ml)  \\\n",
       "0                               0   \n",
       "1                               0   \n",
       "2                               0   \n",
       "3                              94   \n",
       "4                             168   \n",
       "\n",
       "   Body mass index (weight in kg/(height in m)^2)  Diabetes pedigree function  \\\n",
       "0                                            33.6                       0.627   \n",
       "1                                            26.6                       0.351   \n",
       "2                                            23.3                       0.672   \n",
       "3                                            28.1                       0.167   \n",
       "4                                            43.1                       2.288   \n",
       "\n",
       "   Age (years)  Class variable  \n",
       "0           50               1  \n",
       "1           31               0  \n",
       "2           32               1  \n",
       "3           21               0  \n",
       "4           33               1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\n",
    "    \"Number of times pregnant\",\n",
    "    \"Plasma glucose concentration a 2 hours in an oral glucose tolerance test\",\n",
    "    \"Diastolic blood pressure (mm Hg)\",\n",
    "    \"Triceps skin fold thickness (mm)\",\n",
    "    \"2-Hour serum insulin (mu U/ml)\",\n",
    "    \"Body mass index (weight in kg/(height in m)^2)\",\n",
    "    \"Diabetes pedigree function\",\n",
    "    \"Age (years)\",\n",
    "    \"Class variable\"\n",
    "]\n",
    "data = pd.read_csv(\"data/pima-indians-diabetes.data.csv\", names=columns)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poglejmo osnovne informacije o našem datasetu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                                                                    Non-Null Count  Dtype  \n",
      "---  ------                                                                    --------------  -----  \n",
      " 0   Number of times pregnant                                                  768 non-null    int64  \n",
      " 1   Plasma glucose concentration a 2 hours in an oral glucose tolerance test  768 non-null    int64  \n",
      " 2   Diastolic blood pressure (mm Hg)                                          768 non-null    int64  \n",
      " 3   Triceps skin fold thickness (mm)                                          768 non-null    int64  \n",
      " 4   2-Hour serum insulin (mu U/ml)                                            768 non-null    int64  \n",
      " 5   Body mass index (weight in kg/(height in m)^2)                            768 non-null    float64\n",
      " 6   Diabetes pedigree function                                                768 non-null    float64\n",
      " 7   Age (years)                                                               768 non-null    int64  \n",
      " 8   Class variable                                                            768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of times pregnant</th>\n",
       "      <th>Plasma glucose concentration a 2 hours in an oral glucose tolerance test</th>\n",
       "      <th>Diastolic blood pressure (mm Hg)</th>\n",
       "      <th>Triceps skin fold thickness (mm)</th>\n",
       "      <th>2-Hour serum insulin (mu U/ml)</th>\n",
       "      <th>Body mass index (weight in kg/(height in m)^2)</th>\n",
       "      <th>Diabetes pedigree function</th>\n",
       "      <th>Age (years)</th>\n",
       "      <th>Class variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Number of times pregnant  \\\n",
       "count                768.000000   \n",
       "mean                   3.845052   \n",
       "std                    3.369578   \n",
       "min                    0.000000   \n",
       "25%                    1.000000   \n",
       "50%                    3.000000   \n",
       "75%                    6.000000   \n",
       "max                   17.000000   \n",
       "\n",
       "       Plasma glucose concentration a 2 hours in an oral glucose tolerance test  \\\n",
       "count                                         768.000000                          \n",
       "mean                                          120.894531                          \n",
       "std                                            31.972618                          \n",
       "min                                             0.000000                          \n",
       "25%                                            99.000000                          \n",
       "50%                                           117.000000                          \n",
       "75%                                           140.250000                          \n",
       "max                                           199.000000                          \n",
       "\n",
       "       Diastolic blood pressure (mm Hg)  Triceps skin fold thickness (mm)  \\\n",
       "count                        768.000000                        768.000000   \n",
       "mean                          69.105469                         20.536458   \n",
       "std                           19.355807                         15.952218   \n",
       "min                            0.000000                          0.000000   \n",
       "25%                           62.000000                          0.000000   \n",
       "50%                           72.000000                         23.000000   \n",
       "75%                           80.000000                         32.000000   \n",
       "max                          122.000000                         99.000000   \n",
       "\n",
       "       2-Hour serum insulin (mu U/ml)  \\\n",
       "count                      768.000000   \n",
       "mean                        79.799479   \n",
       "std                        115.244002   \n",
       "min                          0.000000   \n",
       "25%                          0.000000   \n",
       "50%                         30.500000   \n",
       "75%                        127.250000   \n",
       "max                        846.000000   \n",
       "\n",
       "       Body mass index (weight in kg/(height in m)^2)  \\\n",
       "count                                      768.000000   \n",
       "mean                                        31.992578   \n",
       "std                                          7.884160   \n",
       "min                                          0.000000   \n",
       "25%                                         27.300000   \n",
       "50%                                         32.000000   \n",
       "75%                                         36.600000   \n",
       "max                                         67.100000   \n",
       "\n",
       "       Diabetes pedigree function  Age (years)  Class variable  \n",
       "count                  768.000000   768.000000      768.000000  \n",
       "mean                     0.471876    33.240885        0.348958  \n",
       "std                      0.331329    11.760232        0.476951  \n",
       "min                      0.078000    21.000000        0.000000  \n",
       "25%                      0.243750    24.000000        0.000000  \n",
       "50%                      0.372500    29.000000        0.000000  \n",
       "75%                      0.626250    41.000000        1.000000  \n",
       "max                      2.420000    81.000000        1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset nima null vrednosti, torej ga ni potrebno dodatno očistiti.\n",
    "\n",
    "Vrednosti so številčne, torej ni potrebno pretvarjati stringe.\n",
    "\n",
    "V informacijah o našem datasetu vidimo, da sta razreda razdeljena na sledeč način:\n",
    "* Vzorcev za razred **1** imamo 500, oziroma 65.1%\n",
    "* Vzorcev za razred **0** imamo 268, oziroma 34.9%\n",
    "\n",
    "To pomeni, da naš dataset **ni balanced**, kar bo lahko predstavljalo izvor slabega učenja. Model se lahko preprosto nauči zmeraj napovedati razred 1 in tako doseže 65.1% accuracy. Ker imamo premajhen dataset bomo pustil tako kot je."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevronske mreže delujejo najboljše, če so vhodni podatki okoli 0 do 1 oziroma -1 do 1.\n",
    "\n",
    "Pri skaliranju je potrebno paziti, da uporabimo isto metodo skaliranja na **training dataset** in **testing dataset**. Pomembno je tudi, da **znanje o skaliranju izvira le iz training dataset-a**.\n",
    "> V praksi to pomeni sledeče - imamo training dataset z vhodnimi vrednostmi $\\Large [0,1,2,3,4,5]$ in testing dataset z vhodnimi vrednostmi $\\Large [0,2,4,6,8]$. Podatke želimo normirati, da se nahajajo znotraj intervala 0 in 1. Naša scaling metoda bo delovala po enačbi: $\\Large x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$, kjer sta $\\Large x_{min} = 0$ in $\\Large x_{max}=5$. Če bi uporabili vrednost $8$ bi tako nehote vpeljali znanje o testing datasetu v naše training podatke. Končne vrednost vhodnih podatkov se sedaj ne bodo nahajale med 0 in 1 ampak to ne bi smel biti velik problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of times pregnant</th>\n",
       "      <th>Plasma glucose concentration a 2 hours in an oral glucose tolerance test</th>\n",
       "      <th>Diastolic blood pressure (mm Hg)</th>\n",
       "      <th>Triceps skin fold thickness (mm)</th>\n",
       "      <th>2-Hour serum insulin (mu U/ml)</th>\n",
       "      <th>Body mass index (weight in kg/(height in m)^2)</th>\n",
       "      <th>Diabetes pedigree function</th>\n",
       "      <th>Age (years)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.223606</td>\n",
       "      <td>0.608323</td>\n",
       "      <td>0.561662</td>\n",
       "      <td>0.208650</td>\n",
       "      <td>0.094917</td>\n",
       "      <td>0.475267</td>\n",
       "      <td>0.172152</td>\n",
       "      <td>0.203284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.196835</td>\n",
       "      <td>0.165171</td>\n",
       "      <td>0.160722</td>\n",
       "      <td>0.161194</td>\n",
       "      <td>0.137877</td>\n",
       "      <td>0.118856</td>\n",
       "      <td>0.143837</td>\n",
       "      <td>0.196213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403875</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.043735</td>\n",
       "      <td>0.476900</td>\n",
       "      <td>0.131725</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.712121</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.323232</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.543964</td>\n",
       "      <td>0.242955</td>\n",
       "      <td>0.316667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Number of times pregnant  \\\n",
       "count                614.000000   \n",
       "mean                   0.223606   \n",
       "std                    0.196835   \n",
       "min                    0.000000   \n",
       "25%                    0.058824   \n",
       "50%                    0.176471   \n",
       "75%                    0.352941   \n",
       "max                    1.000000   \n",
       "\n",
       "       Plasma glucose concentration a 2 hours in an oral glucose tolerance test  \\\n",
       "count                                         614.000000                          \n",
       "mean                                            0.608323                          \n",
       "std                                             0.165171                          \n",
       "min                                             0.000000                          \n",
       "25%                                             0.500000                          \n",
       "50%                                             0.585859                          \n",
       "75%                                             0.712121                          \n",
       "max                                             1.000000                          \n",
       "\n",
       "       Diastolic blood pressure (mm Hg)  Triceps skin fold thickness (mm)  \\\n",
       "count                        614.000000                        614.000000   \n",
       "mean                           0.561662                          0.208650   \n",
       "std                            0.160722                          0.161194   \n",
       "min                            0.000000                          0.000000   \n",
       "25%                            0.524590                          0.000000   \n",
       "50%                            0.573770                          0.232323   \n",
       "75%                            0.655738                          0.323232   \n",
       "max                            1.000000                          1.000000   \n",
       "\n",
       "       2-Hour serum insulin (mu U/ml)  \\\n",
       "count                      614.000000   \n",
       "mean                         0.094917   \n",
       "std                          0.137877   \n",
       "min                          0.000000   \n",
       "25%                          0.000000   \n",
       "50%                          0.043735   \n",
       "75%                          0.148936   \n",
       "max                          1.000000   \n",
       "\n",
       "       Body mass index (weight in kg/(height in m)^2)  \\\n",
       "count                                      614.000000   \n",
       "mean                                         0.475267   \n",
       "std                                          0.118856   \n",
       "min                                          0.000000   \n",
       "25%                                          0.403875   \n",
       "50%                                          0.476900   \n",
       "75%                                          0.543964   \n",
       "max                                          1.000000   \n",
       "\n",
       "       Diabetes pedigree function  Age (years)  \n",
       "count                  614.000000   614.000000  \n",
       "mean                     0.172152     0.203284  \n",
       "std                      0.143837     0.196213  \n",
       "min                      0.000000     0.000000  \n",
       "25%                      0.072588     0.050000  \n",
       "50%                      0.131725     0.133333  \n",
       "75%                      0.242955     0.316667  \n",
       "max                      1.000000     1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = data.shape[0]\n",
    "train_samples = 0.8 # % of how much samples to have in the training dataset\n",
    "\n",
    "X_train = data.iloc[:int(num_samples*train_samples), :-1]\n",
    "y_train = data.iloc[:int(num_samples*train_samples), -1]\n",
    "X_test = data.iloc[int(num_samples*train_samples):, :-1]\n",
    "y_test = data.iloc[int(num_samples*train_samples):, -1]\n",
    "\n",
    "scaling_min = X_train.min()\n",
    "scaling_max = X_train.max()\n",
    "\n",
    "def data_scaling(data, scaling_min, scaling_max):\n",
    "    return (data-scaling_min)/(scaling_max - scaling_min)\n",
    "\n",
    "X_train = data_scaling(X_train, scaling_min, scaling_max)\n",
    "X_test = data_scaling(X_test, scaling_min, scaling_max)\n",
    "\n",
    "X_train.describe() # More met vse min na 0 in vse max na 1\n",
    "# X_test.describe() ne bomo nč vn razbral ker ne rabi met od 0 do 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Neural Network Model\n",
    "\n",
    "Modeli znotraj Keras knjižnjice so definirani kot zaporedja plasti.\n",
    "\n",
    "Za začetek ustvarimo *Sequence* model in nato dodajamo plasti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kot prvo bomo ustvarili naš *input layer*, ki ima 8 nevronov, za naših 8 značilk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za arhitekturo našega modela bomo uporabili 3 *dense layers*, kjer imata prvi 2 plasti aktivacijsko funkcijo *ReLU*, zadnja pa *Sigmoid*, ker imamo primer binarne klasifikacije. Vrednosti manjše od 0.5 bodo razred 0, vrednosti večje od 0.5 bodo razred 1.\n",
    "\n",
    "[Dense documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?hl=en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(32, input_dim=8, activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nato bomo naš model zaključili s tem, da mu podamo *loss function* in *optimizer*.\n",
    "\n",
    "[Compile documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model treniramo s **fit()** metodo v kateri lahko definiramo velikost *batch* in število *epochs*.\n",
    "\n",
    "[Fit documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 614 samples\n",
      "Epoch 1/100\n",
      "614/614 [==============================] - 3s 5ms/sample - loss: 0.6929 - accuracy: 0.5228\n",
      "Epoch 2/100\n",
      "614/614 [==============================] - 0s 39us/sample - loss: 0.6754 - accuracy: 0.6450\n",
      "Epoch 3/100\n",
      "614/614 [==============================] - 0s 41us/sample - loss: 0.6662 - accuracy: 0.6531\n",
      "Epoch 4/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.6607 - accuracy: 0.6531\n",
      "Epoch 5/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.6567 - accuracy: 0.6531\n",
      "Epoch 6/100\n",
      "614/614 [==============================] - 0s 38us/sample - loss: 0.6532 - accuracy: 0.6531\n",
      "Epoch 7/100\n",
      "614/614 [==============================] - 0s 44us/sample - loss: 0.6491 - accuracy: 0.6531\n",
      "Epoch 8/100\n",
      "614/614 [==============================] - 0s 42us/sample - loss: 0.6449 - accuracy: 0.6531\n",
      "Epoch 9/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.6407 - accuracy: 0.6515\n",
      "Epoch 10/100\n",
      "614/614 [==============================] - 0s 34us/sample - loss: 0.6367 - accuracy: 0.6547\n",
      "Epoch 11/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.6313 - accuracy: 0.6596\n",
      "Epoch 12/100\n",
      "614/614 [==============================] - 0s 39us/sample - loss: 0.6264 - accuracy: 0.6612\n",
      "Epoch 13/100\n",
      "614/614 [==============================] - 0s 52us/sample - loss: 0.6213 - accuracy: 0.6661\n",
      "Epoch 14/100\n",
      "614/614 [==============================] - 0s 60us/sample - loss: 0.6158 - accuracy: 0.6678\n",
      "Epoch 15/100\n",
      "614/614 [==============================] - 0s 39us/sample - loss: 0.6107 - accuracy: 0.6775\n",
      "Epoch 16/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.6047 - accuracy: 0.6906\n",
      "Epoch 17/100\n",
      "614/614 [==============================] - 0s 44us/sample - loss: 0.5984 - accuracy: 0.7036\n",
      "Epoch 18/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.5918 - accuracy: 0.7036\n",
      "Epoch 19/100\n",
      "614/614 [==============================] - 0s 50us/sample - loss: 0.5850 - accuracy: 0.7134\n",
      "Epoch 20/100\n",
      "614/614 [==============================] - 0s 44us/sample - loss: 0.5780 - accuracy: 0.7231\n",
      "Epoch 21/100\n",
      "614/614 [==============================] - 0s 55us/sample - loss: 0.5707 - accuracy: 0.7264\n",
      "Epoch 22/100\n",
      "614/614 [==============================] - 0s 42us/sample - loss: 0.5640 - accuracy: 0.7378\n",
      "Epoch 23/100\n",
      "614/614 [==============================] - 0s 41us/sample - loss: 0.5578 - accuracy: 0.7427\n",
      "Epoch 24/100\n",
      "614/614 [==============================] - 0s 57us/sample - loss: 0.5516 - accuracy: 0.7378\n",
      "Epoch 25/100\n",
      "614/614 [==============================] - 0s 42us/sample - loss: 0.5477 - accuracy: 0.7524\n",
      "Epoch 26/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.5391 - accuracy: 0.7557\n",
      "Epoch 27/100\n",
      "614/614 [==============================] - 0s 36us/sample - loss: 0.5376 - accuracy: 0.7459\n",
      "Epoch 28/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.5313 - accuracy: 0.7622\n",
      "Epoch 29/100\n",
      "614/614 [==============================] - 0s 36us/sample - loss: 0.5228 - accuracy: 0.7704\n",
      "Epoch 30/100\n",
      "614/614 [==============================] - 0s 46us/sample - loss: 0.5205 - accuracy: 0.7622\n",
      "Epoch 31/100\n",
      "614/614 [==============================] - 0s 41us/sample - loss: 0.5146 - accuracy: 0.7622\n",
      "Epoch 32/100\n",
      "614/614 [==============================] - 0s 39us/sample - loss: 0.5095 - accuracy: 0.7687\n",
      "Epoch 33/100\n",
      "614/614 [==============================] - 0s 34us/sample - loss: 0.5054 - accuracy: 0.7720\n",
      "Epoch 34/100\n",
      "614/614 [==============================] - 0s 41us/sample - loss: 0.5012 - accuracy: 0.7720\n",
      "Epoch 35/100\n",
      "614/614 [==============================] - 0s 36us/sample - loss: 0.4982 - accuracy: 0.7720\n",
      "Epoch 36/100\n",
      "614/614 [==============================] - 0s 47us/sample - loss: 0.4945 - accuracy: 0.7834\n",
      "Epoch 37/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.4887 - accuracy: 0.7915\n",
      "Epoch 38/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.4897 - accuracy: 0.7769\n",
      "Epoch 39/100\n",
      "614/614 [==============================] - 0s 39us/sample - loss: 0.4835 - accuracy: 0.7834\n",
      "Epoch 40/100\n",
      "614/614 [==============================] - 0s 36us/sample - loss: 0.4833 - accuracy: 0.7818\n",
      "Epoch 41/100\n",
      "614/614 [==============================] - 0s 49us/sample - loss: 0.4829 - accuracy: 0.7785\n",
      "Epoch 42/100\n",
      "614/614 [==============================] - 0s 54us/sample - loss: 0.4761 - accuracy: 0.7834\n",
      "Epoch 43/100\n",
      "614/614 [==============================] - 0s 42us/sample - loss: 0.4740 - accuracy: 0.7883\n",
      "Epoch 44/100\n",
      "614/614 [==============================] - 0s 46us/sample - loss: 0.4700 - accuracy: 0.7834\n",
      "Epoch 45/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.4681 - accuracy: 0.7883\n",
      "Epoch 46/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.4666 - accuracy: 0.7801\n",
      "Epoch 47/100\n",
      "614/614 [==============================] - 0s 42us/sample - loss: 0.4689 - accuracy: 0.7818\n",
      "Epoch 48/100\n",
      "614/614 [==============================] - 0s 54us/sample - loss: 0.4615 - accuracy: 0.7932\n",
      "Epoch 49/100\n",
      "614/614 [==============================] - 0s 39us/sample - loss: 0.4641 - accuracy: 0.7785\n",
      "Epoch 50/100\n",
      "614/614 [==============================] - 0s 42us/sample - loss: 0.4626 - accuracy: 0.7834\n",
      "Epoch 51/100\n",
      "614/614 [==============================] - 0s 41us/sample - loss: 0.4567 - accuracy: 0.7752\n",
      "Epoch 52/100\n",
      "614/614 [==============================] - 0s 39us/sample - loss: 0.4538 - accuracy: 0.7866\n",
      "Epoch 53/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.4541 - accuracy: 0.7932\n",
      "Epoch 54/100\n",
      "614/614 [==============================] - 0s 39us/sample - loss: 0.4526 - accuracy: 0.7818\n",
      "Epoch 55/100\n",
      "614/614 [==============================] - 0s 44us/sample - loss: 0.4508 - accuracy: 0.7915\n",
      "Epoch 56/100\n",
      "614/614 [==============================] - 0s 49us/sample - loss: 0.4520 - accuracy: 0.7850\n",
      "Epoch 57/100\n",
      "614/614 [==============================] - 0s 39us/sample - loss: 0.4487 - accuracy: 0.7899\n",
      "Epoch 58/100\n",
      "614/614 [==============================] - 0s 34us/sample - loss: 0.4470 - accuracy: 0.7915\n",
      "Epoch 59/100\n",
      "614/614 [==============================] - 0s 39us/sample - loss: 0.4478 - accuracy: 0.7834\n",
      "Epoch 60/100\n",
      "614/614 [==============================] - 0s 36us/sample - loss: 0.4468 - accuracy: 0.7818\n",
      "Epoch 61/100\n",
      "614/614 [==============================] - 0s 41us/sample - loss: 0.4463 - accuracy: 0.7932\n",
      "Epoch 62/100\n",
      "614/614 [==============================] - 0s 52us/sample - loss: 0.4447 - accuracy: 0.7850\n",
      "Epoch 63/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.4451 - accuracy: 0.7915\n",
      "Epoch 64/100\n",
      "614/614 [==============================] - 0s 39us/sample - loss: 0.4420 - accuracy: 0.7883\n",
      "Epoch 65/100\n",
      "614/614 [==============================] - 0s 36us/sample - loss: 0.4418 - accuracy: 0.7899\n",
      "Epoch 66/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.4412 - accuracy: 0.7899\n",
      "Epoch 67/100\n",
      "614/614 [==============================] - 0s 36us/sample - loss: 0.4429 - accuracy: 0.7785\n",
      "Epoch 68/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.4457 - accuracy: 0.7932\n",
      "Epoch 69/100\n",
      "614/614 [==============================] - 0s 49us/sample - loss: 0.4389 - accuracy: 0.8013\n",
      "Epoch 70/100\n",
      "614/614 [==============================] - 0s 41us/sample - loss: 0.4391 - accuracy: 0.7980\n",
      "Epoch 71/100\n",
      "614/614 [==============================] - 0s 39us/sample - loss: 0.4392 - accuracy: 0.7899\n",
      "Epoch 72/100\n",
      "614/614 [==============================] - 0s 57us/sample - loss: 0.4397 - accuracy: 0.7948\n",
      "Epoch 73/100\n",
      "614/614 [==============================] - 0s 39us/sample - loss: 0.4367 - accuracy: 0.7997\n",
      "Epoch 74/100\n",
      "614/614 [==============================] - 0s 39us/sample - loss: 0.4369 - accuracy: 0.7964\n",
      "Epoch 75/100\n",
      "614/614 [==============================] - 0s 46us/sample - loss: 0.4375 - accuracy: 0.7964\n",
      "Epoch 76/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.4369 - accuracy: 0.7915\n",
      "Epoch 77/100\n",
      "614/614 [==============================] - 0s 36us/sample - loss: 0.4350 - accuracy: 0.7899\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 46us/sample - loss: 0.4357 - accuracy: 0.7932\n",
      "Epoch 79/100\n",
      "614/614 [==============================] - 0s 36us/sample - loss: 0.4389 - accuracy: 0.7948\n",
      "Epoch 80/100\n",
      "614/614 [==============================] - 0s 42us/sample - loss: 0.4338 - accuracy: 0.7948\n",
      "Epoch 81/100\n",
      "614/614 [==============================] - 0s 41us/sample - loss: 0.4338 - accuracy: 0.7932\n",
      "Epoch 82/100\n",
      "614/614 [==============================] - 0s 47us/sample - loss: 0.4348 - accuracy: 0.7915\n",
      "Epoch 83/100\n",
      "614/614 [==============================] - 0s 41us/sample - loss: 0.4361 - accuracy: 0.7866\n",
      "Epoch 84/100\n",
      "614/614 [==============================] - 0s 42us/sample - loss: 0.4320 - accuracy: 0.7932\n",
      "Epoch 85/100\n",
      "614/614 [==============================] - 0s 41us/sample - loss: 0.4317 - accuracy: 0.7964\n",
      "Epoch 86/100\n",
      "614/614 [==============================] - 0s 44us/sample - loss: 0.4326 - accuracy: 0.7948\n",
      "Epoch 87/100\n",
      "614/614 [==============================] - 0s 42us/sample - loss: 0.4318 - accuracy: 0.7899\n",
      "Epoch 88/100\n",
      "614/614 [==============================] - 0s 37us/sample - loss: 0.4310 - accuracy: 0.7948\n",
      "Epoch 89/100\n",
      "614/614 [==============================] - 0s 44us/sample - loss: 0.4323 - accuracy: 0.7964\n",
      "Epoch 90/100\n",
      "614/614 [==============================] - 0s 41us/sample - loss: 0.4339 - accuracy: 0.7883\n",
      "Epoch 91/100\n",
      "614/614 [==============================] - 0s 49us/sample - loss: 0.4354 - accuracy: 0.7997\n",
      "Epoch 92/100\n",
      "614/614 [==============================] - 0s 46us/sample - loss: 0.4302 - accuracy: 0.7801\n",
      "Epoch 93/100\n",
      "614/614 [==============================] - 0s 41us/sample - loss: 0.4303 - accuracy: 0.7980\n",
      "Epoch 94/100\n",
      "614/614 [==============================] - 0s 41us/sample - loss: 0.4280 - accuracy: 0.7980\n",
      "Epoch 95/100\n",
      "614/614 [==============================] - 0s 44us/sample - loss: 0.4291 - accuracy: 0.7883\n",
      "Epoch 96/100\n",
      "614/614 [==============================] - 0s 49us/sample - loss: 0.4287 - accuracy: 0.8029\n",
      "Epoch 97/100\n",
      "614/614 [==============================] - 0s 39us/sample - loss: 0.4269 - accuracy: 0.7980\n",
      "Epoch 98/100\n",
      "614/614 [==============================] - 0s 44us/sample - loss: 0.4281 - accuracy: 0.7883\n",
      "Epoch 99/100\n",
      "614/614 [==============================] - 0s 93us/sample - loss: 0.4266 - accuracy: 0.7948\n",
      "Epoch 100/100\n",
      "614/614 [==============================] - 0s 36us/sample - loss: 0.4284 - accuracy: 0.7932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b81303e320>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Znanje modela bomo preverili s pomočjo test dataset-a.\n",
    "\n",
    "[Evaluate documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 3ms/sample - loss: 0.4773 - accuracy: 0.7792\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Model lahko uporabimo za predikcijo novih podatkov s pomočjo *predict()* metode, kjer dobimo vrednosti zadnje plasti.\n",
    "\n",
    "Oziroma v našem primeru lahko uporabimo *predict_classes()* metodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times pregnant                                                    0.647059\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.696970\n",
      "Diastolic blood pressure (mm Hg)                                            0.606557\n",
      "Triceps skin fold thickness (mm)                                            0.262626\n",
      "2-Hour serum insulin (mu U/ml)                                              0.170213\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.538003\n",
      "Diabetes pedigree function                                                  0.204526\n",
      "Age (years)                                                                 0.483333\n",
      "Name: 614, dtype: float64.\n",
      "PREDICTION: [1], \t REAL: 1\n",
      "\n",
      "Number of times pregnant                                                    0.176471\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.535354\n",
      "Diastolic blood pressure (mm Hg)                                            0.590164\n",
      "Triceps skin fold thickness (mm)                                            0.000000\n",
      "2-Hour serum insulin (mu U/ml)                                              0.000000\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.384501\n",
      "Diabetes pedigree function                                                  0.055081\n",
      "Age (years)                                                                 0.100000\n",
      "Name: 615, dtype: float64.\n",
      "PREDICTION: [0], \t REAL: 0\n",
      "\n",
      "Number of times pregnant                                                    0.352941\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.590909\n",
      "Diastolic blood pressure (mm Hg)                                            0.786885\n",
      "Triceps skin fold thickness (mm)                                            0.000000\n",
      "2-Hour serum insulin (mu U/ml)                                              0.000000\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.427720\n",
      "Diabetes pedigree function                                                  0.033732\n",
      "Age (years)                                                                 0.150000\n",
      "Name: 616, dtype: float64.\n",
      "PREDICTION: [0], \t REAL: 0\n",
      "\n",
      "Number of times pregnant                                                    0.117647\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.343434\n",
      "Diastolic blood pressure (mm Hg)                                            0.508197\n",
      "Triceps skin fold thickness (mm)                                            0.131313\n",
      "2-Hour serum insulin (mu U/ml)                                              0.017730\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.299553\n",
      "Diabetes pedigree function                                                  0.076430\n",
      "Age (years)                                                                 0.033333\n",
      "Name: 617, dtype: float64.\n",
      "PREDICTION: [0], \t REAL: 0\n",
      "\n",
      "Number of times pregnant                                                    0.529412\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.565657\n",
      "Diastolic blood pressure (mm Hg)                                            0.672131\n",
      "Triceps skin fold thickness (mm)                                            0.242424\n",
      "2-Hour serum insulin (mu U/ml)                                              0.000000\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.420268\n",
      "Diabetes pedigree function                                                  0.514091\n",
      "Age (years)                                                                 0.483333\n",
      "Name: 618, dtype: float64.\n",
      "PREDICTION: [1], \t REAL: 1\n",
      "\n",
      "Number of times pregnant                                                    0.000000\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.601010\n",
      "Diastolic blood pressure (mm Hg)                                            0.000000\n",
      "Triceps skin fold thickness (mm)                                            0.000000\n",
      "2-Hour serum insulin (mu U/ml)                                              0.000000\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.482861\n",
      "Diabetes pedigree function                                                  0.026900\n",
      "Age (years)                                                                 0.050000\n",
      "Name: 619, dtype: float64.\n",
      "PREDICTION: [1], \t REAL: 1\n",
      "\n",
      "Number of times pregnant                                                    0.117647\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.565657\n",
      "Diastolic blood pressure (mm Hg)                                            0.704918\n",
      "Triceps skin fold thickness (mm)                                            0.424242\n",
      "2-Hour serum insulin (mu U/ml)                                              0.189125\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.572280\n",
      "Diabetes pedigree function                                                  0.071734\n",
      "Age (years)                                                                 0.116667\n",
      "Name: 620, dtype: float64.\n",
      "PREDICTION: [0], \t REAL: 0\n",
      "\n",
      "Number of times pregnant                                                    0.117647\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.464646\n",
      "Diastolic blood pressure (mm Hg)                                            0.622951\n",
      "Triceps skin fold thickness (mm)                                            0.202020\n",
      "2-Hour serum insulin (mu U/ml)                                              0.000000\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.360656\n",
      "Diabetes pedigree function                                                  0.691716\n",
      "Age (years)                                                                 0.116667\n",
      "Name: 621, dtype: float64.\n",
      "PREDICTION: [0], \t REAL: 0\n",
      "\n",
      "Number of times pregnant                                                    0.352941\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.924242\n",
      "Diastolic blood pressure (mm Hg)                                            0.770492\n",
      "Triceps skin fold thickness (mm)                                            0.000000\n",
      "2-Hour serum insulin (mu U/ml)                                              0.000000\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.608048\n",
      "Diabetes pedigree function                                                  0.590521\n",
      "Age (years)                                                                 0.400000\n",
      "Name: 622, dtype: float64.\n",
      "PREDICTION: [1], \t REAL: 0\n",
      "\n",
      "Number of times pregnant                                                    0.000000\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.474747\n",
      "Diastolic blood pressure (mm Hg)                                            0.573770\n",
      "Triceps skin fold thickness (mm)                                            0.272727\n",
      "2-Hour serum insulin (mu U/ml)                                              0.135934\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.648286\n",
      "Diabetes pedigree function                                                  0.114859\n",
      "Age (years)                                                                 0.000000\n",
      "Name: 623, dtype: float64.\n",
      "PREDICTION: [0], \t REAL: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(X_test)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"{X_test.iloc[i, :]}.\")\n",
    "    print(f\"PREDICTION: {predictions[i]}, \\t REAL: {y_test.iloc[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model\n",
    "\n",
    "Na koncu lahko model shranimo.\n",
    "\n",
    "Keras model je sestavljen iz večih kompoment:\n",
    "* Arhitektura, ki specificira sosledje plasti in število nevronov v plasteh ter način kako so povezane med seboj\n",
    "* Vrednosti weights in biases\n",
    "* Optimizer in njegovih vrednosti (kot so trenutna vrednost learning rate spremenljivke, itd.)\n",
    "* Vrednosti in stanje loss funkcije\n",
    "\n",
    "Shranimo lahko celotni model ali le določene dele. Standardna praksa je shraniti celotni model v TensorFlow SavedModel arhiv (oziroma v starejšem, Keras H5 formatu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\best_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"models\\\\best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da naložimo naš model uporabimo *model.load_model(\"location\")*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times pregnant                                                    0.647059\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.696970\n",
      "Diastolic blood pressure (mm Hg)                                            0.606557\n",
      "Triceps skin fold thickness (mm)                                            0.262626\n",
      "2-Hour serum insulin (mu U/ml)                                              0.170213\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.538003\n",
      "Diabetes pedigree function                                                  0.204526\n",
      "Age (years)                                                                 0.483333\n",
      "Name: 614, dtype: float64.\n",
      "PREDICTION: [1], \t REAL: 1\n",
      "\n",
      "Number of times pregnant                                                    0.176471\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.535354\n",
      "Diastolic blood pressure (mm Hg)                                            0.590164\n",
      "Triceps skin fold thickness (mm)                                            0.000000\n",
      "2-Hour serum insulin (mu U/ml)                                              0.000000\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.384501\n",
      "Diabetes pedigree function                                                  0.055081\n",
      "Age (years)                                                                 0.100000\n",
      "Name: 615, dtype: float64.\n",
      "PREDICTION: [0], \t REAL: 0\n",
      "\n",
      "Number of times pregnant                                                    0.352941\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.590909\n",
      "Diastolic blood pressure (mm Hg)                                            0.786885\n",
      "Triceps skin fold thickness (mm)                                            0.000000\n",
      "2-Hour serum insulin (mu U/ml)                                              0.000000\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.427720\n",
      "Diabetes pedigree function                                                  0.033732\n",
      "Age (years)                                                                 0.150000\n",
      "Name: 616, dtype: float64.\n",
      "PREDICTION: [0], \t REAL: 0\n",
      "\n",
      "Number of times pregnant                                                    0.117647\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.343434\n",
      "Diastolic blood pressure (mm Hg)                                            0.508197\n",
      "Triceps skin fold thickness (mm)                                            0.131313\n",
      "2-Hour serum insulin (mu U/ml)                                              0.017730\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.299553\n",
      "Diabetes pedigree function                                                  0.076430\n",
      "Age (years)                                                                 0.033333\n",
      "Name: 617, dtype: float64.\n",
      "PREDICTION: [0], \t REAL: 0\n",
      "\n",
      "Number of times pregnant                                                    0.529412\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.565657\n",
      "Diastolic blood pressure (mm Hg)                                            0.672131\n",
      "Triceps skin fold thickness (mm)                                            0.242424\n",
      "2-Hour serum insulin (mu U/ml)                                              0.000000\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.420268\n",
      "Diabetes pedigree function                                                  0.514091\n",
      "Age (years)                                                                 0.483333\n",
      "Name: 618, dtype: float64.\n",
      "PREDICTION: [1], \t REAL: 1\n",
      "\n",
      "Number of times pregnant                                                    0.000000\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.601010\n",
      "Diastolic blood pressure (mm Hg)                                            0.000000\n",
      "Triceps skin fold thickness (mm)                                            0.000000\n",
      "2-Hour serum insulin (mu U/ml)                                              0.000000\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.482861\n",
      "Diabetes pedigree function                                                  0.026900\n",
      "Age (years)                                                                 0.050000\n",
      "Name: 619, dtype: float64.\n",
      "PREDICTION: [1], \t REAL: 1\n",
      "\n",
      "Number of times pregnant                                                    0.117647\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.565657\n",
      "Diastolic blood pressure (mm Hg)                                            0.704918\n",
      "Triceps skin fold thickness (mm)                                            0.424242\n",
      "2-Hour serum insulin (mu U/ml)                                              0.189125\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.572280\n",
      "Diabetes pedigree function                                                  0.071734\n",
      "Age (years)                                                                 0.116667\n",
      "Name: 620, dtype: float64.\n",
      "PREDICTION: [0], \t REAL: 0\n",
      "\n",
      "Number of times pregnant                                                    0.117647\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.464646\n",
      "Diastolic blood pressure (mm Hg)                                            0.622951\n",
      "Triceps skin fold thickness (mm)                                            0.202020\n",
      "2-Hour serum insulin (mu U/ml)                                              0.000000\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.360656\n",
      "Diabetes pedigree function                                                  0.691716\n",
      "Age (years)                                                                 0.116667\n",
      "Name: 621, dtype: float64.\n",
      "PREDICTION: [0], \t REAL: 0\n",
      "\n",
      "Number of times pregnant                                                    0.352941\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.924242\n",
      "Diastolic blood pressure (mm Hg)                                            0.770492\n",
      "Triceps skin fold thickness (mm)                                            0.000000\n",
      "2-Hour serum insulin (mu U/ml)                                              0.000000\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.608048\n",
      "Diabetes pedigree function                                                  0.590521\n",
      "Age (years)                                                                 0.400000\n",
      "Name: 622, dtype: float64.\n",
      "PREDICTION: [1], \t REAL: 0\n",
      "\n",
      "Number of times pregnant                                                    0.000000\n",
      "Plasma glucose concentration a 2 hours in an oral glucose tolerance test    0.474747\n",
      "Diastolic blood pressure (mm Hg)                                            0.573770\n",
      "Triceps skin fold thickness (mm)                                            0.272727\n",
      "2-Hour serum insulin (mu U/ml)                                              0.135934\n",
      "Body mass index (weight in kg/(height in m)^2)                              0.648286\n",
      "Diabetes pedigree function                                                  0.114859\n",
      "Age (years)                                                                 0.000000\n",
      "Name: 623, dtype: float64.\n",
      "PREDICTION: [0], \t REAL: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model2 = load_model(\"models\\\\best_model\")\n",
    "\n",
    "\n",
    "predictions = model2.predict_classes(X_test)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"{X_test.iloc[i, :]}.\")\n",
    "    print(f\"PREDICTION: {predictions[i]}, \\t REAL: {y_test.iloc[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This import registers the 3D projection, but is otherwise unused.\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_dataset(samples=30):\n",
    "    X = np.linspace(-2, 2, samples)\n",
    "    y = np.exp(X) + 0.4*np.random.random(size=X.shape[-1])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = exponential_dataset()\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "ax.scatter(X, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from matplotlib import pyplot\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=2)\n",
    "for i in range(len(y)):\n",
    "    print(X[i], \"\\t\", y[i])\n",
    "# plot regression dataset\n",
    "#pyplot.scatter(X,y)\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
