{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d6a743",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523302bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def32ce0",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4a7b5",
   "metadata": {},
   "source": [
    "Overfitting and Underfitting are two **crucial concepts in machine learning** and are the prevalent **causes for the poor performance** of a machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba540fa",
   "metadata": {},
   "source": [
    "<img src=\"./images/overfitting.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e305bb25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f56d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import Output\n",
    "from helpers.plt_overfit import overfit_example, output\n",
    "plt.style.use('helpers/deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd389b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "display(output)\n",
    "ofit = overfit_example(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aceecc",
   "metadata": {},
   "source": [
    "### What is Overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f623b12",
   "metadata": {},
   "source": [
    "<img width=\"700\" src=\"images/reg_01.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a697869",
   "metadata": {},
   "source": [
    "**Reasons for Overfitting**:\n",
    "- Data used for training is not cleaned and contains noise (garbage values) in it\n",
    "- The model has a high variance\n",
    "- The size of the training dataset used is not enough\n",
    "- The model is too complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab5238",
   "metadata": {},
   "source": [
    "**Ways to Tackle Overfitting:**\n",
    "- Using K-fold cross-validation\n",
    "- Using Regularization techniques such as Lasso and Ridge\n",
    "- Training model with sufficient data\n",
    "- Adopting ensembling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f3085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7280c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b66a91a6",
   "metadata": {},
   "source": [
    "## What is Underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a5cb6",
   "metadata": {},
   "source": [
    "<img width=\"700\" src=\"images/reg_02.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea0c7b",
   "metadata": {},
   "source": [
    "**Reasons for Underfitting:**\n",
    "- Data used for training is not cleaned and contains noise (garbage values) in it\n",
    "- The model has a high bias\n",
    "- The size of the training dataset used is not enough\n",
    "- The model is too simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f2e5fa",
   "metadata": {},
   "source": [
    "**Ways to Tackle Underfitting:**\n",
    "- Increase the number of features in the dataset\n",
    "- Increase model complexity\n",
    "- Reduce noise in the data\n",
    "- Increase the duration of training the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8361e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d049d729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034db3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cac7a784",
   "metadata": {},
   "source": [
    "## Bias And Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72a6ff",
   "metadata": {},
   "source": [
    "### Errors in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501294e3",
   "metadata": {},
   "source": [
    "<img width=\"400\" src=\"images/reg_03.jfif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a6982",
   "metadata": {},
   "source": [
    "### What is Bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794e62a",
   "metadata": {},
   "source": [
    "Bias is the **difference between our actual and predicted values**. Bias is the simple assumptions that our model makes about our data to be able to predict new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc9a91c",
   "metadata": {},
   "source": [
    "<img width=\"300\" src=\"images/reg_04.jfif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b218f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f142877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96b69d40",
   "metadata": {},
   "source": [
    "### What is Variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a58d6c",
   "metadata": {},
   "source": [
    "We can define **variance as the modelâ€™s sensitivity to fluctuations in the data**. Our model may **learn from noise**. This will cause our model to **consider trivial features as important**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c3e6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aca93e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c31ab0b2",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e6bd06",
   "metadata": {},
   "source": [
    "<img src=\"images/reg_07.webp\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20558989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "502281b0",
   "metadata": {},
   "source": [
    "- `Underfitting` -> High bias\n",
    "- `Overfitting` -> High variance\n",
    "- `Good balance` -> Low bias, low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5612b0c2",
   "metadata": {},
   "source": [
    "<img  src=\"images/reg_05.jfif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb9d908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1015fdbf",
   "metadata": {},
   "source": [
    "<img  src=\"images/reg_06.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45626b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b1b18de",
   "metadata": {},
   "source": [
    "To build a good model, we need to find a good balance between bias and variance such that it **minimizes the total error.**\n",
    "\n",
    "    Total Error = Bias^2 + Variance + Irreducible errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18fbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d57b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ff0b72f",
   "metadata": {},
   "source": [
    "<img  src=\"images/reg_08.webp\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a0156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ccbb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e14afc11",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2a0cb",
   "metadata": {},
   "source": [
    "**Regularization** refers to **techniques that are used to calibrate machine learning models** in order to **minimize the adjusted loss function and prevent overfitting or underfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd93e2e8",
   "metadata": {},
   "source": [
    "<img width=\"700\" src=\"images/reg_09.jfif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07680b32",
   "metadata": {},
   "source": [
    "<img src=\"images/reg_11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd66c78",
   "metadata": {},
   "source": [
    "<img src=\"images/reg_10.jfif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1bf47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c41f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6fe96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import Output\n",
    "from helpers.plt_overfit import overfit_example, output\n",
    "plt.style.use('helpers/deeplearning.mplstyle')\n",
    "\n",
    "plt.close(\"all\")\n",
    "display(output)\n",
    "ofit = overfit_example(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d25ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa0e0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "182386fc",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85125ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from helpers.datasets import load_extended_boston\n",
    "\n",
    "X, y = load_extended_boston()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "ridge = Ridge()\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "print(f\"Training set score: {ridge.score(X_train_scaled, y_train):.2f}\")\n",
    "print(f\"Test set score: {ridge.score(X_test_scaled, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e86b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "traning_scores = {}\n",
    "testing_scores = {}\n",
    "\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 50, 100]\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha).fit(X_train_scaled, y_train)\n",
    "    traning_scores[alpha] = ridge.score(X_train_scaled, y_train)\n",
    "    testing_scores[alpha] = ridge.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(traning_scores.keys(), traning_scores.values(), c=\"blue\", label=\"training\")\n",
    "plt.plot(testing_scores.keys(), testing_scores.values(), c=\"red\", label=\"testing\")\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge10 = Ridge(alpha=0.001).fit(X_train_scaled, y_train)\n",
    "print(f\"Training set score: {ridge10.score(X_train_scaled, y_train):.2f}\")\n",
    "print(f\"Test set score: {ridge10.score(X_test_scaled, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd76fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge01 = Ridge(alpha=5).fit(X_train_scaled, y_train)\n",
    "print(f\"Training set score: {ridge01.score(X_train_scaled, y_train):.2f}\")\n",
    "print(f\"Test set score: {ridge01.score(X_test_scaled, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e28369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ridge = Ridge().fit(X_train_scaled, y_train)\n",
    "ridge10 = Ridge(alpha=10).fit(X_train_scaled, y_train)\n",
    "ridge01 = Ridge(alpha=0.1).fit(X_train_scaled, y_train)\n",
    "lr = LinearRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "plt.plot(ridge.coef_, 's', label=\"Ridge alpha=1\")\n",
    "plt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\n",
    "plt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\n",
    "plt.plot(lr.coef_, 'o', label=\"LinearRegression\")\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.hlines(0, 0, len(lr.coef_))\n",
    "plt.ylim(-25, 25)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plot_ridge_n_samples import plot_ridge_n_samples\n",
    "\n",
    "plot_ridge_n_samples()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5d4f0c",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aee360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso().fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Training set score: {lasso.score(X_train_scaled, y_train):.2f}\")\n",
    "print(f\"Test set score: {lasso.score(X_test_scaled, y_test):.2f}\")\n",
    "print(f\"Number of features used: {np.sum(lasso.coef_ != 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4198b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "traning_scores = {}\n",
    "testing_scores = {}\n",
    "\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=100000).fit(X_train_scaled, y_train)\n",
    "    traning_scores[alpha] = lasso.score(X_train_scaled, y_train)\n",
    "    testing_scores[alpha] = lasso.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada52b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(traning_scores.keys(), traning_scores.values(), c=\"blue\", label=\"training\")\n",
    "plt.plot(testing_scores.keys(), testing_scores.values(), c=\"red\", label=\"testing\")\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we increase the default setting of \"max_iter\",\n",
    "# otherwise the model would warn us that we should increase max_iter.\n",
    "lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train_scaled, y_train)\n",
    "print(f\"Training set score: {lasso001.score(X_train_scaled, y_train):.2f}\")\n",
    "print(f\"Test set score: {lasso001.score(X_test_scaled, y_test):.2f}\")\n",
    "print(f\"Number of features used: {np.sum(lasso001.coef_ != 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train_scaled, y_train)\n",
    "print(f\"Training set score: {lasso00001.score(X_train_scaled, y_train):.2f}\")\n",
    "print(f\"Test set score: {lasso00001.score(X_test_scaled, y_test):.2f}\")\n",
    "print(f\"Number of features used: {np.sum(lasso00001.coef_ != 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lasso.coef_, 's', label=\"Lasso alpha=1\")\n",
    "plt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")\n",
    "plt.plot(lasso00001.coef_, 'v', label=\"Lasso alpha=0.0001\")\n",
    "plt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\")\n",
    "\n",
    "plt.legend(ncol=2, loc=(0, 1.05))\n",
    "plt.ylim(-25, 25)\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231003d5",
   "metadata": {},
   "source": [
    "### Elastic Net Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542acaf0",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ced6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "e_net = ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Training set score: {e_net.score(X_train_scaled, y_train):.2f}\")\n",
    "print(f\"Test set score: {e_net.score(X_test_scaled, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae5927",
   "metadata": {},
   "source": [
    "## Strengths, weaknesses, and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be21fd",
   "metadata": {},
   "source": [
    "<img src=\"images/reg_12.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50941a12",
   "metadata": {},
   "source": [
    "**Parameters**:\n",
    "- The main parameter of linear models is the regularization parameter:\n",
    "    - called `alpha` in the regression models\n",
    "- Large values for alpha mean simple models. \n",
    "- In particular for the regression models, tuning these parameters is quite important. \n",
    "- Usually alpha is searched for on a logarithmic scale. \n",
    "- The other decision you have to make is whether you want to use **L1 regularization or L2 regularization**. \n",
    "    - If you assume that only a few of your features are actually important, you should use L1. \n",
    "    - Otherwise, you should default to L2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf1f07",
   "metadata": {},
   "source": [
    "**Strengths**:\n",
    "- Linear models are **very fast** to train, and also fast to predict. \n",
    "- They **scale to very large datasets** and **work well with sparse data***. \n",
    "- Linear models often **perform well when the number of features is large compared to the number of samples**. \n",
    "- They are also often used on very large datasets, simply because itâ€™s not feasible to train other models. \n",
    "\n",
    "**Weaknesses**:\n",
    "- However, in lower-dimensional spaces, other models might yield better generalization performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
