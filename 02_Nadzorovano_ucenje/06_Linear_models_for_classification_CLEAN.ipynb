{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12a2608",
   "metadata": {},
   "source": [
    "# Linear models for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2afefe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a18fcc",
   "metadata": {},
   "source": [
    "## Intro to classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14198e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([0., 1, 2, 3, 4, 5])\n",
    "y_train = np.array([0,  0, 0, 1, 1, 1])\n",
    "X_train2 = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_train2 = np.array([0, 0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.lab_utils_common_lc import dlc, plot_data\n",
    "from helpers.plt_one_addpt_onclick import plt_one_addpt_onclick\n",
    "\n",
    "plt.style.use('helpers/deeplearning.mplstyle')\n",
    "\n",
    "pos = y_train == 1\n",
    "neg = y_train == 0\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(8,3))\n",
    "#plot 1, single variable\n",
    "ax[0].scatter(x_train[pos], y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
    "ax[0].scatter(x_train[neg], y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none', \n",
    "              edgecolors=dlc[\"dlblue\"],lw=3)\n",
    "\n",
    "ax[0].set_ylim(-0.08,1.1)\n",
    "ax[0].set_ylabel('y', fontsize=12)\n",
    "ax[0].set_xlabel('x', fontsize=12)\n",
    "ax[0].set_title('one variable plot')\n",
    "ax[0].legend()\n",
    "\n",
    "#plot 2, two variables\n",
    "plot_data(X_train2, y_train2, ax[1])\n",
    "ax[1].axis([0, 4, 0, 4])\n",
    "ax[1].set_ylabel('$x_1$', fontsize=12)\n",
    "ax[1].set_xlabel('$x_0$', fontsize=12)\n",
    "ax[1].set_title('two variable plot')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02830f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bb2133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cbccae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd972e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "w_in = np.zeros((1))\n",
    "b_in = 0\n",
    "plt.close('all') \n",
    "addpt = plt_one_addpt_onclick( x_train,y_train, w_in, b_in, logistic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0889d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe04241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20a9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99a79dee",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a01d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ac0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211d6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e740371",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all') \n",
    "addpt = plt_one_addpt_onclick( x_train,y_train, w_in, b_in, logistic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b520a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85a887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93529140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a925ceb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e969aabb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c0f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.datasets import make_forge\n",
    "from helpers.plot_2d_separator import plot_2d_separator\n",
    "from helpers.plot_helpers import discrete_scatter\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline\n",
    "\n",
    "X, y = make_forge()\n",
    "model = LogisticRegression()\n",
    "clf = model.fit(X, y)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "for model, ax in zip([LogisticRegression(), LogisticRegression(penalty=\"none\")], axes):\n",
    "    clf = model.fit(X, y)\n",
    "    plot_2d_separator(clf, X, fill=False, eps=0.5,ax=ax, alpha=.7)\n",
    "    discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "    ax.set_title(f\"{clf.__class__.__name__}_{clf.penalty}\")\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "    \n",
    "axes[0].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95efa21f",
   "metadata": {},
   "source": [
    "**Types of Logistic Regression**:\n",
    "- `Binary Logistic Regression`: The target variable has only two possible outcomes such as Spam or Not Spam, Cancer or No Cancer.\n",
    "- `Multinomial Logistic Regression`: The target variable has three or more nominal categories such as predicting the type of Wine.\n",
    "- `Ordinal Logistic Regression`: the target variable has three or more ordinal categories such as restaurant or product rating from 1 to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6288c40",
   "metadata": {},
   "source": [
    "**Advantages**:\n",
    "- Because of its efficient and straightforward nature, it doesn't require high computation power, is easy to implement, easily interpretable, and used widely by data analysts and scientists.\n",
    "- Also, it doesn't require scaling of features (it's faster with scaling). \n",
    "- Logistic regression provides a probability score for observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b9ece",
   "metadata": {},
   "source": [
    "**Disadvantages**:\n",
    "- Logistic regression is **not able to handle a large number of categorical** features/variables.\n",
    "- It is **vulnerable to overfitting**.\n",
    "- Also, **can't solve the non-linear problem** with the logistic regression that is why it requires a transformation of non-linear features (Polynomial features?).\n",
    "- Logistic regression will **not perform well with independent variables that are not correlated to the target variable** and are very similar or correlated to each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a44a86f",
   "metadata": {},
   "source": [
    "## Regularization for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c49ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5720bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8643ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Training set score: {logreg.score(X_train_scaled, y_train):.3f}\")\n",
    "print(f\"Test set score: {logreg.score(X_test_scaled, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f06fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg100 = LogisticRegression(max_iter=10000, C=100).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Training set score: {logreg100.score(X_train_scaled, y_train):.3f}\")\n",
    "print(f\"Test set score: {logreg100.score(X_test_scaled, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f63959",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg001 = LogisticRegression(max_iter=10000, C=0.01).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Training set score: {logreg001.score(X_train_scaled, y_train):.3f}\")\n",
    "print(f\"Test set score: {logreg001.score(X_test_scaled, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728dac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "traning_scores = {}\n",
    "testing_scores = {}\n",
    "\n",
    "c_values = [0.01, 0.1, 0.5, 1, 10, 100, 500, 1000, 5000]\n",
    "\n",
    "for c in c_values:\n",
    "    logreg_diff = LogisticRegression(C=c, max_iter=100000).fit(X_train_scaled, y_train)\n",
    "    traning_scores[c] = logreg_diff.score(X_train_scaled, y_train)\n",
    "    testing_scores[c] = logreg_diff.score(X_test_scaled, y_test)\n",
    "    \n",
    "plt.plot(traning_scores.keys(), traning_scores.values(), c=\"blue\", label=\"training\")\n",
    "plt.plot(testing_scores.keys(), testing_scores.values(), c=\"red\", label=\"testing\")\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=10000).fit(X_train, y_train)\n",
    "logreg100 = LogisticRegression(max_iter=10000, C=100).fit(X_train, y_train)\n",
    "logreg001 = LogisticRegression(max_iter=10000, C=0.01).fit(X_train, y_train)\n",
    "\n",
    "plt.plot(logreg.coef_.T, 'o', label=\"C=1\")\n",
    "plt.plot(logreg100.coef_.T, '^', label=\"C=100\")\n",
    "plt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\n",
    "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
    "plt.hlines(0, 0, cancer.data.shape[1])\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d71e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n",
    "    lr_l1 = LogisticRegression(C=C, penalty=\"l1\", solver=\"liblinear\").fit(X_train_scaled, y_train)\n",
    "    print(f\"Training accuracy of l1 logreg with C={C:.3f}: {lr_l1.score(X_train_scaled, y_train):.2f}\")\n",
    "    print(f\"Test accuracy of l1 logreg with C={C:.3f}: {lr_l1.score(X_test_scaled, y_test):.2f}\")\n",
    "    plt.plot(lr_l1.coef_.T, marker, label=f\"C={C:.3f}\")\n",
    "\n",
    "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
    "plt.hlines(0, 0, cancer.data.shape[1])\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.ylim(-5, 5)\n",
    "plt.legend(loc=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a717c531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532cb2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19716098",
   "metadata": {},
   "source": [
    "## Uncertainty Estimates from Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185cf11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a325b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa6c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from helpers.plot_helpers import discrete_scatter\n",
    "\n",
    "X, y = make_forge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c203f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_scatter(X[:, 0], X[:, 1], y, markers='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# we rename the classes \"blue\" and \"red\" for illustration purposes\n",
    "y_named = np.array([\"blue\", \"red\"])[y]\n",
    "\n",
    "# we can call train_test_split with arbitrarily many arrays;\n",
    "# all will be split in a consistent manner\n",
    "X_train, X_test, y_train_named, y_test_named, y_train, y_test = train_test_split(X, y_named, y, random_state=0)\n",
    "\n",
    "# build the gradient boosting model\n",
    "lc = LogisticRegression(random_state=0)\n",
    "lc.fit(X_train, y_train_named)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc7ebf2",
   "metadata": {},
   "source": [
    "### The Decision Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e561882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_test.shape: {X_test.shape}\")\n",
    "print(f\"Decision function shape: {lc.decision_function(X_test).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d74b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first few entries of decision_function\n",
    "lc.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9511e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc.decision_function(X_test) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65369ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce7d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aad8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the boolean True/False into 0 and 1\n",
    "greater_zero = (lc.decision_function(X_test) > 0).astype(int)\n",
    "\n",
    "# use 0 and 1 as indices into classes_\n",
    "pred = lc.classes_[greater_zero]\n",
    "\n",
    "# pred is the same as the output of gbrt.predict\n",
    "print(f\"pred is equal to predictions: {np.all(pred == lc.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b630831",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_function = lc.decision_function(X_test)\n",
    "\n",
    "print(f\"Decision function minimum: {np.min(decision_function):.2f} maximum: {np.max(decision_function):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed005334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import tools\n",
    "from helpers import plot_helpers\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "tools.plot_2d_separator(lc, X, ax=axes[0], alpha=.4, fill=True, cm=plot_helpers.cm2)\n",
    "\n",
    "scores_image = tools.plot_2d_scores(lc, X, ax=axes[1], alpha=.4, cm=plot_helpers.ReBl)\n",
    "\n",
    "\n",
    "for ax in axes:\n",
    "    # plot training and test points\n",
    "    discrete_scatter(X_test[:, 0], X_test[:, 1], y_test, markers='^', ax=ax)\n",
    "    discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, markers='o', ax=ax)\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "    \n",
    "plt.rcParams['axes.grid'] = False\n",
    "cbar = plt.colorbar(scores_image, ax=axes.tolist())\n",
    "axes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\", \"Train class 1\"], ncol=4, loc=(.1, 1.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b7794b",
   "metadata": {},
   "source": [
    "### Predicting Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9309fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc.predict_proba(X_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3aa3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00887bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a0415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2841bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5)) \n",
    "\n",
    "tools.plot_2d_separator(lc, X, ax=axes[0], alpha=.4, fill=True, cm=plot_helpers.cm2)\n",
    "scores_image = tools.plot_2d_scores(lc, X, ax=axes[1], alpha=.5, cm=plot_helpers.ReBl, function='predict_proba')\n",
    "\n",
    "for ax in axes:\n",
    "    # plot training and test points\n",
    "    discrete_scatter(X_test[:, 0], X_test[:, 1], y_test, markers='^', ax=ax)\n",
    "    discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, markers='o', ax=ax)\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "cbar = plt.colorbar(scores_image, ax=axes.tolist())\n",
    "axes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\", \"Train class 1\"], ncol=4, loc=(.1, 1.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990b7f04",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db71cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "pima = pd.read_csv(\"data/diabetes.csv\", header=None, names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149031c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20244536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima[\"label\"] # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eea4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(random_state=16)\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "logreg.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e4bf41",
   "metadata": {},
   "source": [
    "## Linear models for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from helpers.tools import discrete_scatter\n",
    "\n",
    "X, y = make_blobs(random_state=42)\n",
    "\n",
    "discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16734d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_lr = LogisticRegression().fit(X, y)\n",
    "\n",
    "print(\"Coefficient shape: \", linear_lr.coef_.shape)\n",
    "print(\"Intercept shape: \", linear_lr.intercept_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2888bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "\n",
    "line = np.linspace(-15, 15)\n",
    "\n",
    "for coef, intercept, color in zip(linear_lr.coef_, linear_lr.intercept_, ['b', 'r', 'g']):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "    \n",
    "plt.ylim(-10, 15)\n",
    "plt.xlim(-10, 8)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1','Line class 2'], loc=(1.01, 0.3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plot_2d_separator import plot_2d_classification\n",
    "\n",
    "plot_2d_classification(linear_lr, X, fill=True, alpha=.7)\n",
    "discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "\n",
    "line = np.linspace(-15, 15)\n",
    "\n",
    "for coef, intercept, color in zip(linear_lr.coef_, linear_lr.intercept_,['b', 'r', 'g']):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1', 'Line class 2'], loc=(1.01, 0.3))\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bde099",
   "metadata": {},
   "source": [
    "> All classifiers in scikit-learn do multiclass classification out-of-the-box. You don’t need to use the sklearn.multiclass module unless you want to experiment with different multiclass strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4df4c2",
   "metadata": {},
   "source": [
    "## Uncertainty in Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0806ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.decision_function(X_test_scaled).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.decision_function(X_test_scaled)[:6, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c279f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(lr.decision_function(X_test_scaled), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322633f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the first few entries of predict_proba\n",
    "print(f\"Predicted probabilities:\\n{lr.predict_proba(X_test_scaled)[:6]}\")\n",
    "# show that sums across rows are one\n",
    "print(f\"Sums: {lr.predict_proba(X_test_scaled)[:6].sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a96a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(lr.predict_proba(X_test_scaled), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af003e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# represent each target by its class name in the iris dataset\n",
    "named_target = iris.target_names[y_train]\n",
    "\n",
    "logreg.fit(X_train_scaled, named_target)\n",
    "\n",
    "print(f\"unique classes in training data: {logreg.classes_}\")\n",
    "print(f\"predictions: {logreg.predict(X_test_scaled)[:10]}\")\n",
    "\n",
    "argmax_dec_func = np.argmax(logreg.decision_function(X_test_scaled), axis=1)\n",
    "print(f\"argmax of decision function: {argmax_dec_func[:10]}\")\n",
    "print(f\"argmax combined with classes_: {logreg.classes_[argmax_dec_func][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79366d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2799aa0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a3299b9",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression in Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b6a3c",
   "metadata": {},
   "source": [
    "- [OpenML](https://www.openml.org/): A worldwide machine learning lab\n",
    "- [Kaggle](https://www.kaggle.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09147674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print to show there are 1797 images (8 by 8 images for a dimensionality of 64)\n",
    "print(\"Image Data Shape\" , digits.data.shape)\n",
    "\n",
    "# Print to show there are 1797 labels (integers from 0-9)\n",
    "print(\"Label Data Shape\", digits.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, label) in enumerate(zip(digits.data[0:5], digits.target[0:5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
    "    plt.title('Training: %i\\n' % label, fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e29d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e20ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logisticRegr = LogisticRegression(max_iter=10000)\n",
    "logisticRegr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = logisticRegr.score(x_test, y_test)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
